<!DOCTYPE html>













<html class="theme-next " lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="">












<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">






















<link href="/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="//main.css?v=6.5.0" rel="stylesheet" type="text/css">













<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: '',
    version: '6.5.0',
    sidebar: "",
    fancybox: ,
    fastclick: ,
    lazyload: ,
    tabs: ,
    motion: "",
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: "",
      labels: ""
    }
  };
</script>


  




  <meta name="description" content="Neural Network and Deep Learning, Course Notes">
<meta name="keywords" content="Programming">
<meta property="og:type" content="article">
<meta property="og:title" content="Neural Network and Deep Learning, Course Notes">
<meta property="og:url" content="http://yoursite.com/2019/03/08/神经网络和深度学习课堂笔记/index.html">
<meta property="og:site_name" content="Qingliu">
<meta property="og:description" content="Neural Network and Deep Learning, Course Notes">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2019-03-11T03:20:54.592Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Neural Network and Deep Learning, Course Notes">
<meta name="twitter:description" content="Neural Network and Deep Learning, Course Notes">



  <link rel="alternate" href="/atom.xml" title="Qingliu" type="application/atom+xml">






<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Neural Network and Deep Learning, Course Notes | Qingliu</title>
  











  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="en">

  
  

  <div class="container  page-post-detail">
    <div class="headband"></div>

<a href="https://github.com/Aden-Q" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Qingliu</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/08/神经网络和深度学习课堂笔记/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qingliu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Qingliu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Neural Network and Deep Learning, Course Notes
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Neural Network and Deep Learning, Course Notes</p>
<a id="more"></a>
<p>Goal:</p>
<ul>
<li>理解驱动深度学习的主要技术趋势</li>
<li>能够搭建、训练并运用全连接的深层网络</li>
<li>了解如何实现高效的（向量化）神经网络</li>
<li>理解神经网络架构中的关键参数</li>
</ul>
<h2 id="课时2-1"><a href="#课时2-1" class="headerlink" title="课时2.1"></a>课时2.1</h2><p>学习神经网络编程的基础知识</p>
<p>正向传播过程和反向传播过程forward, backward</p>
<p>二分类问题</p>
<p>计算机保存一张图片要用三个矩阵，分别表示三个通道</p>
<p>矩阵向量化：变成特征向量，如果是64*64的矩阵。则向量化后向量的维度是64*64*3</p>
<p>用m表示训练样本数</p>
<p>设计矩阵用列向量堆叠的方式会好很多：</p>
<p>X.shape</p>
<p>标签的向量化矩阵是1*m的，其中m表示样本数。Y.shape = (1, m)</p>
<h2 id="课时2-2"><a href="#课时2-2" class="headerlink" title="课时2.2"></a>课时2.2</h2><p>logistic regression</p>
<p>给定x，希望计算出y的预测值 y = P(1|x)</p>
<p>$$\hat{y} = w^{T}x+b$$</p>
<p>由于y的值不在0-1之间，希望要把它进行概率归一化</p>
<p>需要使用sigmoid函数</p>
<p>$$\hat{y} = \sigma(w^{T}x+b)$$</p>
<p>$$\sigma(z)=\frac{1}{1+e^{-z}}$$</p>
<p>如果z是很大的负数，则概率接近于0。如果z是很大的正数，则概率接近于1</p>
<p>预测函数里的b项是intercepter</p>
<p>$$\theta$$是列向量，用$$\theta_{0}$$来表示常数项b。然后在样本项x中引入一个1即可</p>
<h2 id="课时2-3"><a href="#课时2-3" class="headerlink" title="课时2.3"></a>课时2.3</h2><p>loss funciton 和 cost function不一样</p>
<p>表示第i个样本的预测值<br>$$\hat{y}^{(i)} = \sigma(w^{T}x^{(i)}+b)$$</p>
<p>$$\sigma(z)=\frac{1}{1+e^{-z}}$$</p>
<p>用上标指明数据样本</p>
<p>在logistic中使用的损失函数loss function：<br>$$L(\hat{y},y) = -(ylog\hat{y}+(1-y)log(1-\hat{y}))$$</p>
<p>选修视频中会讲为什么这么选损失函数</p>
<p>cost function:</p>
<p>$$J(w,b) = \frac{1}{m}\sum_{i=1}^{m}L(\hat{y,y})$$</p>
<p>loss funtion应用于单个训练样本，计算单个样本的损失程度</p>
<p>cost function是成本函数，计算整个训练集的总体损失</p>
<p>目标是找w，b，使得cost function尽量小</p>
<h2 id="课时2-4"><a href="#课时2-4" class="headerlink" title="课时2.4"></a>课时2.4</h2><p>梯度下降法</p>
<p>训练和学习w和b</p>
<p>凸函数可以更好地进行梯度下降</p>
<p>对logistic回归而言，几乎任何初始化值都是可行的</p>
<p>minimize J(w,b)</p>
<p>梯度下降的过程：</p>
<p>$$w := w-\alpha\frac{dJ(w)}{dw}$$</p>
<p>$$\alpha$$表示学习速率。之后会讲如何选择learning rate</p>
<p>在表示上，用dw表示导数项</p>
<p>$$w := w - \alpha dw$$</p>
<p>梯度表示切线斜率</p>
<p>后面的课讲导数和微积分</p>
<h2 id="课时2-5"><a href="#课时2-5" class="headerlink" title="课时2.5"></a>课时2.5</h2><p>讲导数</p>
<h2 id="课时2-6"><a href="#课时2-6" class="headerlink" title="课时2.6"></a>课时2.6</h2><p>讲导数</p>
<h2 id="课时2-7"><a href="#课时2-7" class="headerlink" title="课时2.7"></a>课时2.7</h2><p>Computation Graph计算图</p>
<p>前向传播用来计算神经网路的输出，反向传播用来计算出对应的梯度或导数</p>
<p>根据运算符优先级计算</p>
<h2 id="课时2-8"><a href="#课时2-8" class="headerlink" title="课时2.8"></a>课时2.8</h2><p>计算图的导数计算</p>
<p>链式法则计算导数的反向传播</p>
<p>chain rule</p>
<p>在编程的时候，就用dvar表示目标输出对于var的导数</p>
<p>分析梯度就是给变量一个微小增量，看目标输出变量的变化情况</p>
<h2 id="课时2-9"><a href="#课时2-9" class="headerlink" title="课时2.9"></a>课时2.9</h2><p>logistic回归中的梯度下降法</p>
<p>表示输出<br>$$\hat{y} = a = \sigma(z)$$</p>
<p>$$z = w^{T}x+b$$</p>
<p>步骤：<br>w1,w2,x1,x2,b</p>
<p>$$z = w_{1}x_{1}+w_{2}x_{2}+b$$ 计算输出</p>
<p>$$\hat{y} = a = \sigma(z)$$ 计算概率输出，也即计算激活函数值</p>
<p>$$L(a,y)$$ 计算损失函数</p>
<p>需要做的是修正w和b，从而减少损失函数L</p>
<h2 id="课时2-10"><a href="#课时2-10" class="headerlink" title="课时2.10"></a>课时2.10</h2><p>m个样本的梯度下降</p>
<p>$$a^{(i)}=\hat{y}^{(i)}=\sigma(z^{i})=\sigma(w^{T}x^{(i)}+b)$$</p>
<p>累加器，全局成本函数cost function，然后对训练样本数做平均，计算完后，做全局更新</p>
<p>这节课还没讲向量化。向量化之后就会好很多。</p>
<p>后面的视频会讲向量化</p>
<h1 id="课时2-11"><a href="#课时2-11" class="headerlink" title="课时2.11"></a>课时2.11</h1><p>向量化</p>
<p>numpy：z = np.dot(w,x)</p>
<p>向量化版本比非向量化版本快很多</p>
<p>numpy可以自己执行并行化</p>
<h2 id="课时2-12"><a href="#课时2-12" class="headerlink" title="课时2.12"></a>课时2.12</h2><p>讲了logistic的向量化</p>
<p>recap了一下</p>
<p>对于正向传播，每个样本都需要计算激活函数值，sigmoid激活到0-1之间</p>
<h2 id="课时2-13-2-14"><a href="#课时2-13-2-14" class="headerlink" title="课时2.13-2.14"></a>课时2.13-2.14</h2><p>设计矩阵列堆叠，也是横向堆叠<br> 一次性计算所有z：</p>
<p> 构建一个1*m的矩阵</p>
<p> $$Z = np.dot(w.T,X)+b$$</p>
<p> numpy会将b自动广播为一个行向量</p>
<p> 然后计算a：激活函数值</p>
<p> 无论是正向传播还是反向传播，向量化都可以加速计算</p>
<p>$$dZ = A-Y$$</p>
<p>$$dw = \frac{1}{m}XdZ^{T}$$</p>
<p>$$db = \frac{1}{m}np.sum(dZ)$$</p>
<p>迭代次数的for没有办法去掉</p>
<h2 id="课时2-15"><a href="#课时2-15" class="headerlink" title="课时2.15"></a>课时2.15</h2><p>axis表示竖向</p>
<h2 id="课时2-16"><a href="#课时2-16" class="headerlink" title="课时2.16"></a>课时2.16</h2><p>python numpy</p>
<p>broadcasting</p>
<p>tricks来排除问题，简化程序</p>
<p>建议不要用(5,)或(n,)这种秩为1的结构</p>
<p>用(5,1)，不要用(5,)</p>
<p>用assert(a.shape == (5,1))来保证不出错，检查矩阵维度</p>
<p>图片属于非结构化数据？</p>
<p>神经元节点先计算线性函数再算激活函数</p>
<h2 id="课时3-1"><a href="#课时3-1" class="headerlink" title="课时3.1"></a>课时3.1</h2><p>神经网络概览</p>
<p>表示：用[1] [2]来表示神经网络的各个层</p>
<h2 id="课时3-2"><a href="#课时3-2" class="headerlink" title="课时3.2"></a>课时3.2</h2><p>神经网络的表示</p>
<p>输入层，隐藏层，输出层</p>
<p>$$a^{[0]}$$表示输入层的激活值</p>
<p>$$a^{[1]}<em>{1} a^{[1]}</em>{2}$$表示隐藏层的激活值</p>
<p>$$a^{[1]}$$是一个多维列向量</p>
<p>计算网络的层数的时候不管输入层</p>
<h2 id="课时3-3"><a href="#课时3-3" class="headerlink" title="课时3.3"></a>课时3.3</h2><p>计算神经网络的输出</p>
<p>recat表示方法：上标表示神经网路的层数，下标表示第几个神经元</p>
<p>向量化表示：把w的转置进行行堆叠，作为左乘矩阵，然后输入向量x是特征列堆叠，得到一个列向量，b向量也是一个列向量</p>
<p>把激活函数值也可以堆叠成列向量</p>
<p>$$z^{[1]} = W^{[1]}x+b^{[1]}$$</p>
<p>$$a^{[1]}=\sigma(z^{[2]}) $$</p>
<h2 id="课时3-4"><a href="#课时3-4" class="headerlink" title="课时3.4"></a>课时3.4</h2><p>多个例子中的向量化（举例）</p>
<p>$$a^{<a href="1">2</a>}$$表示第二层神经元中的第一个样本激活值</p>
<p>数据输入X还是进行列向量堆叠，同样将Z，a都进行列向量堆叠</p>
<p>以A为例，由于是列堆叠，横向看表示不同的样本的激活值。纵向看是同一个样本在同一层中不同神经元上的激活值。纵向different hidden unions，横向different training examples。</p>
<h2 id="课时3-5"><a href="#课时3-5" class="headerlink" title="课时3.5"></a>课时3.5</h2><p>向量化的直观解释</p>
<p>多样本向量化的解释</p>
<p>把X(列堆叠成的输入向量)看成是$$A^{[0]}$$</p>
<p>然后就可以完全向量化表示了</p>
<h2 id="课时3-6"><a href="#课时3-6" class="headerlink" title="课时3.6"></a>课时3.6</h2><p>使用不同的激活函数</p>
<p>之前一直用的是$$\sigma$$激活函数，但有时候其他的激活函数的效果会更好</p>
<p>$$z^{[1]}=W^{[1]}x+b^{[1]}$$</p>
<p>$$a^{[1]} = \sigma(z^{[1]})$$</p>
<p>tanh函数总是比sigmoid函数表现好，即双曲正弦函数</p>
<p>$$tanh(z) = \frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$$</p>
<p>实际上tanh函数是sigmoid函数平移后的结果</p>
<p>tanh函数的一个好处是有数据中心化的效果</p>
<p>Ag说tanh函数几乎在任何场合下都比sigmoid函数表现优越</p>
<p> 例外是在输出层，二分类问题中可能会在输出层用到sigmoid函数来进行激活</p>
<p>有时候在激活函数上面加上上标用来表示不同层的激活函数是不同的</p>
<p>sigmoid函数和tanh函数的共同缺点是梯度可能消失</p>
<p>另外一个比较受欢迎的激活函数是纠正线性单元：ReLU，Rectifier linear unit</p>
<p>$$a = max(0,z)$$</p>
<p>讲了选择激活函数的经验法则：</p>
<p>如果输出值时0或1，即如果做二元分类，那么sigmoid函数比较适合做输出层的激活函数，然后其他单元用ReLU做激活函数。这种组合也变成了现在默认的激活函数选择方案。ReLU有一个修正版本：leaky ReLU：当z为负数时，给予一个比较小的导数，而不是令导数为0。使用ReLU的一个好处也是训练神经网络速度会更快，主要原因是ReLU没有导数接近于0这种效应，所以学习速率不会被减慢</p>
<p>Ag在这节课严重批判了sigmoid函数</p>
<h2 id="课时3-7"><a href="#课时3-7" class="headerlink" title="课时3.7"></a>课时3.7</h2><p>讲了一下为什么总是要用非线性激活函数，这个问题机器学习课老师也讲了</p>
<p>因为非线性函数激活理论上可以用来拟合任意非线性可分问题</p>
<h2 id="课时3-8"><a href="#课时3-8" class="headerlink" title="课时3.8"></a>课时3.8</h2><p>激活函数的导数</p>
<p>backpropagation</p>
<p>导数的表示：<br>$$g^{‘}(z)$$叫做g prime of z</p>
<h2 id="课时3-9"><a href="#课时3-9" class="headerlink" title="课时3.9"></a>课时3.9</h2><p>据说是激动人心的一个课时。。。</p>
<p>实现back propagation / gradient descent</p>
<p>表示：</p>
<p>参数：</p>
<p>$$n^{[0]}, n^{[1]}, n^{[2]}$$</p>
<p>分别表示输入量个数，隐藏单元个数，输出单元个数</p>
<p>损失函数cost function：</p>
<p>$$J(W^{[1]},b^{[1]}, W^{[2]}, b^{[2]}) = \frac{1}{m}\sum{L(y,\hat{y})}$$</p>
<p>训练参数就用梯度下降</p>
<p>在训练神经网络的时候，随机初始化参数很重要</p>
<p>Update:</p>
<p>$$ W^{[1]} = W^{[2]} - \alpha dW^{[1]}$$</p>
<p>$$ b^{[1]} = b^{[2]} - \alpha db^{[1]}$$</p>
<p>关于dW和db的含义，之前讲过的，就不重复了</p>
<p>正向传播：反正就是计算线性函数，计算激活值，然后从前面向后面逐层传递</p>
<p>反向传播：计算导数</p>
<p>然后np中keepdims = true这个参数的作用是防止numpy输出秩为1的矩阵：(n,)这种，而是命令它保持维度，输出(n,1)这样的</p>
<p>如果不调用keepdims参数，则要用reshape来调整维度</p>
<h2 id="课时3-10-选修"><a href="#课时3-10-选修" class="headerlink" title="课时3.10(选修)"></a>课时3.10(选修)</h2><p>看看反向传播的灵感来源</p>
<p>由流程图推导出梯度公式</p>
<p>这个视频中有一个等式写错了，当然可能是我钻牛角尖了才觉得它错了，在01:19</p>
<h2 id="课时3-11"><a href="#课时3-11" class="headerlink" title="课时3.11"></a>课时3.11</h2><p>如何初始化参数</p>
<p>随机初始化</p>
<p>全0初始化会导数symmetric的问题，无论训练时间多长，每个神经元的权重参数都是一样的，这样的话，多个隐藏神经元就没有啥用了</p>
<p>solution:</p>
<p>$$W^{[1]} = np.random.randn(…)*0.01$$</p>
<p> b不会导致对称问题，所以是可以把b初始化成0的</p>
<p>将权重乘一个很小的系数是为了防止激活函数的参数绝对值比较大，而导致梯度平缓的问题，也叫做饱和</p>
<h2 id="课时4-1"><a href="#课时4-1" class="headerlink" title="课时4.1"></a>课时4.1</h2><p><strong>看这一章的收获还是非常大的，对向量化的概念有了更加深入的理解</strong></p>
<p>深度神经网络</p>
<p>目标是把学习的综合起来，build your own deep neural network</p>
<p>做一个编程大作业</p>
<p>logistic regression —- shallow model</p>
<p>denotion: 用L表示层数</p>
<p>用$$n^{[i]}$$表示第i个层上的单元数量</p>
<p>$$a^{[l]} = g^{[l]}(z^{[l]})$$表示l层中的的激活函数</p>
<p>$$W^{[l]}$$表示</p>
<h2 id="课时4-2"><a href="#课时4-2" class="headerlink" title="课时4.2"></a>课时4.2</h2><p>深层网络中的前向传播和反向传播</p>
<p>前向传播公式：</p>
<p>$$Z^{[n+1]}=W^{[n+1]}a^{[n]}+b^{[n+1]}$$</p>
<p>$$A^{[n+1]}=g^{[n+1]}(Z^{[n]})$$</p>
<p>这边02:26的地方一个公式又写错了</p>
<p>Z是一层上单个样本的所有Z，然后所有样本竖向堆叠而成的</p>
<p>正向传播中用for loop没问题</p>
<p>防止程序bug的方法是想办法检查矩阵的维度</p>
<h2 id="课时4-3"><a href="#课时4-3" class="headerlink" title="课时4.3"></a>课时4.3</h2><p>核对矩阵的维数</p>
<p>是拿出一张纸，然后确保各个地方计算的矩阵维度没有问题</p>
<p>$$W^{[l]}$$的维度是$$(n^{[l]}, n^{[l-1]})$$</p>
<p>a和z的维度是相同的</p>
<p>聚合所有样本之后的向量化：</p>
<p>$$Z^{[1]} = W^{[1]}X+b^{[1]}$$</p>
<p>其中Z是所有小z的列堆叠，每个小z代表一个样本在一个层上所有输出的堆叠向量，W维度不变，而X变成了所有输入向量的列堆叠，b也随之广播成了列堆叠</p>
<p>和之前的比一下：</p>
<p>$$z^{[l]}$$和$$a^{[l]}$$的维度原来都是$$(n^{[l]},1)$$</p>
<p>后来变成了$$(n^{[l]},m)$$。是把所有样本进行列堆叠而成的结果</p>
<h2 id="课时4-4"><a href="#课时4-4" class="headerlink" title="课时4.4"></a>课时4.4</h2><p>为什么深度神经网络表现得比浅层神经网络好？</p>
<p>深度网络的前几层学习低层次的特征，在后面几层，就会把简单的特征结合起来，去探测更加复杂的东西</p>
<p>deep learning: great brand</p>
<h2 id="课时4-5"><a href="#课时4-5" class="headerlink" title="课时4.5"></a>课时4.5</h2><p>搭建深度网络块</p>
<p>把过程中计算得到的a，z之类的缓存起来</p>
<h2 id="课时4-6"><a href="#课时4-6" class="headerlink" title="课时4.6"></a>课时4.6</h2><p>正向传播和反向传播的具体实现过程</p>
<h2 id="课时4-7"><a href="#课时4-7" class="headerlink" title="课时4.7"></a>课时4.7</h2><p>参数和超参数</p>
<p>超参数：比如学习速率和迭代次数、隐藏层数、隐藏单元数、激活函数</p>

      
    </div>

    

    
    
    

    <div>
    
    <div>

    <div style="text-align:center;color: #ccc;font-size:14px;">-------------End of the article <i class="fa fa-paw"></i> Thanks for reading!-------------</div>

</div>
    
    </div>

    

    
       
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Programming/" rel="tag"><i class="fa fa-tag"></i> Programming</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/03/08/卷积神经网络课堂笔记/" rel="next" title="Convolutional Neural Network, Course Notes">
                <i class="fa fa-chevron-left"></i> Convolutional Neural Network, Course Notes
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/03/10/Algorithms-Part-1/" rel="prev" title="Algorithms, Part 1">
                Algorithms, Part 1 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Qingliu</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          

          
          

          
            
          
          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qingliu</span>

  

  
</div>



<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
    Number of visitors: <span id="busuanzi_value_site_uv"></span>
</span>
</div>


<!--
<span class="post-meta-divider">|</span>









-->

<span class="post-meta-divider">|</span>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">Total 63.6k words</span>
</div>

        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  
    <script type="text/javascript" src="//jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="//velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="//velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="//src/utils.js?v=6.5.0"></script>

  <script type="text/javascript" src="//src/motion.js?v=6.5.0"></script>



  
  

  
  <script type="text/javascript" src="//src/scrollspy.js?v=6.5.0"></script>
<script type="text/javascript" src="//src/post-details.js?v=6.5.0"></script>



  


  <script type="text/javascript" src="//src/bootstrap.js?v=6.5.0"></script>



  



  










  





  

  

  

  

  

  
  

  

  

  

  

  

  


</body>
</html>
