<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.5.0" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.5.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.5.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.5.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.5.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '6.5.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Stanford University, Machine Learning Course Notes">
<meta name="keywords" content="Programming,Machine Learning,Matlab">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning">
<meta property="og:url" content="http://yoursite.com/2019/03/29/Machine Learning/index.html">
<meta property="og:site_name" content="Qingliu">
<meta property="og:description" content="Stanford University, Machine Learning Course Notes">
<meta property="og:locale" content="en">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-1.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-2.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-3.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-03-31-Week1-4.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-03-31-Week1-6.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-03-31-Week1-7.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-01-Week1-7.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-03-Week2-1.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-03-Week2-2.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-03-Week2-3.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-05-Week3-1.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-05-Week3-2.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-06-Week3-3.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-06-Week3-4.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-06-Week3-5.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-06-Week3-6.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-08-Week4-1.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-08-Week4-2.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-08-Week4-3.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-08-Week4-4.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-08-Week4-5.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-09-Week6-1.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-09-Week6-2.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-09-Week6-3.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-09-Week6-4.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-09-Week6-6.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-10-Week7-1.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-10-Week7-2.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-10-Week7-3.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-10-Week7-4.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-10-Week7-5.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week8-1.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week8-2.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week8-3.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week8-4.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week8-5.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-1.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-2.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-4.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-5.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-6.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-7.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-8.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-9.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-10.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-11.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-12.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-13.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-14.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week10-1.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week10-2.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week10-3.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week10-4.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week10-5.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week11-1.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week11-2.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week11-3.png">
<meta property="og:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week11-4.png">
<meta property="og:updated_time" content="2019-05-01T22:45:33.778Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Machine Learning">
<meta name="twitter:description" content="Stanford University, Machine Learning Course Notes">
<meta name="twitter:image" content="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-1.png">






  <link rel="canonical" href="http://yoursite.com/2019/03/29/Machine Learning/">



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Machine Learning | Qingliu</title>
  











  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

<a href="https://github.com/Aden-Q" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Qingliu</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>Search</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/29/Machine Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Qingliu">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Qingliu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Machine Learning
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-03-29 15:03:34" itemprop="dateCreated datePublished" datetime="2019-03-29T15:03:34-05:00">2019-03-29</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-05-01 17:45:33" itemprop="dateModified" datetime="2019-05-01T17:45:33-05:00">2019-05-01</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Note/" itemprop="url" rel="index"><span itemprop="name">Note</span></a></span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2019/03/29/Machine Learning/" class="leancloud_visitors" data-flag-title="Machine Learning">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Heat: </span>
               
                 <span class="leancloud-visitors-count"></span>
                 <span>℃</span>
             </span>
          

          
            <span class="post-meta-divider">|</span>
            <span class="post-meta-item-icon">
            <i class="fa fa-eye"></i>
             Views:  
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Stanford University, Machine Learning</p>
<p>Course Notes</p>
<a id="more"></a>
<h1 id="Week-1"><a href="#Week-1" class="headerlink" title="Week 1"></a>Week 1</h1><p>Facebook, Apple’s photo application.</p>
<p>Google’s page rank algorithm.</p>
<p>Email spam filter.</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Algorithms, math and how to get them work.</p>
<p>Machine Learning:</p>
<ul>
<li>Grew out of work in AI</li>
<li>New capability for computers</li>
</ul>
<p>Examples:</p>
<ul>
<li>Database mining, E.g., Web click data, medical records, biology, engineering.</li>
<li>Applications can’t program by hand. Helicopter, handwriting recognition, most of NLP, CV.</li>
<li>Self-customizing programs: E.g., Amazon, Netflix product recommendations.</li>
<li>Understanding human learning (brain, real AI).</li>
</ul>
<blockquote>
<p>  Arthur Samuel (1959). Machine Learning: Field of study that gives computers the ability to learn without being explicitly programmed.</p>
</blockquote>
<blockquote>
<p>  Tom Mitchell (1998). Well-posed Learnining Problem: A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.</p>
</blockquote>
<p>Machine learning algorithms:</p>
<ul>
<li>Supervised learning</li>
<li>Unsupervised learning</li>
</ul>
<p>Others: Reinforcement learning, recommender systems.</p>
<p>Supervised learning</p>
<hr>
<p>E.g., housing price prediction (regression)</p>
<center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-1.png" style="zoom:60%"><br></center>


<p>E.g., Breast cancer (malignant, benign) (classification)</p>
<center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-2.png" style="zoom:60%"><br></center>



<p>Unsupervised learning</p>
<hr>
<p>Clustering</p>
<center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-3.png" style="zoom:60%"><br></center>

<p>Applications:</p>
<ul>
<li>Organize computing clusters</li>
<li>Social network analysis</li>
<li>Market segmentation</li>
<li>Astronomical data analysis</li>
</ul>
<p>E.g.</p>
<p>Cocktail party algorithm</p>
<h2 id="Linear-Regression-with-One-Variable"><a href="#Linear-Regression-with-One-Variable" class="headerlink" title="Linear Regression with One Variable"></a>Linear Regression with One Variable</h2><h3 id="Linear-Regression-with-One-Variable-1"><a href="#Linear-Regression-with-One-Variable-1" class="headerlink" title="Linear Regression with One Variable"></a>Linear Regression with One Variable</h3><p>Housing Prices.</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-31-Week1-4.png" style="zoom:60% /"><br></center>

<p>Traing set: housing prices</p>
<p>Notation:</p>
<ul>
<li>m = Number of training examples</li>
<li>x’s = “input” variable / features</li>
<li>y’s = “output” variable / “target” variable</li>
</ul>
<p>(x, y) – one training example</p>
<p>$(x^{(i)}, y^{(i)})$ – $i^{th}$ trainining example</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-31-Week1-6.png" style="zoom:60%"><br></center>



<p>$$<br>h_\theta(x)=\theta_0+\theta_1x<br>$$<br>Linear regression with one variable.</p>
<p>Univariate linear regerssion.</p>
<p>Cost function</p>
<hr>
<p>Idea: Choose $\theta_0, \theta_1$ so that $h_\theta(x)$ is close to $y$ for our training examples $(x, y)$</p>
<p>square error cost function:<br>$$<br>J(\theta_0,\theta_1)=\frac1{2m}\sum_1^m(h_\theta(x^{(1)})-y^{(1)})^2<br>$$<br>Target: choose $\theta_0, \theta_1 $ to minimize $J(\theta_0,\theta_1)$</p>
<p>Contour plots to show 3D surface:</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-31-Week1-7.png" style="zoom:60%"><br></center>






<p>Gradient descent</p>
<hr>
<p>Have some function $J(\theta_0,\theta_1)$</p>
<p>Want $min_{\theta_0,\theta_1}J(\theta_0,\theta_1)$</p>
<p><strong>Outline:</strong></p>
<ul>
<li><p>Start with some $\theta_0, \theta_1 $</p>
</li>
<li><p>Keep changing $\theta_0, \theta_1$ to reduce $J(\theta_0,\theta_1) $ until we hopefully end up at a minimum</p>
</li>
</ul>
<p><strong>Gradient descent algorithm:</strong></p>
<p>repeat until convergence:<br>$$<br>\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)<br>$$<br>Note: Correct implementation is simultaneous update as following:<br>$$<br>temp0 :=\theta_0-\alpha\frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)<br>\\<br>temp1 :=\theta_1-\alpha\frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1)<br>\\<br>\theta_0:=temp0<br>\\<br>\theta_1:=temp1<br>$$</p>
<ul>
<li>:=    assignment</li>
<li>=         assertion</li>
<li>$\alpha$        step length</li>
</ul>
<p>If $\alpha$ is too small, gradient descent can be slow.</p>
<p>If $\alpha$ is too large, gradient descent can overshoot the minimum. It may fail to converge, or even diverge.</p>
<p>Gradient descent can converge to a local minimum, even with the learning rate $\alpha$ fixed.</p>
<p>As we approach a local minimum, gradient descent will automatically take smaller steps. So, no need to decrease $\alpha$ over time.</p>
<p>Gradient for linear regression:<br>$$<br>j = 0 : \frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1) = \frac1m\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})<br>\\<br>j = 1 : \frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1) = \frac1m\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})\cdot x^{(i)}<br>$$<br>Note again: update $\theta_0$ and $\theta_1$ simultaneously.</p>
<p>Gradient descent always works with convex function (without local optimum).</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-01-Week1-7.png" style="zoom:60%"><br></center>



<p>Batch Gradient Descent:</p>
<p>Batch: Each step of gradient descent uses all the training examples. (Refer to the fact that the cost function is over the entire training set.)</p>
<h2 id="Linear-Algebra-Review"><a href="#Linear-Algebra-Review" class="headerlink" title="Linear Algebra Review"></a>Linear Algebra Review</h2><p><strong>Matrix:</strong> Rectangular array of numbers:<br>$$<br>\begin {pmatrix}<br>1402 &amp; 191 \\<br>1371 &amp; 821<br>\end {pmatrix}<br>$$<br>Dimension of matrix: number of rows x number of columns</p>
<p>$A_{ij}$ = “$i,j$ entry” in the $i^{th}$ row, $j^{th}$ column.</p>
<p><strong>Vector:</strong> An nx1 matrix.<br>$$<br>\begin {pmatrix}<br>1402 \\<br>901<br>\end {pmatrix}<br>$$<br>$y_i$ = $i^{th}$ element</p>
<p>1-indexed or 0-indexed<br>$$<br>\begin {pmatrix}<br>y_1 \\<br>y_2 \\<br>y_3 \\<br>y_4<br>\end {pmatrix}<br>\space \space<br>\begin {pmatrix}<br>y_0 \\<br>y_1 \\<br>y_2 \\<br>y_3<br>\end {pmatrix}<br>$$</p>
<p>Convention: upper case to refer to matrics and lower case to refer to numbers or vectors.</p>
<p><strong>Matrix Addition</strong></p>
<p>Element-wise addition.</p>
<p>Legal matrix addtion requires matrixes with same dimension.</p>
<p><strong>Scalar Multiplication</strong></p>
<p>$$<br>3 *<br>\begin {bmatrix}<br>1 &amp; 0 \\<br>2 &amp; 5 \\<br>3 &amp; 1<br>\end {bmatrix}<br>=<br>\begin {bmatrix}<br>3 &amp; 0 \\<br>6 &amp; 15 \\<br>9 &amp; 3<br>\end {bmatrix}<br>$$</p>
<p><strong>Matrix Multiplication: </strong></p>
<p>Matrix multiply vector: To get $y_i$, multiply A’s $i^{th} $ row with elements of vector$x$, and add them up.</p>
<p>Prediction = DataMatrix * parameters</p>
<p>Matrix-matrix multiplication:</p>
<p>$$<br>\begin {bmatrix}<br>1 &amp; 3 &amp; 2 \\<br>4 &amp; 0 &amp; 1<br>\end {bmatrix}</p>
<p>\begin {bmatrix}<br>1 &amp; 3 \\<br>0 &amp; 1 \\<br>5 &amp; 2<br>\end {bmatrix}<br>=<br>\begin {bmatrix}<br>11 &amp; 10 \\<br>9 &amp; 14<br>\end {bmatrix}<br>$$</p>
<p><strong>Details:</strong>  </p>
<ul>
<li>A m$\times $n matrix</li>
<li>B n$\times $o matrix</li>
<li>m$\times $o matrix</li>
</ul>
<p>The $i^{th}$ column of the matrix $C$ is obtained by multiplying $A$ with the $i^{th}$ column of $B$. (for $i$ = 1,2,…,o). Then convert to matrix-vector multiplication.</p>
<p>Multiple compeing hypotheses: let each hypotheses corespond to a column in the second matrix.</p>
<p>Let $A$ and $B$ be matrices. Then in general,<br>$$<br>A \times B \ne B \times A<br>$$</p>
<p>$$<br>A \times (B \times C) = (A \times B) \times C<br>$$</p>
<p><strong>Identity Matrix:</strong></p>
<p>Denoted $I$ (or $I_{n\times n}$)</p>
<p>Examples of identity matrices:<br>$$<br>\begin {bmatrix}<br>1 &amp; 0 \\<br>0 &amp; 1<br>\end {bmatrix}<br>\<br>\begin {bmatrix}<br>1 &amp; 0 &amp; 0 \\<br>0 &amp; 1 &amp; 0 \\<br>0 &amp; 0 &amp; 1<br>\end {bmatrix}<br>\<br>\begin {bmatrix}<br>1 &amp; 0 &amp; 0 &amp; 0 \\<br>0 &amp; 1 &amp; 0 &amp; 0 \\<br>0 &amp; 0 &amp; 1 &amp; 0 \\<br>0 &amp; 0 &amp; 0 &amp; 1<br>\end {bmatrix}<br>$$<br>For any matrix $A$<br>$$<br>A \cdot I = I \cdot A = A<br>$$</p>
<p><strong>Matrix inverse:</strong></p>
<p>Not all numbers have an inverse.</p>
<p>If $A$ is an m$\times $m matrix (square matrix), and if it has an inverse,<br>$$<br>AA^{-1}=A^{-1}A=I<br>$$<br>Matrices that don’t have an inverse are “singular” or “degenerate”.</p>
<p><strong>Matrix Transpose:</strong></p>
<p>Example:</p>
<p>$$<br>A =<br>\begin {bmatrix}<br>1 &amp; 2 &amp; 0 \\<br>3 &amp; 5 &amp; 9<br>\end {bmatrix}</p>
<p>\</p>
<p>A^T =<br>\begin {bmatrix}<br>1 &amp; 3 \\<br>2 &amp; 5 \\<br>0 &amp; 9<br>\end {bmatrix}<br>$$</p>
<p>Let $A$ be an m$\times $n matrix, and let $B=A^T$. Then $B$ is an n$\times $m matrix, and<br>$$<br>B_{ij}=A_{ji}<br>$$</p>
<h1 id="Week-2"><a href="#Week-2" class="headerlink" title="Week 2"></a>Week 2</h1><h2 id="Linear-Regression-with-Multiple-Variables"><a href="#Linear-Regression-with-Multiple-Variables" class="headerlink" title="Linear Regression with Multiple Variables"></a>Linear Regression with Multiple Variables</h2><p>Multiple features (variables).</p>
<hr>
<p>Notation:</p>
<p>$n$ = number of features</p>
<p>$x^{(i)}$ = input (features) of $i^{th}$ training example.</p>
<p>$x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example.</p>
<p>Hypothesis:<br>$$<br>h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+….+\theta_nx_n<br>$$<br>For convenience of notation, define $x_0=1$. ($x_0^{(i)}=1$)<br>$$<br>x =<br>\begin {bmatrix}<br>x_0 \\<br>x_1 \\<br>… \\<br>x_n<br>\end {bmatrix}</p>
<p>\space<br>\space</p>
<p>\theta =<br>\begin {bmatrix}<br>\theta_0 \\<br>\theta_1 \\<br>… \\<br>\theta_n<br>\end {bmatrix}<br>\in<br>R^{n+1}<br>$$</p>
<p>$$<br>h_\theta(x)=\theta_0x_0+\theta_1x_1+\theta_2x_2+….+\theta_nx_n<br>=\theta^Tx<br>$$</p>
<p>Multivariate linear regression.</p>
<p>Gradient descent for multivariate linear regression</p>
<hr>
<p>$$<br>\theta_j:=\theta_j-\alpha \frac{\partial}{\partial\theta_j}J(\theta)<br>$$</p>
<p>$$<br>\theta_j:=\theta_j-\alpha \frac{1}{m}\sum_{i=1}^m<br>(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}<br>$$</p>
<p>Feature Scaling</p>
<hr>
<p>Idea: Make sure features are on a similar scale.</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-03-Week2-1.png" style="zoom:60%"><br></center>

<p>Get every feature into approximately a $-1\leq x_i \leq 1$ range.</p>
<p><strong>Mean normalization</strong></p>
<p>Replace $x_i$ with $x_i-\mu_i$ to make features have approximately zero mean (Do not apply to $x_0=1$) and then divide it by its standard deviatioin (divide by max-min is also fine).</p>
<p>Convergence test:</p>
<p>Declare convergence if $J(\theta)$ decreases by less than $10^{-3}$ in one iteration.</p>
<ul>
<li>For sufficiently small $\alpha$, $J(\theta )$ should decrease on every iteration.</li>
<li>But if $\alpha$ is too small, gradient descent can be slow to converge.</li>
</ul>
<p><strong>Summary</strong></p>
<ul>
<li>If $\alpha$ is too small: slow convergence.</li>
<li>If $\alpha$ is too large: $J(\theta) $ may not decrease on every iteration; may not converge.</li>
</ul>
<p>To choose $\alpha$ , try: …, 0.001, 0.01, 0.1, 1, … (0.003, 0.03, 0.3), 3x each time.</p>
<p><strong>Polynomial regression</strong></p>
<p>Sometimes by defining new features we can get better data and model.</p>
<p>We can transform a polynomial regression into a mutlivariant linear regression model:</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-03-Week2-2.png" style="zoom:60%"><br></center>



<p><strong>Normal Equation</strong></p>
<p>Method to solve for $\theta$ analytically.</p>
<p>Design matrix: each row represents a sample.</p>
<p>With normal equation, feature scaling is not necessary.</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-03-Week2-3.png" style="zoom:60%"><br></center>



<p><strong>Comparasion</strong></p>
<p>Gradient Descent:</p>
<ul>
<li>Need to choose $\alpha$.</li>
<li>Needs many iterations.</li>
<li>Works well even when $n$ is large.</li>
</ul>
<p>Normal Equation:</p>
<ul>
<li>No need to choose $\alpha$.</li>
<li>Don’t need to iterate.</li>
<li>Need to compute $(X^TX)^{-1}$</li>
<li>Slow if $n$ is very large. (depending on the number of features, typically n ~ 1000)</li>
</ul>
<p><strong>Noninvertibility when using normal equation</strong></p>
<p>For normal equation method:<br>$$<br>\theta = (X^TX)^{-1}X^Ty<br>$$<br>What if $X^TX$ is non-invertible? (Singular/degenerate)</p>
<ul>
<li>pinv</li>
<li>inv</li>
</ul>
<p>What causes $X^TX$ non-invertible?</p>
<ul>
<li>Redundant features (linearly dependent).</li>
<li>Too many features (e.g. $m\leq n$). (delete some features, or use regularization).</li>
</ul>
<h2 id="Octave-Matlab-Tutorial"><a href="#Octave-Matlab-Tutorial" class="headerlink" title="Octave/Matlab Tutorial"></a>Octave/Matlab Tutorial</h2><p>Choices: Octave, MATLAB, Python Numpy and R.</p>
<ul>
<li>who: list all the variables.</li>
<li>whos: list all the variables in details.</li>
<li>max(A(:)): take the maximum element of matrix A.</li>
<li>disp(“…”): print a string.</li>
</ul>
<p>Function:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">function y = squareThisNumber(x)</span><br><span class="line">y = x^2;</span><br></pre></td></tr></table></figure>
<ul>
<li>addpath(): add search path.</li>
</ul>
<p>Vectorization</p>
<hr>
<p>$$<br>h_\theta(x)=\sum_{j=0}^n\theta_jx_j=\theta^Tx<br>$$</p>
<p>$$<br>\theta =<br>\begin {bmatrix}<br>\theta_0 \\<br>\theta_1 \\<br>\theta_2<br>\end {bmatrix}<br>\<br>x =<br>\begin {bmatrix}<br>x_0 \\<br>x_1 \\<br>x_2<br>\end {bmatrix}<br>$$</p>
<p>Unvectorized implementation:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">prediction = 0.0;</span><br><span class="line">for j = 1:n+1</span><br><span class="line">	prediction = prediction + theta(j) * x(j)</span><br><span class="line">end;</span><br></pre></td></tr></table></figure>
<p>Vectorized implementation:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prediction = theta&apos; * x;</span><br></pre></td></tr></table></figure>
<h2 id="Programming-Assignment-Linear-Regression"><a href="#Programming-Assignment-Linear-Regression" class="headerlink" title="Programming Assignment: Linear Regression"></a>Programming Assignment: Linear Regression</h2><p><a href="https://github.com/Aden-Q/Machine-Learning/tree/master/Week2" target="_blank" rel="noopener">Linear Regression</a></p>
<h1 id="Week-3"><a href="#Week-3" class="headerlink" title="Week 3"></a>Week 3</h1><h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><p><strong>Classification</strong></p>
<ul>
<li>Email: Spam / Not Spam?</li>
<li>Online Transactions: Fraudulent (Yes/ No)?</li>
<li>Tumor: Malignant / Benign?</li>
</ul>
<p>$$<br>y \in {0,1}<br>$$</p>
<p>0: “Negative Class”</p>
<p>1: “Positive Class”</p>
<p>Binary Classification by Linear Regression is easily affected by extreme value.</p>
<p><strong>Logistic Regression:</strong><br>$$<br>0 \leq h_\theta(x) \leq 1<br>$$<br>Hypothesis:<br>$$<br>h_\theta(x) = g(\theta^Tx)<br>\\<br>g(z) = \frac{1}{1+e^{-z}}<br>$$<br>g is called <strong>sigmoid function</strong> or <strong>logistic function</strong>.<br>$$<br>h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}<br>$$<br><strong>Interpretation of Hypothesis Output</strong></p>
<p>$h_\theta(x)$ = estimated probability that y = 1 on input x<br>$$<br>h_\theta(x)=P(y=1|x;\theta)<br>$$<br>“Probability that y = 1, given x, parameteruzed by $\theta $”<br>$$<br>P(y=0|x;\theta)+P(y=1|x;\theta)=1<br>$$<br><strong>Decision Boundary</strong></p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-05-Week3-1.png" style="zoom:60%"><br></center>

<p><strong>Cost Function</strong></p>
<p>If we use the same cost function as before, it would be non-convex. Thus it’s not a wise choice. So we tend to choose a convex function.</p>
<p>Logistic regression cost function:<br>$$<br>Cost(h_\theta(x),y)=<br>\begin{equation}<br>\left{<br>             \begin{array}{lr}<br>             -log(h_\theta(x)), &amp; if \space y =1  \\<br>             -log(1-h_\theta(x)), &amp;  if \space y = 0<br>             \end{array}<br>\right.<br>\end{equation}<br>$$</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-05-Week3-2.png" style="zoom:60%"><br></center>

<p>$$<br>Cost(h_\theta(x),y)=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))<br>$$</p>
<p>$$<br>J(\theta)=-\frac1m[\sum_{i=1}^my^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]<br>$$</p>
<p><strong>Gradient descent</strong><br>$$<br>\theta_j:=\theta_j - \alpha\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}<br>$$<br>It looks identical to linear regression! (only hypothesis changes)</p>
<p><strong>Advanced optimization</strong></p>
<ul>
<li>Conjugate gradient</li>
<li>BFGS</li>
<li>L-BFGS</li>
</ul>
<p>Advantages:</p>
<ul>
<li>No need to manually pick $\alpha $</li>
<li>Often faster than gradient descent</li>
</ul>
<p>Disadvantages:</p>
<ul>
<li>More complex</li>
</ul>
<p><strong>Multiclass classification</strong></p>
<p>Email foldering/tagging: Work, Friends, Family, Hobby</p>
<p>Medical diagrams: Nol ill, Cold, Flu</p>
<p>Weather: Sunny, Cloudy, Rain, Snow</p>
<p><strong>One-vs-all (one-vs-rest):</strong></p>
<p>Treat one multiple class problem as several binary classification problems.</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-06-Week3-3.png" style="zoom:60%"><br></center>

<p>Train a logistic regression classifier $h_\theta^{(i)}(x)$ for each class $i$ to predict the probability that $y=i$.</p>
<p>On a new input $x$, to make a prediction, pick the class $i$ that maximizes<br>$$<br>max\space h_\theta^{(i)}(x)<br>$$<br>Which means the prediction takes the maximum probability the sample belongs to.</p>
<h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-06-Week3-4.png" style="zoom:60%"><br></center>



<p>Underfit: High bias</p>
<p>Overfit: High variance</p>
<p><strong>Overfitting:</strong> If we have too many features, the learned hypothesis may fit the training set very well, but fail to generalize to new examples (predict prices on new examples).</p>
<p><strong>Addressing overfitting:</strong></p>
<ul>
<li>Reduce number of features<ul>
<li>Manually select which features to keep.</li>
<li>Model selection algorithm.</li>
</ul>
</li>
<li>Regularization<ul>
<li>Keep all the features, but reduce magnitude/values of parameters $\theta_j$.</li>
<li>Works well when we have a lot of features, each of which contributes a bit to predicting $y$.</li>
</ul>
</li>
</ul>
<p><strong>Regularization</strong></p>
<p>Small values for parameters $\theta_0,\theta_1,…,\theta_n$</p>
<ul>
<li>Simpler hypothesis</li>
<li>Less prone to overfitting</li>
</ul>
<p>$$<br>J(\theta)=\frac1{2m}[\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\lambda \sum_{i=1}^m\theta_j^2]<br>$$</p>
<p>$\lambda$: regularization parameter, control the trade off between fitting the training set well and keeping parameters small (keeping hypothesis simple)</p>
<p>If $\lambda $ is extremely large, then $h_\theta(x)=\theta_0$. (end up with the simplest hypothesis and high bias)</p>
<p><strong>Gradient descent with regularization</strong><br>$$<br>\theta_0:=\theta_0-\alpha \frac1m<br>\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}<br>$$</p>
<p>$$<br>\theta_j:=\theta_j-\alpha [\frac1m<br>\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac\lambda m\theta_j], \space for \space j = 1, 2, …, n<br>$$</p>
<p>Equivalent:<br>$$<br>\theta_j:=\theta_j(1-\alpha\frac\lambda m)-\alpha \frac1m<br>\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}<br>$$</p>
<p><strong>Normal equation with normalization</strong></p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-06-Week3-5.png" style="zoom:60%"><br></center>



<p><strong>Regularized logistic regression</strong></p>
<p>Cost function:<br>$$<br>J(\theta) = -[\frac1m \sum_{i=1}^my^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]+<br>\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2<br>$$</p>
<p><strong>Regularized advanced optimization</strong></p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-06-Week3-6.png" style="zoom:60%"><br></center>







<h2 id="Programming-Assignment-Logistic-Regression"><a href="#Programming-Assignment-Logistic-Regression" class="headerlink" title="Programming Assignment: Logistic Regression"></a>Programming Assignment: Logistic Regression</h2><p><a href="https://github.com/Aden-Q/Machine-Learning/tree/master/Week3" target="_blank" rel="noopener">Logistic Regression</a></p>
<p>One way to fit the data better is to create more features from each data point.</p>
<p>Logistic classifier prones to be overfitting  on higher-dimension features. (consider the decision boundary)</p>
<h1 id="Week-4"><a href="#Week-4" class="headerlink" title="Week 4"></a>Week 4</h1><h2 id="Neural-Networks-Representation"><a href="#Neural-Networks-Representation" class="headerlink" title="Neural Networks: Representation"></a>Neural Networks: Representation</h2><p>Linear regression and logistic regression are not designed for Non-linear Classification problems and multiple features.</p>
<p><strong>Neural Networks</strong></p>
<p>Origins: Algorithms that try to mimic the brain.</p>
<p>Was very widely used in 80s and early 90s; popularity diminished in late 90s.</p>
<p>Recent resurgence: State-of-the-art technique for many applications</p>
<p><strong>Neuron model: Logistic unit</strong></p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-08-Week4-1.png" style="zoom:60%"><br></center>

<p>Sigmoid (logistic) activation function.<br>$$<br>g(z) = \frac{1}{1+e^{-z}}<br>$$<br>$\theta $: “weights”</p>
<p><strong>Neural Network</strong></p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-08-Week4-2.png" style="zoom:60%"><br></center>



<p>$a_i^{(j)}$ = “activation” of unit $i$ in layer $j$</p>
<p>$\theta^{(j)}$ = matrix of weights controlling function mapping from layer $j$ to layer $j+1$</p>
<p>If network has $s_j$ units in layer $j$, $s_{j+1}$ units in layer $j+1$, then $\theta^{(j)}$ will be of dimension $s_{j+1}*(s_j+1)$. The extra dimension represents the bias.</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-08-Week4-3.png" style="zoom:60%"><br></center>



<p><strong>Forward propagation: Vectorized implementation</strong></p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-08-Week4-4.png" style="zoom:60%"><br></center>


<p><strong>Neural Network learning its own features</strong></p>
<p>“Architectures” refers to how the different neurons are connected to each other.</p>
<p><strong>Handwritten digit classification</strong></p>
<p><strong>Multi-class Classification</strong></p>
<p>Multiple output units: One-vs-all.</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-08-Week4-5.png" style="zoom:60%"><br></center>




<h2 id="Programming-Assignment-Multi-class-Classification-and-Neural-Networks"><a href="#Programming-Assignment-Multi-class-Classification-and-Neural-Networks" class="headerlink" title="Programming Assignment: Multi-class Classification and Neural Networks"></a>Programming Assignment: Multi-class Classification and Neural Networks</h2><p><a href>Multile-class Classification and Neural Networks</a></p>
<h1 id="Week-5"><a href="#Week-5" class="headerlink" title="Week 5"></a>Week 5</h1><h2 id="Neural-Networks-Learning"><a href="#Neural-Networks-Learning" class="headerlink" title="Neural Networks: Learning"></a>Neural Networks: Learning</h2><p>$L$ = total no. of layers in network</p>
<p>$s_l$ = no. of units (not counting bias unit) in layer $l$</p>
<p><strong>Binary classification:</strong> $y$ = 0 or 1, 1 output unit</p>
<p><strong>Multi-class classification (K classes):</strong> $y \in R^K$</p>
<p><strong>Cost function</strong></p>
<p><strong>Gradient computation</strong></p>
<p>Given one training example $(x,y)$:</p>
<p>Forward propagation:<br>$$<br>a^{(1)} = x \\<br>z^{(2)}=\theta^{(1)}a^{(1)} \\<br>a^{(2)}=g(z^{(2)}) \space (add \space a_0^{(2)}) \\<br>z^{(3)}=\theta^{(2)}a^{(2)} \\<br>a^{(3)}=g(z^{(3)}) \space (add \space a_0^{(3)}) \\<br>z^{(4)}=\theta^{(3)}a^{(3)} \\<br>a^{(4)}=h_\theta(x)=g(z^{(4)})<br>$$<br><strong>Backpropagation algorithm</strong></p>
<p>Intuition: $\delta_j^{(l)}$ = “error” of node $j$ in layer $l$.<br>$$<br>\delta^{(4)} = a^{(4)}-y<br>$$</p>
<p>$$<br>\delta^{(3)}=(\theta^{(3)})^T\delta^{(4)}.*g’(z^{(3)})<br>$$</p>
<p>$$<br>\delta^{(2)}=(\theta^{(2)})^T\delta^{(3)}.*g’(z^{(2)})<br>$$</p>
<p>$$<br>\frac{\partial}{\partial\theta_{ij}^{(l)}}J(\theta) = a_j^{(l)}\delta_i^{(l+1)}<br>$$<br>Training set ${   (x^{(1)},y^{(1)}),…, (x^{(m)},y^{(m)})}$</p>
<p>Set $\Delta_{ij}^{(l)}=0 $ for all $l,i,j$.</p>
<p>For $i$ = 1 to $m$</p>
<p>​    Set $a^{(1)}=x^{(i)}$</p>
<p>​    Perform forward propagation to compute $a^{(l)}$ for $l$ = 2,3,…,L</p>
<p>​    Using $y^{(i)}$, compute $\delta^{(L)}=a^{(L)}-y^{(i)}$</p>
<p>​    Compute $\delta^{(L-1)},\delta^{(L-2)},…,\delta^{(2)}$    // backpropagation step</p>
<p>​    $\Delta_{ij}^{(l)}:=\Delta_{ij}^{(l)}+a_j^{(l)}\delta_i^{(l+1)}$<br>$$<br>D_{ij}^{(l)}:=\frac1m\Delta_{ij}^{(l)}+\lambda\theta_{ij}^{(l)} \space if \space j \neq0 \\<br>D_{ij}^{(l)}:=\frac1m\Delta_{ij}^{(l)} \space if \space j \ =0<br>$$</p>
<p><strong>Random initialization: Symmetry breaking</strong></p>
<p><strong>Training a neural network</strong></p>
<p>Pick a network architecture (connectivity pattern between neurons)</p>
<p>No. of input units: Dimension of features $x^{(i)}$</p>
<p>No. output units: Number of classes</p>
<p>Reasonable default: 1 hidden layer, or if &gt;1 hidden layer, have same no. of hidden units in every layer (usually the more the better)</p>
<ol>
<li>Randomly initialize weights</li>
<li>Implement forward propagation to get $h_\theta(x^{(i)})$ for any $x^{(i)}$</li>
<li>Implement code to compute cost function $J(\theta)$</li>
<li>Implement backprop to compute partial derivatives $\frac{\partial}{\partial\theta_{jk}^{(l)}}J(\theta)$</li>
<li>Use gradient checking to compare $\frac{\partial}{\partial\theta_{jk}^{(l)}}J(\theta)$ using backpropagation vs. using numerical estimate of gradient of $J(\theta)$, Then disable gradient checking code.</li>
<li>Use gradient descent or advanced optimization method with backpropagation to try to minimize $J(\theta)$ as a function of parameters $\theta$</li>
</ol>
<h2 id="Programming-Assignment-Neural-Network-Learning"><a href="#Programming-Assignment-Neural-Network-Learning" class="headerlink" title="Programming Assignment: Neural Network Learning"></a>Programming Assignment: Neural Network Learning</h2><p><a href="https://github.com/Aden-Q/Machine-Learning/tree/master/Week5" target="_blank" rel="noopener">Neural Network Learning</a></p>
<h1 id="Week-6"><a href="#Week-6" class="headerlink" title="Week 6"></a>Week 6</h1><h2 id="Advice-for-Applying-Machine-Learning"><a href="#Advice-for-Applying-Machine-Learning" class="headerlink" title="Advice for Applying Machine Learning"></a>Advice for Applying Machine Learning</h2><p><strong>Debuggin a learning algorithm:</strong></p>
<ul>
<li>Get more training examples</li>
<li>Try smaller sets of features</li>
<li>Try getting additional features</li>
<li>Try adding polynomial features $(x_1^2,x_2^2,x_1x_2,etc)$</li>
<li>Try decreasing $\lambda$</li>
<li>Try increasing $\lambda$</li>
</ul>
<p><strong>Evaluating your hypothesis</strong></p>
<p>Training set/ Test set split, typically 70% vs. 30% (remember to randomly shuffle)</p>
<ul>
<li>Learn parameter $\theta$ from training data (minimizing training error $J(\theta)$)</li>
<li>Compute test set error</li>
<li>Misclassification error (0/1 misclassification error)</li>
</ul>
<p><strong>Model selection</strong></p>
<p>Training set, cross validation set and test set split. Typically 60%, 20% and 20%</p>
<p>Select by validation set, and finally test on test set. Note that we should not select by test set. (Means that we fit an extra parameter to the validation set)</p>
<p><strong>Bias and variance</strong></p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-09-Week6-1.png" style="zoom:60%"><br></center>



<p><strong>Choosing the regularization parameter $\lambda$</strong></p>
<ol>
<li>Try $\lambda=0$</li>
<li>Try $\lambda=0.01$</li>
<li>Try $\lambda=0.02$</li>
<li>Try $\lambda=0.04$</li>
<li>…</li>
<li>Try $\lambda=10.24$</li>
</ol>
<p>Steps of choosing $\lambda $:</p>
<ol>
<li>Create a list of lambdas (i.e. λ∈{0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24});</li>
<li>Create a set of models with different degrees or any other variants.</li>
<li>Iterate through the \lambda<em>λ</em>s and for each \lambda<em>λ</em> go through all the models to learn some Θ.</li>
<li>Compute the cross validation error using the learned Θ (computed with λ) on the JCV(Θ) <strong>without</strong> regularization or λ = 0.</li>
<li>Select the best combo that produces the lowest error on the cross validation set.</li>
<li>Using the best combo Θ and λ, apply it on Jtest(Θ) to see if it has a good generalization of the problem.</li>
</ol>
<p><strong>Learning curves</strong></p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-09-Week6-2.png" style="zoom:60%"><br></center>


<p><strong>High bias</strong></p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-09-Week6-3.png" style="zoom:60%"><br></center>

<p>If a learning algorithm is suffering from high bias, getting more training data will not (by itself) help much.</p>
<p><strong>High variance</strong></p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-09-Week6-4.png" style="zoom:60%"><br></center>

<p>If a learning algorithm is suffering from high variance, getting more training data is likely to help.</p>
<p>“Small” neural network (fewer parameters; more prone to underfitting): Computationally cheaper</p>
<p>“Large” neural network (more parameters; more prone to overfitting): Computationally more expensive</p>
<h2 id="Programming-Assignment-Regularized-Linear-Regression-and-Bias-Variance"><a href="#Programming-Assignment-Regularized-Linear-Regression-and-Bias-Variance" class="headerlink" title="Programming Assignment: Regularized Linear Regression and Bias/Variance"></a>Programming Assignment: Regularized Linear Regression and Bias/Variance</h2><p><a href="https://github.com/Aden-Q/Machine-Learning/tree/master/Week6" target="_blank" rel="noopener">Regularized Linear Regression and Bias/Variance</a></p>
<h2 id="Machine-Learning-System-Design"><a href="#Machine-Learning-System-Design" class="headerlink" title="Machine Learning System Design"></a>Machine Learning System Design</h2><p><strong>Building a spam classifier</strong></p>
<p>Supervised learning. $x$ = features of emails. $y$ = spam (1) or not spam (0).</p>
<p>Feature $x$: Choose 100 words indicative of spam/not spam.</p>
<p>Note: In practice, take most frequently occurring $n$ words (10,000 to 50,000) in training set, rather than manually pick 100 words.</p>
<p>How to improve model performance?</p>
<ul>
<li>Collect lots of data</li>
<li>Develop sophisticated features based on email routing information (from email header).</li>
<li>Develop sophisticated features for message body, e.g. should “discount” and “discounts” be treated as the same word? How about “deal” and “Dealer”? Features about punctuation?</li>
<li>Develop sophisticated algorithm to detect misspellings (e.g. m0rtgage, med1cine, w4tches.)</li>
</ul>
<p><strong>Error Analysis</strong></p>
<ul>
<li>Start with a simple algorithm that you can implement quickly. Implement it and test it on your cross-validation data.</li>
<li>Plot learning curves to decide if more data, more features, etc. are likely to help.</li>
<li>Error analysis: Manually examine the examples (in cross validation set) that your algorithm made errors on. See if you spot any systematic trend in what type of examples it is making errors on.</li>
</ul>
<p><strong>Precision/Recall</strong></p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-09-Week6-6.png" style="zoom:60%"><br></center>

<p>We prefer high <strong>Precision</strong> and high <strong>Recall</strong>.</p>
<p><strong>Trade off precision and recall</strong></p>
<p><strong>$F_1$ Score (F score)</strong><br>$$<br>2\frac{PR}{P+R}<br>$$</p>
<h1 id="Week-7"><a href="#Week-7" class="headerlink" title="Week 7"></a>Week 7</h1><h2 id="Support-Vector-Machines"><a href="#Support-Vector-Machines" class="headerlink" title="Support Vector Machines"></a>Support Vector Machines</h2><p>Cost:<br>$$<br>minC<br>\sum_{i=1}^m[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+<br>\frac{1}{2}\sum_{i=0}^n\theta_j^2<br>$$<br>Trade-off<br>$$<br>CA+B<br>$$<br>Hypothesis<br>$$<br>h_\theta(x)=<br>\begin{equation}<br>\left{<br>             \begin{array}{lr}<br>             1, &amp; if \space \theta^Tx \geq0  \\<br>             0, &amp;  otherwise<br>             \end{array}<br>\right.<br>\end{equation}<br>$$</p>
<p><strong>SVM Decision Boundary: Linearly separable case</strong></p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-10-Week7-1.png" style="zoom:60%"><br></center>





<p><strong>Non-linear Decision Boundary</strong></p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-10-Week7-2.png" style="zoom:60%"><br></center>

<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-10-Week7-3.png" style="zoom:60%"><br></center>

<p>Each landmarks define a new feature thought all samples.</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-10-Week7-4.png" style="zoom:60%"><br></center>

<p>Some intuition of the kernel function</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-10-Week7-5.png" style="zoom:60%"><br></center>



<p><strong>Choosing the landmarks</strong></p>
<p>Exactly the location as the training examples.</p>
<p>Given $(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})$.</p>
<p>choose $l^{(1)}=x^{(1)},l^{(2)}=x^{(2)},…,l^{(m)}=x^{(m)}$.</p>
<p>Given example $x$:<br>$$<br>f_1 = similarity(x,l^{(1)}) \\<br>f_2 = similarity(x,l^{(2)}) \\<br>…<br>$$</p>
<p>$$<br>f =<br>\begin {bmatrix}<br>f_0 \\<br>f_1 \\<br>f_2 \\<br>… \\<br>f_m<br>\end {bmatrix}<br>$$</p>
<p>Hypothesis: Given $x$, compute features $f \in R^{m+1}$</p>
<p>Predict “y=1” if $\theta^Tf \geq 0$</p>
<p>Training:<br>$$<br>minC<br>\sum_{i=1}^m[y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)})]+<br>\frac{1}{2}\sum_{i=0}^n\theta_j^2<br>$$<br>C (=$\frac{1}{\lambda}$)</p>
<ul>
<li>Large C: Lower bias, high variance. (small $\lambda $)</li>
<li>Small C: Higher bias, lower variance. (Large $\lambda $)</li>
</ul>
<p>$\sigma^2$</p>
<ul>
<li>Large $\sigma^2$: Features $f_i$ vary more smoothly. High bias, lower variance.</li>
<li>Small $\sigma^2$: Features $f_i$ vary less smoothly. Lower bias, higher variance.</li>
</ul>
<p>Use SVM software package (e.g. liblinear, libsvm, …) to solve for parameters $\theta$.</p>
<p>Need to specify:</p>
<ul>
<li>Choice of parameter C.</li>
<li>Choice of kernel (similarity function): No kernel (“linear kernel”, apply for n large, m small), Gaussian kernel (Non-linear classifier, apply for n small, m large).</li>
</ul>
<p>Note: Not all similarity functions $similarity(x,l)$ make valid kernels. (Need to satisfy technical condition called “Mercer’s Theorem” to make sure SVM packages’ optimizations run correctly, and do not diverge).</p>
<p>Many off-the-shelf kernels available:</p>
<ul>
<li>Polynomial kernel: $k(x,l) = (x^Tl)^2, (x^Tl)^3,(x^Tl+1)^3, (x^Tl+constant)^{degree}$</li>
<li>More esoteric: String kernel, chi-square kernel, histogram intersection kernel, …</li>
</ul>
<p><strong>Multi-class classification</strong></p>
<p>Many SVM packages already have built-in multi-class classification functionality.</p>
<p>Otherwise, use one-vs.-all method. (Train $K$ SVMs, one to distinguish $y=i$ from the rest, for $i$ = 1,2,…,$K$), get $\theta^{(1)},\theta^{(2)},…,\theta^{(K)}$ Pick class $i$ with largest $(\theta^{(i)})^Tx$</p>
<p><strong>Logistic regression vs. SVMs</strong></p>
<p>If $n$ is large (relative to $m$): Use logistic regression, or SVM without a kernel (“linear kernel”)</p>
<p>If $n$ is small, $m$ is intermediate: Use SVM with Gaussian kernel</p>
<p>If $n$ is small, $m$ is large: Create/add more features, then use logistic regression or SVM without a kernel</p>
<p>Neural network likely to work well for most of these settings, but may be slower to train.</p>
<h2 id="Programming-Assignment-Support-Vector-Machines"><a href="#Programming-Assignment-Support-Vector-Machines" class="headerlink" title="Programming Assignment: Support Vector Machines"></a>Programming Assignment: Support Vector Machines</h2><p><a href="https://github.com/Aden-Q/Machine-Learning/tree/master/Week7" target="_blank" rel="noopener">Support Vector Machines</a></p>
<h1 id="Week-8"><a href="#Week-8" class="headerlink" title="Week 8"></a>Week 8</h1><h2 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h2><p>Training set: ${  x^{(1)},x^{(2)},x^{(3)},…,x^{(m)} }$</p>
<p><strong>Applications of clustering</strong></p>
<ul>
<li>Market segmentation</li>
<li>Social network analysis</li>
<li>Organize computing clusters</li>
<li>Astronomical data analysis</li>
</ul>
<p><strong>K-means algorithm</strong></p>
<p>cluster centroids</p>
<p>Input:</p>
<ul>
<li>$K$ (number of clusters)</li>
<li>Training set ${x^{(1)},x^{(2)},…,x^{(m)}}$</li>
</ul>
<p>$x^{(i)}\in R^n$ (drop $=x_0=1$ convention )</p>
<p>Randomly initialize $K$ cluster centroids $\mu_1,\mu_2,…,\mu_K \in R^n$</p>
<p>Repeat{</p>
<p>​    for i = 1 to m</p>
<p>​        $c^{(i)}$ := index (from 1 to $K$) of cluster centroid closest to $x^{(i)}$</p>
<p>​    for k = 1 to $K$</p>
<p>​        $\mu_k$ := average (mean) of points assigned to cluster $k$</p>
<p>}</p>
<p><strong>K-means optimization objective</strong></p>
<p>$c^{(i)}$ = index of cluster (1,2,…,$K$) to which example $x^{(i)}$ is currently assigned</p>
<p>$\mu_k$ = cluster centroid $k$ ($\mu_k \in R^n$)</p>
<p>$\mu_{c^{(i)}}$ = cluster centroid of cluster to which example $x^{(i)}$ has been assigned</p>
<p>Optimization objective:<br>$$<br>J = \frac1m\sum_{i=1}^{m}||x^{(i)}-\mu_{c^{(i)}}||^2<br>$$<br>Distortion cost function</p>
<p><strong>Random initialization</strong></p>
<p>Should have $K &lt; m$</p>
<p>Randomly pick $K$ training examples.</p>
<p>Set $\mu_1 ,…, \mu_K$ equal to these $K$ examples.</p>
<p><strong>Choosing the value of K</strong></p>
<p>Elbow method.</p>
<h2 id="Dimensionality-Reduction"><a href="#Dimensionality-Reduction" class="headerlink" title="Dimensionality Reduction"></a>Dimensionality Reduction</h2><p><strong>Data Compression</strong></p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week8-1.png" style="zoom:60%"><br></center>



<p><strong>Data Visulization</strong></p>
<p>Prefer 2-d or 3d data.</p>
<p><strong>Principal Component Analysis (PCA) problem formulation</strong></p>
<p>Reduce from 2-dimension to 1-dimension: Find a direction (a vector $u^{(1)}\in R^n$) onto which to project the data so as to minimize the projection error.</p>
<p>Reduce from n-dimension to k-dimension: Find $k$ vectors $u^{(1)},u^{(2)},…,u^{(k)}$ onto which to project the data, so as to minimize the projection error.</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week8-2.png" style="zoom:60%"><br></center>

<p><strong>PCA is not linear regression</strong></p>
<p>Left: linear regression</p>
<p>Right: PCA</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week8-3.png" style="zoom:60%"><br></center>



<p><strong>Principal Component Analysis algorithm</strong></p>
<p>Data preprocessing:</p>
<p>Training set: $x^{(1)},x^{(2)},…,x^{(m)}$</p>
<p>Preprocessing (feature scaling/mean normalization)<br>$$<br>\mu_j=\frac1m\sum_{i=1}^mx_j^{(i)}<br>$$<br>Replace each $x_j^{(i)}$ with $x_j-\mu_j$.</p>
<p>If different features on different scales, scale features to have comparable range of values.<br>$$<br>x_j^{(i)}=\frac{x_j^{(i)}-\mu_j}{s_j}<br>$$<br>Algorithm steps:</p>
<ol>
<li><p>Mean normalization and feature scaling.</p>
</li>
<li><p>Compute “covariance matrix”:</p>
</li>
</ol>
<p>$$<br>\Sigma = \frac{1}{m}\sum_{i=1}^n(x^{(i)})(x^{(i)})^T<br>$$</p>
<ol start="3">
<li>Compute “eigenvectors” of matrix $\Sigma$:</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[U,S,V] = svd(Sigma);</span><br></pre></td></tr></table></figure>
<p>Sigma is a $n\times n$ matrix.</p>
<p>U is also a $n \times n$ matrix, with each column represents a eigenvector.</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week8-4.png" style="zoom:60%"><br></center>

<ol start="4">
<li><p>Ureduce = U(:, 1:k)</p>
<p>z = Ureduce’ * x;</p>
</li>
</ol>
<p><strong>Reconstruction from compressed representation</strong><br>$$<br>z = U^T_{reduce}x<br>$$</p>
<p>$$<br>x_{approx} = U_{reduce}*z<br>$$</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week8-5.png" style="zoom:60%"><br></center>


<p><strong>Choosing $k$ (number of principal components)</strong></p>
<p>Average squared projection error: $\frac1m\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2$</p>
<p>Total variation in the data: $\frac1m\sum_{i=1}^m||x^{(i)}||^2$</p>
<p>Typically, choose $k$ to be smallest value so that<br>$$<br>\frac{Average \space error}{Variation} \leq0.01<br>$$<br>“99%” of variance is retained</p>
<p><strong>Application of PCA</strong></p>
<ul>
<li>Compression<ul>
<li>Reduce memory/disk needed to store data</li>
<li>Speed up learning algorithm</li>
</ul>
</li>
<li>Visualization</li>
</ul>
<p>Before implementing PCA, first try running whaever you want to do with the original/raw data. Only if that doesn’t do what you want, then implement PCA.</p>
<h2 id="Programming-Assignment-K-Means-Clustering-and-PCA"><a href="#Programming-Assignment-K-Means-Clustering-and-PCA" class="headerlink" title="Programming Assignment: K-Means Clustering and PCA"></a>Programming Assignment: K-Means Clustering and PCA</h2><p><a href="https://github.com/Aden-Q/Machine-Learning/tree/master/Week8" target="_blank" rel="noopener">K-Means Clustering and PCA</a></p>
<h1 id="Week-9"><a href="#Week-9" class="headerlink" title="Week 9"></a>Week 9</h1><h2 id="Anomaly-Detection"><a href="#Anomaly-Detection" class="headerlink" title="Anomaly Detection"></a>Anomaly Detection</h2><p><strong>Anomaly detection example</strong></p>
<p>Dataset: ${ x^{(1)},x^{(2)},…,x^{(m)} }$</p>
<p>Is $x_{test}$ anomalous?</p>
<p>$p(x_{test}) &lt; \epsilon$ -&gt; flag anomaly</p>
<p>$p(x_{test}) \geq \epsilon$ -&gt; OK</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-1.png" style="zoom:60%"><br></center>



<p>Fraud detection (Online website):</p>
<p>$x^{(i)}$ = features of user $i$’s activities</p>
<p>Model $p(x)$ from data.</p>
<p>Identify unusual users by checking which have $p(x) &lt; \epsilon$</p>
<p>Manufacturing</p>
<p>Monitoring computers in a data center.</p>
<p>$x^{(i)}$ = features of machine $i$</p>
<p>$x_1$ = memory use, $x_2$ = number of disk accesses/sec,</p>
<p>$x_3$ = CPU load, $x_4$ = CPU load/network traffic.</p>
<p>…</p>
<p><strong>Gaussian (Normal) distribution</strong></p>
<p>N ~ N($\mu$, $\sigma ^2$)</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-2.png" style="zoom:60%"><br></center>

<p>$$<br>p(x;\mu,\sigma ^2) = \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2})<br>$$</p>
<p><strong>Parameter estimation</strong><br>$$<br>\mu = \frac1m\sum_{i=1}^mx^{(i)} \\<br>\sigma^2 = \frac1m\sum_{i=1}^m(x^{(i)}-\mu)^2<br>$$</p>
<p><strong>Density estimation</strong></p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-4.png" style="zoom:60%"><br></center>



<p>Data set split:</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-5.png" style="zoom:60%"><br></center>



<p>Possible evaluation metrics:</p>
<ul>
<li>True positive, false positive, false negative, true negative</li>
<li>Precision/Recall</li>
<li>$F_1$-score</li>
</ul>
<p>Can also use cross validation set to choose parameter $\epsilon$</p>
<p><strong>Anomaly detection vs. Supervised learning</strong></p>
<p>Anomaly detection:</p>
<ul>
<li>Very small number of positive examples (y = 1). (0-20 is common).</li>
<li>Large number of negative (y = 0) examples.</li>
<li>Many different “tyles” of anomalies. Hard for any algorithm to learn from positive examples what the anomalies may look like; future anomalies may look nothing like any of the anomalous examples we’ve seen so far.</li>
<li>Fraud detection</li>
<li>Manufacturing (e.g. aircraft engines)</li>
<li>Monitoring machines in a data center</li>
</ul>
<p>Supervised learning:</p>
<ul>
<li>Large number of positive and negative examples.</li>
<li>Enough positive examples for algorithm to get sense of what positive examples are like, future positive examples likely to be similar to ones in training set.</li>
<li>Email spam classification</li>
<li>Weather prediction (sunny/rainy/etc).</li>
<li>Cancer classification</li>
</ul>
<p><strong>Feature selection and transformation</strong></p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-6.png" style="zoom:60%"><br></center>



<p><strong>Error analysis for anomaly detection</strong></p>
<p>Want $p(x)$ large for normal examples $x$.</p>
<p>​      $p(x)$ small for anomalous examples $x$.</p>
<p>Most common problem:</p>
<p>​    $p(x)$ is comparable (say, both large) for normal and anomalous examples</p>
<p>Choosing features that might take on unusually large or small values in the event of an anomaly.</p>
<p><strong>Multivariate Gaussian (Normal) distribution</strong></p>
<p>$x\in R^n$. Don’t model $p(x_1),p(x_2),…,$ etc. separately. Model $p(x)$ all in one go.</p>
<p>Parameters: $\mu \in R^n$, $\Sigma \in R^{n\times n}$ (convariance matrix)<br>$$<br>p(x;\mu,\Sigma) = \frac{1}{(2\pi)^\frac n2 |\Sigma|^{\frac12}}exp(-\frac12(x-\mu)^T\Sigma^{-1}(x-\mu))<br>$$</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-7.png" style="zoom:60%"><br></center>



<p><strong>Original model vs. Multivariate Gaussian</strong></p>
<p>Original: Manually create features to capture anomalies where $x_1,x_2$ take unusual combinations of values.</p>
<p>Multivariate: Automatically captures correlations between features. Have many constrains.</p>
<h2 id="Recommender-Systems"><a href="#Recommender-Systems" class="headerlink" title="Recommender Systems"></a>Recommender Systems</h2><p><strong>Content-based recommendations</strong></p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-8.png" style="zoom:60%"><br></center>

<p>For each user $j$, learn a parameter $\theta^{(j)}\in R^3$. Predict user $j$ as rating movie $i$ with $(\theta^{(j)})^Tx^{(i)} $ stars.</p>
<p>Problem formulation:</p>
<p>$r(i,j)=1$ if user $j$ has rated movie $i$ (0 otherwise)</p>
<p>$y^{(i,j)}$ = rating by user $j$ on movie $i$ (if defined)</p>
<p>$\theta^{(j)}$ = parameter vector for user $j$</p>
<p>$x^{(i)}$ = feature vector for movie $i$</p>
<p>For user $j$, movie $i$, predicted rating: $(\theta^{(j)})^Tx^{(i)} $</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-9.png" style="zoom:60%"><br></center>

<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-10.png" style="zoom:60%"><br></center>



<p><strong>Collaborative filtering</strong></p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-11.png" style="zoom:60%"><br></center>



<p><strong>Algorithm</strong></p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-12.png" style="zoom:60%"><br></center>



<p><strong>Low Rank Vectorization</strong></p>
<p><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-13.png" alt></p>
<p><strong>Finding related movies</strong><br>$$<br>small \space ||x^{(i)}-x^{(j)}|| -&gt; movie \space j \space and \space i \space are \space “similar”<br>$$</p>
<p><strong>Mean Normalization</strong></p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-14.png" style="zoom:60%"><br></center>





<h2 id="Programming-Assignment-Anomaly-Detection-and-Recommender-Systems"><a href="#Programming-Assignment-Anomaly-Detection-and-Recommender-Systems" class="headerlink" title="Programming Assignment: Anomaly Detection and Recommender Systems"></a>Programming Assignment: Anomaly Detection and Recommender Systems</h2><p><a href="https://github.com/Aden-Q/Machine-Learning/tree/master/Week9" target="_blank" rel="noopener">Anomaly Detection and Recommender Systems</a></p>
<h1 id="Week-10"><a href="#Week-10" class="headerlink" title="Week 10"></a>Week 10</h1><h2 id="Large-Scale-Machine-Learning"><a href="#Large-Scale-Machine-Learning" class="headerlink" title="Large Scale Machine Learning"></a>Large Scale Machine Learning</h2><blockquote>
<p>  It’s not who has the best algorithm that wins. It’s who has the most data.</p>
</blockquote>
<p><strong>Stochatic gradient descent</strong></p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week10-1.png" style="zoom:60%"><br></center>

<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week10-2.png" style="zoom:60%"><br></center>

<p>Out loop typically 1-10 times.</p>
<p><strong>Mini-batch gradient descent</strong></p>
<p>Batch gradient descent: Use all $m$ examples in each iteration</p>
<p>Stochastic gradient descent: Use 1 example in each iteration</p>
<p>Mini-batch gradient descent: Use $b$ examples in each iteration</p>
<p>b = mini-batch size</p>
<p>b = 2~100</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week10-3.png" style="zoom:60%"><br></center>

<p>With vectorization, Mini-batch outperforms Stochastic as regard to computation efficiency.</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week10-4.png" style="zoom:60%"><br></center>



<p>Learning rate decay:<br>$$<br>\alpha = \frac{const1}{iterationNumber+const2}<br>$$</p>
<p><strong>Online learning</strong></p>
<p><strong>Map-reduce and data parallelism</strong></p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week10-5.png" style="zoom:60%"><br></center>

<p>Many learning algorithms can be expressed as computing sums of functions over the training set.</p>
<h1 id="Week-11"><a href="#Week-11" class="headerlink" title="Week 11"></a>Week 11</h1><h2 id="Application-Example-Photo-OCR"><a href="#Application-Example-Photo-OCR" class="headerlink" title="Application Example: Photo OCR"></a>Application Example: Photo OCR</h2><p>Photo Optical Character Recognition</p>
<p><strong>Photo OCR pipeline</strong></p>
<ol>
<li>Text detection</li>
<li>Character segmentation</li>
<li>Character classification</li>
</ol>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week11-1.png" style="zoom:60%"><br></center>



<p><strong>Sliding window detection</strong></p>
<p>step-size / stride</p>
<p>These cencepts are related to computer vision which is introduced in the deeplearninng specialization. I should take notes for these in another course/specialization.</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week11-2.png" style="zoom:60%"><br></center>



<p><strong>Artificial data systhesizing</strong></p>
<p>Make sure you have a low bias classifier before expanding the effor. (Plot learning curves). E.g. keep increasing the number  of features/number of hidden units in neural network until you have a low bias classifier.</p>
<p>“How much work would it be to get 10x as much data as we currently have?”</p>
<ul>
<li>Artificial data synthesis</li>
<li>Collect/label it yourself</li>
<li>“Crowd source” (E.g. Amazon Mechanical Turk)</li>
</ul>
<p><strong>Ceiling analysis</strong></p>
<p>Estimating the errors due to each component.</p>
<p>What part of the pipeline should you spend the most time trying to improve?</p>
<p>E.g.</p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week11-3.png" style="zoom:60%"><br></center>

<p>It is really important to do ceiling analysis.</p>
<p><strong>Summary</strong></p>
<center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week11-4.png" style="zoom:60%"><br></center>


      
    </div>

    

    
    
    

    <div>
    
    <div>

    <div style="text-align:center;color: #ccc;font-size:14px;">-------------End of the article <i class="fa fa-paw"></i> Thanks for reading!-------------</div>

</div>
    
    </div>

    

    
       
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Programming/" rel="tag"><i class="fa fa-tag"></i> Programming</a>
          
            <a href="/tags/Machine-Learning/" rel="tag"><i class="fa fa-tag"></i> Machine Learning</a>
          
            <a href="/tags/Matlab/" rel="tag"><i class="fa fa-tag"></i> Matlab</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/03/26/Robotics-Perception/" rel="next" title="Robotics: Perception">
                <i class="fa fa-chevron-left"></i> Robotics: Perception
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/12/Introduction to TensorFlow/" rel="prev" title="Introduction to TensorFlow">
                Introduction to TensorFlow <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div id="gitalk-container"></div>  
  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Qingliu</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">21</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">2</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">21</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/Aden-Q" title="GitHub &rarr; https://github.com/Aden-Q" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:qian0102@umn.edu" title="E-Mail &rarr; mailto:qian0102@umn.edu" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://www.linkedin.com/in/azure-qian-11b128179" title="Linkedin &rarr; https://www.linkedin.com/in/azure-qian-11b128179" rel="noopener" target="_blank"><i class="fa fa-fw fa-linkedin"></i>Linkedin</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://www.facebook.com/zecheng.qian.7" title="FB Page &rarr; https://www.facebook.com/zecheng.qian.7" rel="noopener" target="_blank"><i class="fa fa-fw fa-facebook"></i>FB Page</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Week-1"><span class="nav-number">1.</span> <span class="nav-text">Week 1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Linear-Regression-with-One-Variable"><span class="nav-number">1.2.</span> <span class="nav-text">Linear Regression with One Variable</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-Regression-with-One-Variable-1"><span class="nav-number">1.2.1.</span> <span class="nav-text">Linear Regression with One Variable</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Linear-Algebra-Review"><span class="nav-number">1.3.</span> <span class="nav-text">Linear Algebra Review</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week-2"><span class="nav-number">2.</span> <span class="nav-text">Week 2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Linear-Regression-with-Multiple-Variables"><span class="nav-number">2.1.</span> <span class="nav-text">Linear Regression with Multiple Variables</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Octave-Matlab-Tutorial"><span class="nav-number">2.2.</span> <span class="nav-text">Octave/Matlab Tutorial</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Programming-Assignment-Linear-Regression"><span class="nav-number">2.3.</span> <span class="nav-text">Programming Assignment: Linear Regression</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week-3"><span class="nav-number">3.</span> <span class="nav-text">Week 3</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Logistic-Regression"><span class="nav-number">3.1.</span> <span class="nav-text">Logistic Regression</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Regularization"><span class="nav-number">3.2.</span> <span class="nav-text">Regularization</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Programming-Assignment-Logistic-Regression"><span class="nav-number">3.3.</span> <span class="nav-text">Programming Assignment: Logistic Regression</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week-4"><span class="nav-number">4.</span> <span class="nav-text">Week 4</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-Networks-Representation"><span class="nav-number">4.1.</span> <span class="nav-text">Neural Networks: Representation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Programming-Assignment-Multi-class-Classification-and-Neural-Networks"><span class="nav-number">4.2.</span> <span class="nav-text">Programming Assignment: Multi-class Classification and Neural Networks</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week-5"><span class="nav-number">5.</span> <span class="nav-text">Week 5</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-Networks-Learning"><span class="nav-number">5.1.</span> <span class="nav-text">Neural Networks: Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Programming-Assignment-Neural-Network-Learning"><span class="nav-number">5.2.</span> <span class="nav-text">Programming Assignment: Neural Network Learning</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week-6"><span class="nav-number">6.</span> <span class="nav-text">Week 6</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Advice-for-Applying-Machine-Learning"><span class="nav-number">6.1.</span> <span class="nav-text">Advice for Applying Machine Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Programming-Assignment-Regularized-Linear-Regression-and-Bias-Variance"><span class="nav-number">6.2.</span> <span class="nav-text">Programming Assignment: Regularized Linear Regression and Bias/Variance</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Machine-Learning-System-Design"><span class="nav-number">6.3.</span> <span class="nav-text">Machine Learning System Design</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week-7"><span class="nav-number">7.</span> <span class="nav-text">Week 7</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Support-Vector-Machines"><span class="nav-number">7.1.</span> <span class="nav-text">Support Vector Machines</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Programming-Assignment-Support-Vector-Machines"><span class="nav-number">7.2.</span> <span class="nav-text">Programming Assignment: Support Vector Machines</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week-8"><span class="nav-number">8.</span> <span class="nav-text">Week 8</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Unsupervised-Learning"><span class="nav-number">8.1.</span> <span class="nav-text">Unsupervised Learning</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dimensionality-Reduction"><span class="nav-number">8.2.</span> <span class="nav-text">Dimensionality Reduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Programming-Assignment-K-Means-Clustering-and-PCA"><span class="nav-number">8.3.</span> <span class="nav-text">Programming Assignment: K-Means Clustering and PCA</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week-9"><span class="nav-number">9.</span> <span class="nav-text">Week 9</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Anomaly-Detection"><span class="nav-number">9.1.</span> <span class="nav-text">Anomaly Detection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Recommender-Systems"><span class="nav-number">9.2.</span> <span class="nav-text">Recommender Systems</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Programming-Assignment-Anomaly-Detection-and-Recommender-Systems"><span class="nav-number">9.3.</span> <span class="nav-text">Programming Assignment: Anomaly Detection and Recommender Systems</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week-10"><span class="nav-number">10.</span> <span class="nav-text">Week 10</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Large-Scale-Machine-Learning"><span class="nav-number">10.1.</span> <span class="nav-text">Large Scale Machine Learning</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week-11"><span class="nav-number">11.</span> <span class="nav-text">Week 11</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Application-Example-Photo-OCR"><span class="nav-number">11.1.</span> <span class="nav-text">Application Example: Photo OCR</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Qingliu</span>

  

  
</div>



<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
    Number of visitors: <span id="busuanzi_value_site_uv"></span>
</span>
</div>


<!--
<span class="post-meta-divider">|</span>



  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.9.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> v6.5.0</div>



-->

<span class="post-meta-divider">|</span>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">Total 35.3k words</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  

  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>












  



  
    
      
  
  <script type="text/javascript" color="0,0,0" opacity="0.5" zindex="-1" count="99" src="/lib/canvas-nest/canvas-nest.min.js"></script>













  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.5.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.5.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.5.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.5.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.5.0"></script>



  



  









  <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
  <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
   <script type="text/javascript">
        var gitalk = new Gitalk({
          clientID: '69ff68eec89b6cd91e37',
          clientSecret: '200633852bdb3579481b3fecee7178c1b98f471e',
          repo: 'gitalk-comments',
          owner: 'Aden-Q',
          admin: ['Aden-Q'],
          id: location.pathname,
          distractionFreeMode: 'true'
        })
        gitalk.render('gitalk-container')           
       </script>


  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script>
    
    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();

      Counter('get', '/classes/Counter', { where: JSON.stringify({ url }) })
        .done(function ({ results }) {
          if (results.length > 0) {
            var counter = results[0];
            
            Counter('put', `/classes/Counter/${counter.objectId}`, JSON.stringify({ time: { "__op":"Increment", "amount":1 } }))
            
            .done(function () {
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(counter.time + 1);
            })
            
            .fail(function ({ responseJSON }) {
                console.log('Failed to save Visitor num, with error message: ' + responseJSON.error);
            })
          } else {
            
              Counter('post', '/classes/Counter', JSON.stringify({ title: title, url: url, time: 1}))
                .done(function () {
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(1);
                })
                .fail(function () {
                  console.log('Failed to create');
                });
            
          }
        })
      .fail(function ({ responseJSON }) {
        console.log('LeanCloud Counter Error:' + responseJSON.code + " " + responseJSON.error);
      });
    }
    

    $(function() {
      $.get('https://app-router.leancloud.cn/2/route?appId=' + "TTkRYLVAOPvuxQuV5scOHl9U-gzGzoHsz")
        .done(function ({ api_server }) {
          var Counter = function (method, url, data) {
            return $.ajax({
              method: method,
              url: `https://${api_server}/1.1${url}`,
              headers: {
                'X-LC-Id': "TTkRYLVAOPvuxQuV5scOHl9U-gzGzoHsz",
                'X-LC-Key': "fCRGgKjo3tYIfbW3ygKrNCRN",
                'Content-Type': 'application/json',
              },
              data: data,
            });
          };
          
          addCount(Counter);
          
        })
    });
  </script>



  

  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  

  

  

  

  

  

  


</body>
</html>
