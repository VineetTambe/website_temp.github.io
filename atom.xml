<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Qingliu</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-03-16T18:59:25.450Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Qingliu</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Cryptology</title>
    <link href="http://yoursite.com/2020/02/21/Cryptology/"/>
    <id>http://yoursite.com/2020/02/21/Cryptology/</id>
    <published>2020-02-21T07:24:34.000Z</published>
    <updated>2020-03-16T18:59:25.450Z</updated>
    
    <content type="html"><![CDATA[<p>Interesting things in Cryptology</p><a id="more"></a><h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><h2 id="Divisibility"><a href="#Divisibility" class="headerlink" title="Divisibility"></a>Divisibility</h2><p>For integers $d$, $n$, the integer $d$ divides $n$ (is a divisor of $n$) iff $n/d$ is an integer. Then we say $n$ is a multiple of $d$. This property is written as:<br>$$<br>d|n<br>$$<br>Two integers $m$, $n$ are relatively prime or coprime iff the only integers that divide both $m$ and $n$ are $\pm 1$.</p><h2 id="Multiplicative-Inverse"><a href="#Multiplicative-Inverse" class="headerlink" title="Multiplicative Inverse"></a>Multiplicative Inverse</h2><p>For integers $x$, $y$, $m$, $y$ is a multiplicative inverse of $x$ modulo $m$ if<br>$$<br>x \cdot y \; \% \; m = 1<br>$$</p><h2 id="Congruence"><a href="#Congruence" class="headerlink" title="Congruence"></a>Congruence</h2><p>For integers $x$ and $y$, we say $x$ is congruent to $y$ modulo $m$, written as<br>$$<br>x \equiv y \mod m<br>$$<br>iff<br>$$<br>m|(x-y)<br>$$</p><h2 id="LCM-and-GCD"><a href="#LCM-and-GCD" class="headerlink" title="LCM and GCD"></a>LCM and GCD</h2><p>For two integers $m$ and $n$, we define their least common multiple (LCM) and greatest common divisor (GCD) . $lcm(m,n)$ is the smallest positive integer $N$ so that $N$ is a multiple of $m$, and $N$ is a multiple of $n$. $gcd(m,n)$ is the largest positive integer $d$ so that $d$ divides $m$ and $d$ divides $n$.</p><h1 id="Theorems-and-Proofs"><a href="#Theorems-and-Proofs" class="headerlink" title="Theorems and Proofs"></a>Theorems and Proofs</h1><h2 id="Theorem-1"><a href="#Theorem-1" class="headerlink" title="Theorem 1"></a>Theorem 1</h2><p>If $x$ and $m$ are relatively prime, then $x$ has an unique multiplicative inverse modulo $m$. In particular, if $ax + bm = 1$, the integer $a$ is a multiplicative inverse of $x$ modulo $m$.</p><h2 id="Proof-1"><a href="#Proof-1" class="headerlink" title="Proof 1"></a>Proof 1</h2><p>Given that $x$ and $m$ are relatively prime we have $gcd(x,m) = 1$.</p><p>And $gcd(x,m)$ is the least positive combination of the form $ax+bm$ for any integers $a$ and $b$.</p><p>So we have $ax+bm = 1$ for some integers $a$ and $b$.</p><p>Then<br>$$<br>ax = 1-bm \equiv 1 \mod m<br>$$<br>By definition, $a$ is a multiplicative inverse of $x$ modulo $m$.</p><p>Uniqueness:</p><p>Suppose $a’$ is also a multiplicative inverse of $x$ modulo $m$, then<br>$$<br>a’x \equiv ax \equiv 1 \mod m<br>$$<br>$$<br>(a’-a)x \equiv0 \mod m<br>$$</p><p>Since $x$ and $m$ are relatively prime,<br>$$<br>m | (a’-a)<br>$$<br>$$<br>a’ \equiv a \mod m<br>$$</p><p>This means that $a’$ and $a$ belong to the same residue class modulo $m$, complete the proof.</p><h2 id="Theorem-2"><a href="#Theorem-2" class="headerlink" title="Theorem 2"></a>Theorem 2</h2><p>$$<br>lcm(m,n) \cdot gcd(m,n) = mn<br>$$</p><h2 id="Proof-2"><a href="#Proof-2" class="headerlink" title="Proof 2"></a>Proof 2</h2><p>Think about $lcm(m,n)$ and $gcd(m,n)$ derived from the prime factorization of $m$ and $n$. Then this theorem is obvious.</p><h2 id="Theorem-3"><a href="#Theorem-3" class="headerlink" title="Theorem 3"></a>Theorem 3</h2><p>About factorization:</p><p>For any positive integer $n$:<br>$$<br>x^n-y^n = (x-y)(x^{n-1}+x^{n-2}y+\dots+xy^{n-2}+y^{n-1})<br>$$<br>For any positive odd integer $n$:<br>$$<br>x^n+y^n = (x+y)(x^{n-1}-x^{n-2}y+ \dots - xy^{n-2}+y^{n-1}) \<br>$$</p><h2 id="Theorem-4"><a href="#Theorem-4" class="headerlink" title="Theorem 4"></a>Theorem 4</h2><p>If an integer in the form $M_{n} = 2^n-1$ is a prime, then $n$ has to be prime.</p><h2 id="Proof-4"><a href="#Proof-4" class="headerlink" title="Proof 4"></a>Proof 4</h2><p>Suppose $n$ is not prime, then $n = k \cdot m$, $k$, $m$ $&gt;$ 1.<br>$$<br>2^n-1 = 2^{k\cdot m}-1 = (2^k)^m-1 = (2^k-1)\cdot (\dots)<br>$$<br>Since $k &gt; 1$ and $m &gt; 1$,<br>$$<br>(2^k-1) | (2^n-1)<br>$$<br>and $2^k-1$ is non-trivial. Complete the proof.</p><h1 id="From-Classical-Cipher-to-Modern-Cipher"><a href="#From-Classical-Cipher-to-Modern-Cipher" class="headerlink" title="From Classical Cipher to Modern Cipher"></a>From Classical Cipher to Modern Cipher</h1><h2 id="Shift-Cipher"><a href="#Shift-Cipher" class="headerlink" title="Shift Cipher"></a>Shift Cipher</h2><h2 id="One-Time-Pad"><a href="#One-Time-Pad" class="headerlink" title="One-Time Pad"></a>One-Time Pad</h2><h2 id="Affine-Cipher"><a href="#Affine-Cipher" class="headerlink" title="Affine Cipher"></a>Affine Cipher</h2><h2 id="Vigenere-Cipher"><a href="#Vigenere-Cipher" class="headerlink" title="Vigenere Cipher"></a>Vigenere Cipher</h2><h2 id="Hill-Cipher"><a href="#Hill-Cipher" class="headerlink" title="Hill Cipher"></a>Hill Cipher</h2><h2 id="RSA"><a href="#RSA" class="headerlink" title="RSA"></a>RSA</h2><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><p><em>Cryptology and Number Theory</em>  by Paul Garrett</p><p>Course materials by Andrew Odlyzko</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Interesting things in Cryptology&lt;/p&gt;
    
    </summary>
    
      <category term="Note" scheme="http://yoursite.com/categories/Note/"/>
    
    
      <category term="Math" scheme="http://yoursite.com/tags/Math/"/>
    
  </entry>
  
  <entry>
    <title>Task List</title>
    <link href="http://yoursite.com/2020/02/12/Task%20List/"/>
    <id>http://yoursite.com/2020/02/12/Task List/</id>
    <published>2020-02-13T03:15:34.000Z</published>
    <updated>2020-03-16T00:19:57.044Z</updated>
    
    <content type="html"><![CDATA[<p>列一下近期要做的事情。。。</p><a id="more"></a><p>算是第一次在博客上自我反省（吐槽）</p><p>最近有些忙的爆炸的感觉，很多事情想做却都还没有做。</p><h1 id="2月"><a href="#2月" class="headerlink" title="2月"></a>2月</h1><ul><li style="list-style: none"><input type="checkbox" checked> 强行被拉去再参加一次数学建模美赛，说是国际比赛但其实水分很大，不管怎么说还是好好准备认真对待吧。</li><li style="list-style: none"><input type="checkbox" checked> Github上以前很多的project文档写的是真的烂，找个时间集中整理一遍，顺便把前几个学期的课程project也整理一下写个文档po上去。工作量巨大。(最后简单整理了一波了事)</li><li style="list-style: none"><input type="checkbox"> 知乎专栏不知道多久没更了，总结来说是自己太懒了，啥事都拖着没去做。</li><li style="list-style: none"><input type="checkbox" checked> 月底due的ECCV的project找点事情做，目前就照着模板写了个难度不大的证明，总感觉有点太混了。(等paper交了再勾掉。) 继续考虑一下怎么在CURE-OR数据集上做测试，和学长讨论。尽量把这部分做了。也算锻炼一下实验能力。更新：自我感觉做了不少事情，前一个test弄work了但是数据集问题就不要了。后面这个实验是学长挺久都没做出来的，我弄了一天给弄出来了，而且目前看来效果算是稳定，但是最近忙的有点炸，没太多时间去跑数据。</li><li style="list-style: none"><input type="checkbox"> 自学数学，要看一些数学书，计划先看抽代和实分析，后面泛函和傅里叶分析。</li><li style="list-style: none"><input type="checkbox"> 练练英语口语和听力，准备托福GRE再考。</li><li style="list-style: none"><input type="checkbox"> N久没写算法题了，GitHub OJ repo已经拖更N久，ACM算法笔记也拖更N久。</li><li style="list-style: none"><input type="checkbox" checked> 当所有课程小组作业自己都要担任主力的时候，时间总是感觉不太够用。把小组作业都给做了。</li><li style="list-style: none"><input type="checkbox" checked> 养成修仙习惯，晚睡（已经做到）早起（比原来起得早了）。</li><li style="list-style: none"><input type="checkbox"> 把博客上以前开的坑（那么多）尽量填掉一些。</li><li style="list-style: none"><input type="checkbox" checked> 2月13号，不睡觉的一天。3081zybook assignment扎堆布置下来+scattering transform example，5521机器学习第一次作业还没开始写。加之明天的数模比赛要做很多data analysis，很久没动手了比较生疏。今天也自闭了一天，越来越觉得自己太菜，和我认为优秀的人差了老远。</li><li style="list-style: none"><input type="checkbox" checked> 2月17号，今天用一天的时间把CSCI5521布置下来两周完成的作业完成了，看来潜力还是有待发掘。虽然上个学期日常用2-3天完成两周的作业（CSCI5561），但感觉应该还是尽量早做，ddl不能赶的这么紧张。</li><li style="list-style: none"><input type="checkbox"> 找个时间去做一下scattering transfrom，感觉挺有潜力的，而且有机会做独立的工作。顺便把VAE的idea继续完善一下。</li><li style="list-style: none"><input type="checkbox"> 把孙老师发下来的关于俩project的paper都仔细看一下。感觉project都挺有意思，但是不太好上手，尤其是inverse problem的那个。</li><li style="list-style: none"><input type="checkbox"> 把博客几篇文章的坑填完，包括但不限于：Pytorch tutorial，Kaggle-Scikit learn，feature engineering，Cryptology。</li><li style="list-style: none"><input type="checkbox" checked> 密码学的书看上瘾了，虽然都是些高中数论知识。。。</li><li style="list-style: none"><input type="checkbox" checked> 申请UROP(funding)</li><li style="list-style: none"><input type="checkbox"> 申请奖学金</li><li style="list-style: none"><input type="checkbox" checked> MUDAC想去，但是需要找靠谱的队友，但是如果时间紧张的就不去了呗</li><li style="list-style: none"><input type="checkbox" checked> 春假前这一周已经要忙爆炸了，这次是真爆炸</li><li style="list-style: none"><input type="checkbox"> 春假里边把密码学里边几个有意思的数论题证明写一下po一下</li><li style="list-style: none"><input type="checkbox"> 不鸽不鸽，坚决不能鸽了。好多事情都鸽了太久</li></ul><h1 id="3月"><a href="#3月" class="headerlink" title="3月"></a>3月</h1><ul><li style="list-style: none"><input type="checkbox"> 把二月没做完的事情继续做。</li><li style="list-style: none"><input type="checkbox" checked> 今年暑研在本校继续做或者去外校想想清楚，尽早申请。</li><li style="list-style: none"><input type="checkbox"> 被强行拉去参加数据分析比赛（全美？）</li><li style="list-style: none"><input type="checkbox"> 去申请一下SSN</li><li style="list-style: none"><input type="checkbox"> 普林斯顿算法课的第二部分的坑填完（超级大坑）</li><li style="list-style: none"><input type="checkbox"> 近似算法那个project的文档好好写的一遍，demo重新做一个（大坑），真的是懒得不行</li><li style="list-style: none"><input type="checkbox"> 机器学习书继续看，专栏笔记（大坑）</li><li style="list-style: none"><input type="checkbox"> 所有ddl赶完之后，强迫自己考GT</li><li style="list-style: none"><input type="checkbox" checked> 阿里云一面，面试官看起来太友善了。虽然是第一次面试而且几乎完全没准备，不过没有想象的紧张，几个项目感觉视频面是真的不太好展示，也没有PPT，只靠说的话很难说明白，而且几个项目都是一两年前做的。不知道凉了木有。</li></ul><p>觉得自己不够聪明，做事不够有毅力，不够上进，这样下去不会抑郁吧</p><p>努力成为自己想成为的人，不能继续在舒适区划水，要能静下心来，强者的世界不需要休息</p><p>Be positive and be away from snobbish people.</p><p>想到再更</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;列一下近期要做的事情。。。&lt;/p&gt;
    
    </summary>
    
      <category term="Random" scheme="http://yoursite.com/categories/Random/"/>
    
    
  </entry>
  
  <entry>
    <title>Reading List</title>
    <link href="http://yoursite.com/2019/11/09/Readling%20List/"/>
    <id>http://yoursite.com/2019/11/09/Readling List/</id>
    <published>2019-11-09T18:51:34.000Z</published>
    <updated>2020-02-26T05:10:10.453Z</updated>
    
    <content type="html"><![CDATA[<p>Readling List</p><a id="more"></a><h1 id="Books"><a href="#Books" class="headerlink" title="Books"></a>Books</h1><h2 id="Mathematics"><a href="#Mathematics" class="headerlink" title="Mathematics"></a>Mathematics</h2><p>先学分析</p><ul><li><em>Fourier Analysis: An Introduction</em></li><li><em>Complex Analysis</em></li><li><em>Real Analysis: Measure Theory, Integration, and Hilbert Spaces</em></li><li><em>Functional Analysis: Introduction to Further Topics in Analysis</em></li><li><em>Real Mathematical Analysis</em></li></ul><p>补两本优化</p><ul><li><em>Convex Optimization: Algorithms and Complexity</em></li><li><em>Convex Optimization</em></li></ul><p>抽象代数</p><ul><li><em>Abstract Algebra An Inquiry-based Approach</em></li><li><em>First Course in Abstract Algebra</em> (抽代入门书？)</li><li><em>Algebra</em></li></ul><h2 id="Statistics"><a href="#Statistics" class="headerlink" title="Statistics"></a>Statistics</h2><ul><li><em>An Introduction to Statistical Learning</em></li><li><em>The Elements of Statistical Learning</em></li></ul><h2 id="Computer-Science"><a href="#Computer-Science" class="headerlink" title="Computer Science"></a>Computer Science</h2><ul><li><em>机器学习，周志华</em></li><li><em>Deep Learning</em></li><li><em>算法导论，刘汝佳</em> (找比赛靠谱的队友)</li><li><em>Elements of the Theory of Computation</em></li><li><em>Neural Networks and Deep Learning</em></li></ul><p>想到再更</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Readling List&lt;/p&gt;
    
    </summary>
    
      <category term="Note" scheme="http://yoursite.com/categories/Note/"/>
    
    
      <category term="Mathematics" scheme="http://yoursite.com/tags/Mathematics/"/>
    
      <category term="Computer Science" scheme="http://yoursite.com/tags/Computer-Science/"/>
    
  </entry>
  
  <entry>
    <title>ACM Algorithm Note</title>
    <link href="http://yoursite.com/2019/09/25/ACM%20Algorithm%20Note/"/>
    <id>http://yoursite.com/2019/09/25/ACM Algorithm Note/</id>
    <published>2019-09-26T02:15:34.000Z</published>
    <updated>2020-02-19T05:40:58.015Z</updated>
    
    <content type="html"><![CDATA[<p>ACM算法笔记</p><a id="more"></a><h1 id="欧拉回路"><a href="#欧拉回路" class="headerlink" title="欧拉回路"></a>欧拉回路</h1><p>欧拉回路存在的充要条件：</p><ul><li>无向图：连通并且最多存在两个奇点（奇点：度数为奇数的点）</li><li>有向图：底图连通并且一个点入度比出度大1，一个点出度比入度大1，其他点入度等于出度</li></ul><p>编程的时候DFS，最后判断一遍是否访问到所有节点就行了，用邻接矩阵表示图（因为要随时看边是否被访问过，邻接矩阵效率更高）</p><h1 id="枚举排列"><a href="#枚举排列" class="headerlink" title="枚举排列"></a>枚举排列</h1><p>升序枚举：</p><p>递归调用，一个参数为当前序列，一个参数为可选元素集合，在不可重集时没问题，在可重集的时候要保证下标不重复</p><p>关于升序排列，也可以利用C++ STL里面的<code>next_permutation</code>函数来执行：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line">...;</span><br><span class="line"><span class="keyword">do</span> &#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;n; i++)<span class="built_in">printf</span>(<span class="string">"%d "</span>, p[i]);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">&#125; <span class="keyword">while</span>(next_permutation(p,p+n));</span><br><span class="line">...;</span><br></pre></td></tr></table></figure><p>而且<code>next_permutaiton</code>适用于可重集（那还自己写干啥-.-）</p><h1 id="子集生成"><a href="#子集生成" class="headerlink" title="子集生成"></a>子集生成</h1><h2 id="位向量法"><a href="#位向量法" class="headerlink" title="位向量法"></a>位向量法</h2><p>用一个位向量表示是否选取某个元素</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print_subset</span><span class="params">(<span class="keyword">int</span> n, <span class="keyword">int</span>* B, <span class="keyword">int</span> cur)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(cur == n) &#123;</span><br><span class="line">        <span class="comment">// print</span></span><br><span class="line">    &#125;</span><br><span class="line">    B[cur] = <span class="number">1</span>;</span><br><span class="line">    print_subset(n, B, cur+<span class="number">1</span>);<span class="comment">// 选第cur个元素</span></span><br><span class="line">    B[cur] = <span class="number">0</span>;</span><br><span class="line">    print_subset(n, B, cur+<span class="number">1</span>);<span class="comment">// 不选cur个元素</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>有点回溯剪枝的感觉</p><h2 id="二进制法"><a href="#二进制法" class="headerlink" title="二进制法"></a>二进制法</h2><p>部分没看懂。。。</p><h1 id="回溯"><a href="#回溯" class="headerlink" title="回溯"></a>回溯</h1><p>深度优先</p><h2 id="八皇后"><a href="#八皇后" class="headerlink" title="八皇后"></a>八皇后</h2><p>但是时间复杂度比较难算</p><h2 id="素数环"><a href="#素数环" class="headerlink" title="素数环"></a>素数环</h2><p>问题描述：把整数1，2，3，…，n组成一个环，使相邻整数和为素数</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> cur)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(cur==n &amp;&amp; isp(A[<span class="number">0</span>]+A[n<span class="number">-1</span>])) &#123;</span><br><span class="line">        <span class="comment">//print</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=n; i++) &#123;<span class="comment">// 1固定为环起点，位置无所谓，所以从2开始试</span></span><br><span class="line">        <span class="keyword">if</span>(!vis[i] &amp;&amp; isp[i+A[cur<span class="number">-1</span>]]) &#123;</span><br><span class="line">            A[cur] = i;</span><br><span class="line">            vis[i] = <span class="number">1</span>;<span class="comment">// 记录访问，递归到下一层</span></span><br><span class="line">            dfs(cur+<span class="number">1</span>);</span><br><span class="line">            vis[i] = <span class="number">0</span>;<span class="comment">//清除访问，试下一个(回溯剪枝)</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="状态空间搜索"><a href="#状态空间搜索" class="headerlink" title="状态空间搜索"></a>状态空间搜索</h1><p>没看懂。。。</p><h1 id="迭代加深搜索"><a href="#迭代加深搜索" class="headerlink" title="迭代加深搜索"></a>迭代加深搜索</h1><p>按深度由小到大进行搜索，基本假设是，如果解的深度有限，总能找到解。相比于普通DFS或者回溯的优点是，不会再一条路上走到stack over flow还找不到。</p><p>迭代加深每次只考虑深度不超过maxd的结点。</p><p>对于回溯求解但解答树室深度没有上限的题目，可以考虑使用迭代加深搜索。</p><p>没看懂。。。</p><h1 id="分治"><a href="#分治" class="headerlink" title="分治"></a>分治</h1><p>多练题</p><h1 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h1><p>关键字：状态、状态转移、最优子结构（全局最优解包含局部最优解）</p><p>状态和状态转移方程一起描述算法，也是动态规划的核心</p><p>记忆化搜索，逆序枚举可以优化时间复杂度</p><p>DP没啥好说的，多练题，练出来感觉就行了</p><h1 id="辗转相除法"><a href="#辗转相除法" class="headerlink" title="辗转相除法"></a>辗转相除法</h1><p>辗转相除法（欧几里德算法，Euclid algorithm）的关键在于恒等式</p><p>$$<br>gcd(a,b) = gcd(b,a mod b)<br>$$</p><p>边界条件<br>$$<br>gcd(a,0) = a<br>$$</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">gcd</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> b == <span class="number">0</span> ? a : gcd(b, a%b);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>利用辗转相除法求最小公倍数：<br>$$<br>gcd(a,b) <em> lcm(a,b) = a</em>b<br>$$</p><p>$$<br>lcm(a,b) = a / gcd(a,b) * b<br>$$</p><p>注意一个细节：求最小公倍数的时候先除再乘，目的是为了避免大整数溢出</p><h1 id="Eratosthenes筛法"><a href="#Eratosthenes筛法" class="headerlink" title="Eratosthenes筛法"></a>Eratosthenes筛法</h1><p>筛法基本思想：对于不超过$n$的每个非负整数$p$，删除$2p$，$3p$，$4p$，…，处理完所有数后，剩下没被删除的数就是素数。程序很简单：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">memset</span>(vis, <span class="number">0</span>, <span class="keyword">sizeof</span>(vis));</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=n; i++)</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j=i*<span class="number">2</span>; j&lt;=n; j+=i)</span><br><span class="line">        vis[j] = <span class="number">1</span>;</span><br></pre></td></tr></table></figure><p>内层循环次数是<br>$$<br>\sum_{i=2}^{n} \frac{n}{i} = O(nlogn)<br>$$<br>来源于欧拉常数的公式</p><p>注意到两个细节：外层循环筛其实只要基于素数基就行，可以加一个判断条件以实现；内层循环可以从$i*i$开始，直观理解是，$i$乘以任何一个小于$i$的数都在之前的外循环被筛掉了。</p><p>综上两条，修改后的筛法如下(欧拉线性筛？核心是那条判断以及内循环的下界)：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> m = <span class="built_in">sqrt</span>(n+<span class="number">0.5</span>);</span><br><span class="line"><span class="built_in">memset</span>(vis, <span class="number">0</span>, <span class="keyword">sizeof</span>(vis));</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=m; i++)<span class="keyword">if</span>(!vis[i])</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> j=i*i; j&lt;=n; j++)vis[j] = <span class="number">1</span>;</span><br></pre></td></tr></table></figure><p>不超过$x$的素数约有(素数定理)：<br>$$<br>\frac{x}{lnx}<br>$$</p><h1 id="扩展欧几里德算法"><a href="#扩展欧几里德算法" class="headerlink" title="扩展欧几里德算法"></a>扩展欧几里德算法</h1><p>问题：求直线$ax+by+c=0$上有多少个整点$(x,y)$满足$x\in[x_1,x_2], y\in[y_1,y_2]$。</p><p>原理比较复杂，涉及素数的线性分解</p><h1 id="同余与模算术"><a href="#同余与模算术" class="headerlink" title="同余与模算术"></a>同余与模算术</h1><p>基本公式：<br>$$<br>(a+b) \ mod \ n = ((a \ mod\  n)+(b\  mod\ n))\ mod n \<br>(a-b) \ mod \ n = ((a \ mod\  n)-(b\  mod\ n)+n)\ mod n \<br>ab \ mod \ n = (a\ mod \ n)(b \ mod \ n) \ mod n<br>$$<br>幂取模的分治实现：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">pow_mod</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> n, <span class="keyword">int</span> m)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(n == <span class="number">0</span>)<span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> x = pow_mod(a, n/<span class="number">2</span>, m);</span><br><span class="line">    <span class="keyword">long</span> <span class="keyword">long</span> ans = (<span class="keyword">long</span> <span class="keyword">long</span>)x * x % m;</span><br><span class="line">    <span class="keyword">if</span>(n%<span class="number">2</span> == <span class="number">1</span>)ans = ans * a % m;</span><br><span class="line">    <span class="keyword">return</span> (<span class="keyword">int</span>) ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="数论中的计数问题"><a href="#数论中的计数问题" class="headerlink" title="数论中的计数问题"></a>数论中的计数问题</h1><p>由数的唯一分解定理<br>$$<br>n = p_1^{a_1}p_2^{a_2}p_1^{a_2} \cdot \cdot \cdot p_k^{a_k}<br>$$<br>$n$的正约数个数为<br>$$<br>(a_1+1)(a_2+1)\cdot \cdot \cdot(a_k+1)<br>$$<br>欧拉函数(计数小于$n$且与$n$互素的数的个数)：<br>$$<br>\phi(n) = n(1-\frac{1}{p_1})(1-\frac{1}{p_2})\cdot \cdot \cdot (1-\frac{1}{p_k})<br>$$<br>计算欧拉函数(其中有个找素因子的trick，自己体会)</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">euler_phi</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> m = (<span class="keyword">int</span>)<span class="built_in">sqrt</span>(n+<span class="number">0.5</span>);</span><br><span class="line">    <span class="keyword">int</span> ans = n;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=m; i++)<span class="keyword">if</span>(n % i == <span class="number">0</span>) &#123;</span><br><span class="line">        ans = ans /i * (i<span class="number">-1</span>);</span><br><span class="line">        <span class="keyword">while</span>(n % i == <span class="number">0</span>)n /= i;<span class="comment">// 这就是那个trick，自己体会</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(n &gt; <span class="number">1</span>) ans = ans / n * (n<span class="number">-1</span>);<span class="comment">// 这种情况，处理的应该是n本身未素数的情况</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>求1-n中所有数的欧拉函数值，复杂度为$O(nlognlogn)$，算法如下(是一种自底向上枚举素数的做法，有点难解释)</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">phi_table</span><span class="params">(<span class="keyword">int</span> n, <span class="keyword">int</span>* phi)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=n; i++)phi[i] = <span class="number">0</span>;<span class="comment">// 初始化</span></span><br><span class="line">    phi[<span class="number">1</span>] = <span class="number">1</span>;<span class="comment">// 初始化</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=n; i++)<span class="keyword">if</span>(!phi[i])<span class="comment">// 外层循环枚举素数</span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j=i; j&lt;=n; j+=i) &#123;</span><br><span class="line">            <span class="keyword">if</span>(!phi[j])phi[j] = j;<span class="comment">// 赋初值n</span></span><br><span class="line">            phi[j] = phi[j] / i * (i<span class="number">-1</span>);<span class="comment">// 参见欧拉函数</span></span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>(Working in progress)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ACM算法笔记&lt;/p&gt;
    
    </summary>
    
      <category term="Note" scheme="http://yoursite.com/categories/Note/"/>
    
    
      <category term="Programming" scheme="http://yoursite.com/tags/Programming/"/>
    
      <category term="C++" scheme="http://yoursite.com/tags/C/"/>
    
      <category term="Algorithm" scheme="http://yoursite.com/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>OCaml Learning Notes</title>
    <link href="http://yoursite.com/2019/09/12/OCaml%20Learning%20Notes/"/>
    <id>http://yoursite.com/2019/09/12/OCaml Learning Notes/</id>
    <published>2019-09-12T23:54:34.000Z</published>
    <updated>2019-11-14T05:54:41.583Z</updated>
    
    <content type="html"><![CDATA[<p>OCaml Notes, CSCI 2041, DNA of programming languages.</p><a id="more"></a><h1 id="Part-1-Basics"><a href="#Part-1-Basics" class="headerlink" title="Part 1: Basics"></a>Part 1: Basics</h1><p>Part 1 is a basic introduction of OCaml syntax and characteristics as a functional language. Also we introduce some mechanism such as <strong>pattern matching</strong> and <strong>polymorphism</strong> to make it distinguished.</p><p>First of all, the best REPL choice is <code>utop</code> program running under the terminal. Under this interactive environment, the OCaml phrases terminated with <code>;;</code> will be evaluated. User inputs (system command) are specified with <code>#</code>  at the beginning.</p><p>Type assertion: <code>exp : typ</code>. The type assertion is said to be valid iff the expression <code>exp</code> has type <code>typ</code>.</p><h2 id="Binding"><a href="#Binding" class="headerlink" title="Binding"></a>Binding</h2><p>Binding is permanent and can not be changed within specific scope of the program (normally the global scope).</p><p>Syntax of <strong>type binding</strong>:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> tycon = typ</span><br></pre></td></tr></table></figure><p>Semantics: <code>tycon</code> is a type constructor, bound to type <code>typ</code></p><p>Syntax of <strong>value binding</strong>:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> var : typ = exp</span><br></pre></td></tr></table></figure><p>Semantics: <code>var</code> is a variable, introduced the type <code>typ</code> and value <code>exp</code>. Notice that we must specify both the type and value of this type. There will be type-checking when binding, if succeed, expression <code>exp</code> will be evaluated first, and then its value is bound to variable <code>var</code>. If <code>exp</code> does not have a value, then binding never happens. (Notice that <code>None</code> is a value.)</p><p>Variable name must start with a lowercase letter or an underscore and excludes other punctuations.</p><p><strong>Purpose of binding</strong>: Make a variable or type constructor available for use within its scope.</p><p><strong>Substitution principle</strong>: A bound variable or type constructor is implicitly <em>replaced</em> by its binding prior to type checking and evaluation.</p><p><strong>Compound Declarations (Bindings)</strong>: This is defined by sequential bindings of the form <em>dec1, dec2, …decn</em>, with nested scope.</p><p><strong>Binding is not assignment</strong>: The binding of a variable never changes, once bound to a value, it is always bound to that value (within the scope of the binding). But we can <em>shadow</em> a binding by introducing a second binding for a variable within the scope of the first binding. One might say that old bindings never die, they just fade away.</p><p><strong>Type environment records the types of variables. Value environment records their values.</strong></p><p><strong>local binding</strong>: </p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> x = u <span class="keyword">in</span> v</span><br></pre></td></tr></table></figure><p>The binding of <code>x</code> to the result of <code>u</code> is only visible within the expression <code>v</code>. The semantics is to evaluate <code>u</code> first and bind to variable <code>x</code>, finally evaluate <code>v</code> with this bounded variable <code>x</code>.</p><p>Notes: local binding shadowing only happens during that binding period, for example:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> x = <span class="number">1</span></span><br><span class="line"><span class="keyword">let</span> y = <span class="keyword">let</span> x = <span class="number">2</span> <span class="keyword">in</span> x</span><br><span class="line"><span class="keyword">let</span> z = x + <span class="number">2</span></span><br><span class="line"><span class="comment">(* z will be binded to 3, not 4 because the global binding of x becomes visible again *)</span></span><br></pre></td></tr></table></figure><h2 id="Primitive-Data-Types"><a href="#Primitive-Data-Types" class="headerlink" title="Primitive Data Types"></a>Primitive Data Types</h2><ul><li>int (31-bit sign int on 32-bit processors or 63-bit sign int on 64-bit processors)</li><li><p>float (IEEE double-precision floating point)</p></li><li><p>booleans</p></li></ul><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># (<span class="number">1</span> &lt; <span class="number">2</span>) = <span class="literal">false</span>;;</span><br><span class="line">- : <span class="built_in">bool</span> = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"># <span class="keyword">let</span> one = <span class="keyword">if</span> <span class="literal">true</span> <span class="keyword">then</span> <span class="number">1</span> <span class="keyword">else</span> <span class="number">2</span>;;</span><br><span class="line"><span class="keyword">val</span> one : <span class="built_in">int</span> = <span class="number">1</span></span><br></pre></td></tr></table></figure><ul><li>characters (8-bit)</li></ul><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># <span class="string">'a'</span>;;</span><br><span class="line">- : <span class="built_in">char</span> = <span class="string">'a'</span></span><br><span class="line"></span><br><span class="line"># int_of_char <span class="string">'\n'</span></span><br><span class="line">- : <span class="built_in">int</span> = <span class="number">10</span></span><br></pre></td></tr></table></figure><ul><li>unit ()</li></ul><p>Unit type is useful in signaling the completion of some program. Ex:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">match</span> print_endline <span class="string">"first string"</span> <span class="keyword">with</span></span><br><span class="line">| <span class="literal">()</span> -&gt; print_endline <span class="string">"second string"</span></span><br></pre></td></tr></table></figure><ul><li>immutable character strings</li></ul><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># <span class="string">"Hello"</span> ^ <span class="string">" "</span> ^ <span class="string">"world"</span>;;</span><br><span class="line">- : <span class="built_in">string</span> = <span class="string">"Hello world"</span></span><br></pre></td></tr></table></figure><h2 id="Compound-Data-Types"><a href="#Compound-Data-Types" class="headerlink" title="Compound Data Types:"></a>Compound Data Types:</h2><ul><li>Tuples</li></ul><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># <span class="keyword">let</span> a_tuple : <span class="built_in">int</span> * <span class="built_in">string</span> = (<span class="number">3</span>, <span class="string">"three"</span>);;</span><br></pre></td></tr></table></figure><p>A tuple is an ordered collection of values that can each be of a different type.</p><ul><li>Lists</li></ul><p>Actually </p><p>List is a build-in variants:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> <span class="symbol">'a</span> <span class="built_in">list</span> = <span class="literal">[]</span> | (::) <span class="keyword">of</span> <span class="symbol">'a</span> * <span class="symbol">'a</span> <span class="built_in">list</span></span><br></pre></td></tr></table></figure><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># <span class="keyword">let</span> l = [<span class="string">"is"</span>; <span class="string">"a"</span>; <span class="string">"tale"</span>; <span class="string">"told"</span>; <span class="string">"etc."</span>];;</span><br><span class="line"><span class="keyword">val</span> l : <span class="built_in">string</span> <span class="built_in">list</span> = [<span class="string">"is"</span>; <span class="string">"a"</span>; <span class="string">"tale"</span>; <span class="string">"told"</span>; <span class="string">"etc."</span>]</span><br><span class="line"></span><br><span class="line"># <span class="string">"Life"</span> :: l;;</span><br><span class="line">- : <span class="built_in">string</span> <span class="built_in">list</span> = [<span class="string">"Life"</span>; <span class="string">"is"</span>; <span class="string">"a"</span>; <span class="string">"tale"</span>; <span class="string">"told"</span>; <span class="string">"etc."</span>]</span><br></pre></td></tr></table></figure><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># [<span class="number">1</span>; <span class="number">2</span>; <span class="number">3</span>];;</span><br><span class="line"># <span class="number">1</span> :: (<span class="number">2</span> :: (<span class="number">3</span> :: <span class="literal">[]</span>));;</span><br><span class="line"># <span class="number">1</span> :: <span class="number">2</span> :: <span class="number">3</span> :: <span class="literal">[]</span>;;</span><br></pre></td></tr></table></figure><p>The bracket notation is a syntax sugar for <code>::</code>. <code>::</code> is right-associative.</p><p><code>@</code> can be used to concatenate two lists. (not a constant-time operation)</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># [<span class="number">1</span>; <span class="number">2</span>; <span class="number">3</span>] @ [<span class="number">4</span>; <span class="number">5</span>; <span class="number">6</span>];;</span><br></pre></td></tr></table></figure><ul><li>Options</li></ul><p>This is also a built-in variants:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> <span class="symbol">'a</span> option = <span class="type">None</span> | <span class="type">Some</span> <span class="keyword">of</span> <span class="symbol">'a</span></span><br></pre></td></tr></table></figure><p>An option is used to express that a value might or might not be present.</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># <span class="keyword">let</span> divide x y =</span><br><span class="line"><span class="keyword">if</span> y = <span class="number">0</span> <span class="keyword">then</span> <span class="type">None</span> <span class="keyword">else</span> <span class="type">Some</span> (x/y) ;;</span><br></pre></td></tr></table></figure><p>Keywords <code>Some</code> and <code>None</code> are construtors that let us build optional values. To examine the contents of an option, we use pattern matching.</p><p>The <code>None</code> type is useful in many situations, for example, when allocating memory, if the allocation fails, return <code>None</code> value is usually a good choice.</p><ul><li>Records</li></ul><p>We can also define our own data type.</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># <span class="keyword">type</span> point2d = &#123; x : <span class="built_in">float</span>; y : <span class="built_in">float</span>&#125;;;</span><br></pre></td></tr></table></figure><p>Dot notation for accessing record fields:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> magnitude &#123; x; y&#125; = sqrt (x ** <span class="number">2.</span> +. y ** <span class="number">2.</span>);;</span><br><span class="line"># <span class="keyword">let</span> distance v1 v2 =</span><br><span class="line">magnitude &#123; x = v1.x -. v2.x; y = v1.y -. v2.y&#125;;;</span><br></pre></td></tr></table></figure><h2 id="Function"><a href="#Function" class="headerlink" title="Function"></a>Function</h2><p>OCaml use <em>typer inference</em> to work out the types.</p><p>OCaml doesn’t do any implicit casting nor do any automatic conversion.</p><p>A function which takes input of type <code>a</code> and can often give an output of type <code>b</code> may declare itself as type <code>a -&gt; b option</code>, indicating that it may give a <code>None</code> value for some reason.</p><h3 id="Function-Expression"><a href="#Function-Expression" class="headerlink" title="Function Expression"></a>Function Expression</h3><p>Function is an expression which can be evaluated.</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">(* syntax for a function expression*)</span></span><br><span class="line"><span class="keyword">fun</span> n -&gt; n + <span class="number">1</span>;;</span><br><span class="line"></span><br><span class="line"><span class="comment">(* applying a function to a value *)</span></span><br><span class="line">(<span class="keyword">fun</span> n -&gt; n + <span class="number">1</span>) <span class="number">2</span>;;</span><br></pre></td></tr></table></figure><h3 id="Function-type"><a href="#Function-type" class="headerlink" title="Function type"></a>Function type</h3><p>Each function has a type, which can be writen as:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a -&gt; b</span><br></pre></td></tr></table></figure><p>Where <code>a</code> is its domain, and <code>b</code> is its codoman.</p><p>The arrow operation is right-associative, so the following two type specifications are the same:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">int</span> -&gt; (<span class="built_in">int</span> -&gt; (<span class="built_in">bool</span> -&gt; <span class="built_in">int</span>))</span><br><span class="line"><span class="built_in">int</span> -&gt; <span class="built_in">int</span> -&gt; <span class="built_in">bool</span> -&gt; <span class="built_in">int</span></span><br></pre></td></tr></table></figure><h3 id="Annotations"><a href="#Annotations" class="headerlink" title="Annotations"></a>Annotations</h3><p>There may be some ambiguity for function annotation, for example:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> x : <span class="built_in">int</span> -&gt; exp</span><br></pre></td></tr></table></figure><p>actually means</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> x -&gt; (exp : <span class="built_in">int</span>)</span><br></pre></td></tr></table></figure><p>To annotate the parameter, we have to include a parathesis:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">fun</span> (x : <span class="built_in">int</span>) -&gt; exp</span><br></pre></td></tr></table></figure><h3 id="Function-with-Multiple-Parameters"><a href="#Function-with-Multiple-Parameters" class="headerlink" title="Function with Multiple Parameters"></a>Function with Multiple Parameters</h3><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">(* syntax, the parameter type and expression type is auto-checked by type checking mechanism*)</span></span><br><span class="line"><span class="keyword">fun</span> x y -&gt; x +. y;;</span><br><span class="line"></span><br><span class="line"><span class="comment">(* apply to 2 arguments *)</span></span><br><span class="line">(<span class="keyword">fun</span> x y -&gt; x +. y) <span class="number">3.</span> <span class="number">4.</span> ;;</span><br></pre></td></tr></table></figure><h3 id="Bind-Function-to-Variable"><a href="#Bind-Function-to-Variable" class="headerlink" title="Bind Function to Variable"></a>Bind Function to Variable</h3><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> f = <span class="keyword">fun</span> n -&gt; n + <span class="number">1</span>;;</span><br><span class="line">f <span class="number">2</span>;;</span><br></pre></td></tr></table></figure><h3 id="Define-Named-Function-another-syntax"><a href="#Define-Named-Function-another-syntax" class="headerlink" title="Define Named Function (another syntax)"></a>Define Named Function (another syntax)</h3><ul><li><code>let f = fun p1 p2 ... -&gt; expr</code></li><li><code>let f p1 p2 ... = expr</code></li></ul><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> f x = x + <span class="number">1</span>;;</span><br><span class="line">f <span class="number">3</span>;;</span><br><span class="line"><span class="keyword">let</span> f x y = x + y;;</span><br><span class="line">f <span class="number">3</span> <span class="number">4</span>;;</span><br></pre></td></tr></table></figure><h2 id="Polymorphism"><a href="#Polymorphism" class="headerlink" title="Polymorphism"></a>Polymorphism</h2><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> id : <span class="symbol">'a</span> . <span class="symbol">'a</span> -&gt; <span class="symbol">'a</span></span><br><span class="line">  = <span class="keyword">fun</span> x -&gt; x</span><br></pre></td></tr></table></figure><p>By type annotating the function in this way, we are saying that <code>id</code> function is of type <code>a -&gt; a</code> for any type <code>a</code>. For example, <code>id</code> can be of the types <code>int -&gt; int</code>, <code>bool -&gt; bool</code> or <code>string -&gt; string</code>. The type checking will all pass by annotating in this way.</p><h3 id="Syntax"><a href="#Syntax" class="headerlink" title="Syntax"></a>Syntax</h3><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">'a1</span> <span class="symbol">'a2</span> <span class="symbol">'a3</span> ... <span class="symbol">'an</span> . <span class="keyword">type</span></span><br></pre></td></tr></table></figure><p>The type variables before the dot <code>.</code> Are like universal quantifiers, forcing OCaml to check the generality of the value.</p><h1 id="Part-2-Induction"><a href="#Part-2-Induction" class="headerlink" title="Part 2: Induction"></a>Part 2: Induction</h1><p><strong>Non-effect assumption: we can change the evaluation order and assume every expression will terminate.</strong></p><p>Mathematical induction has two important components:</p><ol><li>Case analysis.</li><li>The ability to assume that the theorem we want to prove holds for “smaller” objects.</li></ol><p>General form of induction works on any mathematical objects with <strong>well-founded</strong> relation (no infinite descending chains). The order chosen for induction is called induction order.</p><p>The proof following the general form of induction involves:</p><ul><li>We want to prove some property <code>P(x)</code> about some collection of mathematical objects <code>x</code>.</li><li>We choose a good order <code>R</code> as the induction order. (For the mathematical induction, <code>R(x,y)</code> if and only if <code>x + 1 = y</code>.)</li><li>We prove that, for any <code>x</code>, if <code>P(x&#39;)</code> holds for any <code>x&#39;</code> such that <code>R(x&#39;, x)</code>, then <code>P(x)</code> holds.</li><li>We conclude that the property <code>P(x)</code> holds for every such mathematical object <code>x</code>.</li></ul><p>Examples of <strong>well-founded</strong> relation: “subtree” relation for binary trees, “sublist” relation for finite lists.</p><p>For example, for <code>a list</code>, we define the relation as <code>R(l1, l2)</code> if and only if <code>l1</code> is a proper suffix of <code>l2</code>. The partition for lists can be either empty list <code>[]</code> with no suffix, or <code>hd :: tl</code>. Then, we perform the following steps:</p><ul><li>We want to prove some property <code>P(l)</code> about values <code>l</code> of type <code>a list</code>.</li><li>We show that <code>P([])</code> holds. (There is no proper suffix).</li><li>We show that for non-empty list <code>l</code> of form <code>hd :: tl</code>, if <code>P(l&#39;)</code> holds for all proper suffixes, then <code>P(l)</code> is true.</li><li>We conclude that the property <code>P(l)</code> holds for any list <code>l</code>.</li></ul><p>Suggestions for deciding ordering and the partition: follow the pattern matching and recursive calls.</p><p><a href="https://github.umn.edu/umn-csci-2041-f19/notes" target="_blank" rel="noopener">Some induction proof</a> (Reference from Favonia)</p><h1 id="Part-3-Effects"><a href="#Part-3-Effects" class="headerlink" title="Part 3: Effects"></a>Part 3: Effects</h1><p>Every OCaml expression either</p><ul><li>evaluates to a value</li><li>raises an exception</li><li>or fails to terminate (“infinite loop”)</li></ul><h2 id="Exceptions"><a href="#Exceptions" class="headerlink" title="Exceptions"></a>Exceptions</h2><p>OCaml’s exception mechanism is similar to many other programming languages.</p><p>Syntax of defining a new type of OCaml exception:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">exception</span> <span class="type">E</span> <span class="keyword">of</span> t</span><br></pre></td></tr></table></figure><p><code>E</code> is a constructor name and <code>t</code> is a type. The <code>of t</code> part is optional, similar to <code>type variants</code>.</p><p>Create an <code>exception</code> value:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">Failure</span> <span class="string">"something went wrong"</span></span><br></pre></td></tr></table></figure><p>Raise and exception value <code>e</code>:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">raise e</span><br></pre></td></tr></table></figure><p>Catch an exception, use:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> e <span class="keyword">with</span></span><br><span class="line">| p1 -&gt; e1</span><br><span class="line">| ...</span><br><span class="line">| pn -&gt; en</span><br></pre></td></tr></table></figure><p>The expression <code>e</code> may raise an exception. If it does not, the entire expression evaluate to what <code>e</code> does. If <code>e</code> does raise an exception value <code>v</code>, <code>v</code> is matched against the provided patterns, like pattern matching.</p><p>All exception values have type <code>exn</code>.</p><h3 id="Common-Usage-of-Exceptions"><a href="#Common-Usage-of-Exceptions" class="headerlink" title="Common Usage of Exceptions"></a>Common Usage of Exceptions</h3><ol><li>Indicators of an error.</li><li>Placeholders for unfinished code.</li><li>Out-of-band communication.</li></ol><h2 id="Sequencing-of-Effects"><a href="#Sequencing-of-Effects" class="headerlink" title="Sequencing of Effects"></a>Sequencing of Effects</h2><p>We can use semicolon operator to sequence effects.</p><ul><li>Syntax: <code>e1; e2</code></li><li>Dynamic semantics:(动态语义涉及程序运行顺序以及结果) To evaluate <code>e1; e2</code>,<ul><li>First evaluate <code>e1</code> to a value <code>v1</code>.</li><li>Then evaluate <code>e2</code> to a value <code>v2</code>.</li><li>Return <code>v2</code>. (Discard <code>v1</code>)</li></ul></li><li>Static semantics: (静态语义涉及表达式类型) <code>e1; e2 : t</code> if <code>e1 : unit</code> and <code>e2 : t</code>.</li></ul><p>Note that all expressions except for the last one should have type <code>unit</code>, otherwise OCaml will raise a warning.</p><h1 id="Part-4-Modules"><a href="#Part-4-Modules" class="headerlink" title="Part 4: Modules"></a>Part 4: Modules</h1><h2 id="Modular-Programming"><a href="#Modular-Programming" class="headerlink" title="Modular Programming"></a>Modular Programming</h2><p>One key solution to managing complexity of large software is <em>modular programming</em>. The key idea of modular programming is “local reasoning”, for which the reasoning is only about just the module. The solution to local reasoning is abstraction (module specification), for which we only need to design the functionality of each components without considering how it is implemented.</p><h3 id="Module-Systems"><a href="#Module-Systems" class="headerlink" title="Module Systems"></a>Module Systems</h3><ul><li>Namespaces: a set of names are grouped together. In OCaml, we can use <em>structures</em> to group names.</li><li>Abstraction: hides information, enables encapsulation, information hiding. In OCaml, we use <em>signature</em> to abstract structures by hiding some of the structure’s names.</li><li>Code reuse: enables code from one module to be used as part of another module without having to copy. In OCaml, we can use <em>functirs</em> and <em>includes</em> to reuse code.</li></ul><h3 id="OCaml-Modules"><a href="#OCaml-Modules" class="headerlink" title="OCaml Modules"></a>OCaml Modules</h3><p>The OCaml module system is based on <em>structures</em> and <em>signatures</em>. In which <em>structures</em> are the core and <em>signatures</em> are the types of structures.</p><h4 id="Structures"><a href="#Structures" class="headerlink" title="Structures"></a>Structures</h4><p>syntax:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> <span class="type">ModuleName</span> = <span class="type">Struct</span></span><br><span class="line"><span class="comment">(* definition *)</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>structure:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="comment">(* definitions *)</span> <span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>A structure is a sequence of definitions, the structure itself is anonymous (has no name).</p><p>Modules partition the namespace, any symbol <code>x</code> in side the module must be accessed with the module name <code>ModuleName.x</code> outside the module. (or use <code>open</code> to make the namespace public)</p><p>The implementation of a module can contain <code>type</code>, <code>exception</code>, <code>let</code>, <code>open</code> and some others.</p><h4 id="Scope"><a href="#Scope" class="headerlink" title="Scope"></a>Scope</h4><p>After a module <code>M</code> has been defined, we can access the names by using the <code>.</code> operator. For example: <code>M.x</code>.</p><p>We can also make the namepace public by using key word <code>open</code>: <code>open M</code>.</p><p>Semantics meaning of <code>open String</code>:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> length = <span class="type">String</span>.length</span><br><span class="line"><span class="keyword">let</span> get = <span class="type">String</span>.get</span><br><span class="line"><span class="keyword">let</span> lowercase_ascii = <span class="type">String</span>.lowercase_ascii</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>The module <em>Stdlib</em> is automatically opened in every OCaml program.</p><p>The shadowing mechanism also fits for module opening:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> <span class="type">M</span> = <span class="keyword">struct</span> <span class="keyword">let</span> x = <span class="number">42</span> <span class="keyword">end</span></span><br><span class="line"><span class="keyword">module</span> <span class="type">N</span> = <span class="keyword">struct</span> <span class="keyword">let</span> x = <span class="string">"bigred"</span> <span class="keyword">end</span></span><br><span class="line"><span class="keyword">open</span> <span class="type">M</span></span><br><span class="line">oepn <span class="type">N</span></span><br><span class="line"><span class="comment">(* The name defined later shadows the name defined before. *)</span></span><br></pre></td></tr></table></figure><p>If we have multiple modules, it’s generally good practice not to <code>open</code> all the modules at the top of the program.</p><p>One fatanstic solution to avoid collision is “local opening”:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">(* without [open] *)</span></span><br><span class="line"><span class="keyword">let</span> f x =</span><br><span class="line"><span class="keyword">let</span> y = <span class="type">List</span>.filter ((&gt;) <span class="number">0</span>) x <span class="keyword">in</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment">(* with [open] *)</span></span><br><span class="line"><span class="keyword">let</span> f x =</span><br><span class="line"><span class="keyword">let</span> <span class="keyword">open</span> <span class="type">List</span> <span class="keyword">in</span></span><br><span class="line"><span class="keyword">let</span> y =filter ((&gt;) <span class="number">0</span>) x <span class="keyword">in</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><h4 id="Signatures"><a href="#Signatures" class="headerlink" title="Signatures"></a>Signatures</h4><p>We can use module type to describe the modules. One solution to defining module types is <em>signatures</em>.</p><p>Syntax:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> <span class="keyword">type</span> <span class="type">ModuleTypeName</span> = <span class="keyword">sig</span></span><br><span class="line"><span class="comment">(* declarations *)</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>Stack signature:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">module type Stack = sig</span><br><span class="line">type &apos;a stack</span><br><span class="line">val empty : &apos;a stack</span><br><span class="line">val is_empty : &apos;a stack -&gt; bool</span><br><span class="line">val push : &apos;a -&gt; &apos;a stack -&gt; &apos;a stack</span><br><span class="line">val peek : &apos;a stack -&gt; &apos;a</span><br><span class="line">val pop : &apos;a stack -&gt; &apos;a stack</span><br><span class="line">end</span><br></pre></td></tr></table></figure><p>By convention, the module type name is also capitalized.</p><p><code>val id : t</code> means there is a value named <code>id</code> whose type is <code>t</code>.</p><p>A structure matches a signature if the structure provides definitions for all the names specified in the signature and the types are also matched. (Allowing polymorphism)</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> <span class="keyword">type</span> <span class="type">Sig</span> = <span class="keyword">sig</span></span><br><span class="line"><span class="keyword">val</span> f : <span class="built_in">int</span> -&gt; <span class="built_in">int</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">module</span> <span class="type">M1</span> : <span class="type">Sig</span> = <span class="keyword">struct</span></span><br><span class="line"><span class="keyword">let</span> f x = x+<span class="number">1</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">module</span> <span class="type">M2</span> : <span class="type">Sig</span> = <span class="keyword">struct</span></span><br><span class="line"><span class="keyword">let</span> f x = x</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>Module <code>M1</code> provides a function <code>f</code> of type <code>int -&gt; int</code>. Module <code>M2</code> provides a function of type <code>&#39;a -&gt; &#39;a</code>. Both <code>M1</code> and <code>M2</code> match <code>Sig</code>.</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> <span class="type">ListStack</span> : <span class="type">Stack</span> = <span class="keyword">struct</span></span><br><span class="line"><span class="keyword">type</span> <span class="symbol">'a</span> stack = <span class="symbol">'a</span> <span class="built_in">list</span></span><br><span class="line"><span class="comment">(* rest *)</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h4 id="Abstract-Types"><a href="#Abstract-Types" class="headerlink" title="Abstract Types"></a>Abstract Types</h4><p>The type <code>&#39;a stack</code> above is abstract. The <code>Stack</code> module type says that there is a type name <code>&#39;a stack</code> in any module that implements the module type.</p><p>A module that implements a module type must specify concrete types for the abstract types in the signature and define all the names declared in the signature.</p><p>Only declarations in the signature are accessible outside of the module. (encapsulation)</p><p>An OCaml convention to short name the declaration:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> <span class="keyword">type</span> <span class="type">Stack</span> = <span class="keyword">sig</span></span><br><span class="line"><span class="keyword">type</span> <span class="symbol">'a</span> t</span><br><span class="line"><span class="keyword">val</span> empty : <span class="symbol">'a</span> t</span><br><span class="line"><span class="keyword">val</span> is_empty : <span class="symbol">'a</span> t -&gt; <span class="built_in">bool</span></span><br><span class="line"><span class="keyword">val</span> push : <span class="symbol">'a</span> -&gt; <span class="symbol">'a</span> t -&gt; <span class="symbol">'a</span> t</span><br><span class="line"><span class="keyword">val</span> peek : <span class="symbol">'a</span> t -&gt; <span class="symbol">'a</span></span><br><span class="line"><span class="keyword">val</span> pop : <span class="symbol">'a</span> t -&gt; <span class="symbol">'a</span> t</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>Custom Printers:</p><p><a href="https://www.cs.cornell.edu/courses/cs3110/2019fa/textbook/modules/abstract_types.html" target="_blank" rel="noopener">https://www.cs.cornell.edu/courses/cs3110/2019fa/textbook/modules/abstract_types.html</a></p><h4 id="Semantics"><a href="#Semantics" class="headerlink" title="Semantics"></a>Semantics</h4><p>Semantics of the OCaml module system</p><ul><li>Dynamic semantics: To evaluate a structure <code>structure D1; ...; Dn end</code> where each of the <code>Di</code> is a definition, evaluate each definition in order.</li><li>Static semantics: For <code>module M : T = struct ... end</code>. There are two checks the compiler must perform:<ul><li>Signature matching: every name declared in <code>T</code> must be defined in <code>M</code>.</li><li>Encapsulation: any name defined in <code>M</code> that does not appear in <code>T</code> is not visible to code outside of <code>M</code>.</li></ul></li></ul><h4 id="Functional-Data-Structures"><a href="#Functional-Data-Structures" class="headerlink" title="Functional Data Structures"></a>Functional Data Structures</h4><p>Functional data structures have the property of being persistent: updating the data structure with one of its operations does not change the existing version of the data structure but instead produces a new version. Both exist and both can be accessed. The opposite of a persistent data structure is an ephemeral data structure: changes are destructive so that only one version exists at any time. Both persistent and ephemeral data structures can be built in both functional and imperative languages.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># open ListStack;;</span><br><span class="line"># let s = push 1 (push 2 empty);;</span><br><span class="line">val s : int list = [1; 2]</span><br><span class="line"># let s&apos; = pop s;;</span><br><span class="line">val s&apos; : int list = [2]</span><br><span class="line"># s;;</span><br><span class="line">- : int list = [1; 2]</span><br></pre></td></tr></table></figure><p>The value <code>s</code> is unchanged by the <code>pop</code> operation; both version of the stack coexist.</p><h4 id="Examples-Stack"><a href="#Examples-Stack" class="headerlink" title="Examples: Stack"></a>Examples: Stack</h4><h4 id="Examples-Queues"><a href="#Examples-Queues" class="headerlink" title="Examples: Queues"></a>Examples: Queues</h4><h4 id="Examples-Dictionaries"><a href="#Examples-Dictionaries" class="headerlink" title="Examples: Dictionaries"></a>Examples: Dictionaries</h4><h4 id="Examples-Sets"><a href="#Examples-Sets" class="headerlink" title="Examples: Sets"></a>Examples: Sets</h4><h4 id="Examples-Arithmetic"><a href="#Examples-Arithmetic" class="headerlink" title="Examples: Arithmetic"></a>Examples: Arithmetic</h4><h3 id="Sharing-Constraints"><a href="#Sharing-Constraints" class="headerlink" title="Sharing Constraints"></a>Sharing Constraints</h3><p>Syntax</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> <span class="type">Ints</span> : (<span class="type">Arith</span> <span class="keyword">with</span> <span class="keyword">type</span> t = <span class="built_in">int</span>) = <span class="keyword">struct</span></span><br><span class="line"><span class="comment">(* all of Ints as before *)</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>We can write sharing constraints that refine a signature by specifying equations that must hold on the abstract types in that signature. E.g. If <code>T</code> is a module type containing an abstract type <code>t</code>, then <code>T with type t = int</code> is a new module type that is the same <code>T</code>, except that <code>t</code> is known to be <code>int</code>. </p><h3 id="Compilation-Units"><a href="#Compilation-Units" class="headerlink" title="Compilation Units"></a>Compilation Units</h3><p>A compilation unit is a pair of OCaml source files in the same directory. They share the same prefix but their extension differ: one file is <code>x.ml</code>, the other is <code>x.mli</code>. The file <code>x.ml</code> is called the implementation, and <code>x.mli</code> is called the interface. Similar to <code>x.c</code> and <code>x.h</code>.</p><p>E.g:</p><p><code>foo.mli</code></p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> x : <span class="built_in">int</span></span><br><span class="line"><span class="keyword">val</span> f : <span class="built_in">int</span> -&gt; <span class="built_in">int</span> -&gt; <span class="built_in">int</span></span><br></pre></td></tr></table></figure><p><code>foo.ml</code></p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> <span class="type">Foo</span> : <span class="keyword">sig</span></span><br><span class="line"><span class="keyword">val</span> x : <span class="built_in">int</span></span><br><span class="line"><span class="keyword">val</span> f : <span class="built_in">int</span> -&gt; <span class="built_in">int</span> -&gt; <span class="built_in">int</span></span><br><span class="line"><span class="keyword">end</span> = <span class="keyword">struct</span></span><br><span class="line"><span class="keyword">let</span> x = <span class="number">0</span></span><br><span class="line"><span class="keyword">let</span> y = <span class="number">12</span></span><br><span class="line"><span class="keyword">let</span> f x y = x + y</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>In general, when the compiler encounters a compilation unit, it treats them as defining a module and a signature like this:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> <span class="type">Foo</span> : <span class="keyword">sig</span> <span class="comment">(* insert contents of foo.mli here *)</span> <span class="keyword">end</span></span><br><span class="line"><span class="comment">(* insert contents of foo.ml here *)</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>The unit name <code>Foo</code> is derived from the base name <code>foo</code> by just capitalizing the first letter. No named module type is defined, the signature of <code>Foo</code> is anonymous.</p><h3 id="Comments"><a href="#Comments" class="headerlink" title="Comments"></a>Comments</h3><p>The comments that go in an interface file vs. an implementation file are different. </p><p>Interface files will be read by clients of an abstraction. These will generally be specification comments describing how to use the abstraction, the preconditions and what exceptions they might raise, even time complexity and space complexity.</p><p>Implementation files will be read by programmers and maintainers of an abstraction, so the comments that go there are for them. These will generally be specification about how the representation type is used, how the code works.</p><h3 id="Code-Reuse"><a href="#Code-Reuse" class="headerlink" title="Code Reuse"></a>Code Reuse</h3><h4 id="Includes"><a href="#Includes" class="headerlink" title="Includes"></a>Includes</h4><p>OCaml provides a language features called included that enables code reuse, similar to the object-oriented language. It enables a structure to include all the values defined by another structure, or a signature to include all the names declared by another signature.</p><p>E.g:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> <span class="type">ListSetDupsExtented</span> = <span class="keyword">struct</span></span><br><span class="line"><span class="keyword">include</span> <span class="type">ListSetDups</span></span><br><span class="line"><span class="keyword">let</span> of_list lst = <span class="type">List</span>.fold_right add lst empty</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>We can also provide a new implementation of one of includedd functions:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> <span class="type">ListSetDupsExtended</span> = <span class="keyword">struct</span></span><br><span class="line"><span class="keyword">include</span> <span class="type">ListSetDups</span></span><br><span class="line"><span class="keyword">let</span> of_list lst = <span class="type">List</span>.fold_right add lst empty</span><br><span class="line"><span class="keyword">let</span> <span class="keyword">rec</span> elts = <span class="keyword">function</span></span><br><span class="line">| <span class="literal">[]</span> -&gt; <span class="literal">[]</span></span><br><span class="line">| h::t -&gt; <span class="keyword">if</span> mem h t <span class="keyword">then</span> elts' t <span class="keyword">else</span> h::(elts' t)</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>The new implementation doesn’t replace the old one. Since the module evaluation is from top to bottom.</p><p>Include vs. Open:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> <span class="type">M</span> = <span class="keyword">struct</span></span><br><span class="line"><span class="keyword">let</span> x = <span class="number">0</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">module</span> <span class="type">N</span> = <span class="keyword">struct</span></span><br><span class="line"><span class="keyword">include</span> <span class="type">M</span></span><br><span class="line"><span class="keyword">let</span> y = x + <span class="number">1</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">module</span> <span class="type">O</span> = <span class="keyword">struct</span></span><br><span class="line"><span class="keyword">open</span> <span class="type">M</span></span><br><span class="line"><span class="keyword">let</span> y = x + <span class="number">1</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>Dynamic semantics:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> <span class="type">M</span> : <span class="keyword">sig</span> <span class="keyword">val</span> x : <span class="built_in">int</span> <span class="keyword">end</span></span><br><span class="line"><span class="keyword">module</span> <span class="type">N</span> : <span class="keyword">sig</span> <span class="keyword">val</span> x : <span class="built_in">int</span> <span class="keyword">val</span> y : <span class="built_in">int</span> <span class="keyword">end</span></span><br><span class="line"><span class="keyword">module</span> <span class="type">O</span> : <span class="keyword">sig</span> <span class="keyword">val</span> y : <span class="built_in">int</span> <span class="keyword">end</span></span><br></pre></td></tr></table></figure><p><code>Include</code> causes all the definitions to also be included. But <code>open</code> only makes those definitions available in the scope, it doesn’t make them part of the structure.</p><h4 id="Functors"><a href="#Functors" class="headerlink" title="Functors"></a>Functors</h4><p>A functor is simply a function from structures to structures.</p><p>E.g:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> <span class="keyword">type</span> <span class="type">X</span> = <span class="keyword">sig</span></span><br><span class="line"><span class="keyword">val</span> x : <span class="built_in">int</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> <span class="type">IncX</span> (<span class="type">M</span> : <span class="type">X</span>) = <span class="keyword">struct</span></span><br><span class="line"><span class="keyword">let</span> x = <span class="type">M</span>.x + <span class="number">1</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>Apply functors:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> <span class="type">A</span> = <span class="keyword">struct</span> <span class="keyword">let</span> x = <span class="number">0</span> <span class="keyword">end</span></span><br><span class="line"><span class="keyword">module</span> <span class="type">B</span> = <span class="type">IncX</span>(<span class="type">A</span>)</span><br><span class="line"><span class="keyword">module</span> <span class="type">C</span> = <span class="type">IncX</span>(<span class="type">B</span>)</span><br></pre></td></tr></table></figure><p>Syntax:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">module</span> <span class="type">F</span> (<span class="type">M</span> : <span class="type">S</span>) = <span class="keyword">struct</span></span><br><span class="line">...</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">(* anonymous functor *)</span></span><br><span class="line"><span class="keyword">module</span> <span class="type">F</span> = <span class="keyword">functor</span> (<span class="type">M</span> : <span class="type">S</span>) -&gt; <span class="keyword">struct</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">module</span> <span class="type">F</span> (<span class="type">M1</span> : <span class="type">S1</span>) ... (<span class="type">Mn</span> : <span class="type">Sn</span>) = <span class="keyword">struct</span></span><br><span class="line">...</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">(* anonymous functor *)</span></span><br><span class="line"><span class="keyword">module</span> <span class="type">F</span> = <span class="keyword">functor</span> (<span class="type">M1</span> : <span class="type">S1</span>) -&gt; ... -&gt; <span class="keyword">functor</span> (<span class="type">Mn</span> : <span class="type">Sn</span>) -&gt; <span class="keyword">struct</span></span><br><span class="line">...</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">(* output type as a functor *)</span></span><br><span class="line"><span class="keyword">module</span> <span class="type">F</span> (<span class="type">M</span> : <span class="type">Si</span>) : <span class="type">So</span> = <span class="keyword">struct</span></span><br><span class="line">...</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">(* alternate form *)</span></span><br><span class="line"><span class="keyword">module</span> <span class="type">F</span> (<span class="type">M</span> : <span class="type">Si</span>) = (<span class="keyword">struct</span></span><br><span class="line">...</span><br><span class="line"><span class="keyword">end</span> : <span class="type">So</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">(* functor type annotation *)</span></span><br><span class="line"><span class="keyword">functor</span> (<span class="type">M</span> : <span class="type">Si</span>) -&gt; <span class="type">So</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">module</span> <span class="type">F</span> : <span class="keyword">functor</span> (<span class="type">M</span> : <span class="type">Si</span>) -&gt; <span class="type">So</span> =</span><br><span class="line"><span class="keyword">functor</span> (<span class="type">M</span> : <span class="type">Si</span>) -&gt; <span class="keyword">struct</span> ... <span class="keyword">end</span></span><br></pre></td></tr></table></figure><p><strong>One thing to note:</strong> The output of the functor can depend on the input module. This can not happend on the expression level!</p><p>Eg:</p><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">functor</span> (<span class="type">M</span> : <span class="type">ORDERED_TYPE</span>) -&gt; <span class="type">EXTENDED_ORDERED_TYPE</span> <span class="keyword">with</span> <span class="keyword">type</span> t = <span class="type">M</span>.t <span class="comment">(* signature annotation explicitly *)</span></span><br></pre></td></tr></table></figure><p>The OCaml compiler won’t complain about the signature annotation without constraints, but this is fatal since we have to make sure <code>type t</code> is the same type as module <code>M</code> (we have to make the invoking function call work in this case, otherwise there might be runtime error).</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>Last time we talked about that OCaml binding is permanent which means “immutability”. But OCaml also supports “mutability” with the “reference” language feature.</p><h3 id="Refs"><a href="#Refs" class="headerlink" title="Refs"></a>Refs</h3><p> A <em>ref</em> is like a pointer or reference in an imperative language. It is a location in memory whose contents may change. Refs are also called ref cells.</p><ul><li>Creates a reference:</li></ul><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">let</span> x = <span class="built_in">ref</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><p>By using the keyword <code>ref</code>. OCaml allocates and initializes a memory location for us and gives us the handle. <code>x</code> has type <code>int ref</code>. This is a new type constructor.</p><ul><li>Access the content:</li></ul><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!x</span><br></pre></td></tr></table></figure><p><code>!x</code> dereferences <code>x</code> and returns the contents of the memory location. <code>!</code> is the dereference operator.</p><ul><li>Assignment</li></ul><figure class="highlight ocaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x := <span class="number">1</span></span><br></pre></td></tr></table></figure><p><code>:=</code> is the assign operator in OCaml, by using this operator, we are not changing the value <code>x</code> but are changing the content which it refers to. (Variable are immutable, memory is mutable but variable bindings are not)</p><h4 id="Dynamic-Semantics"><a href="#Dynamic-Semantics" class="headerlink" title="Dynamic Semantics:"></a>Dynamic Semantics:</h4><ul><li><code>ref e</code><ul><li>Evaluate <code>e</code> to a value <code>v</code></li><li>Allocate a new location <code>loc</code> in memory to hold <code>v</code></li><li>Store <code>v</code> in <code>loc</code></li><li>Return <code>loc</code></li></ul></li><li><code>e1 := e2</code><ul><li>Evaluate <code>e2</code> to a value <code>v</code>, and <code>e1</code> to a location <code>loc</code></li><li>Store <code>v</code> in <code>loc</code></li><li>Return <code>()</code>, i.e., unit</li></ul></li><li><code>!e</code><ul><li>Evaluate <code>e</code> to a location <code>loc</code></li><li>Return the contents of <code>loc</code></li></ul></li></ul><h4 id="Static-Semantics"><a href="#Static-Semantics" class="headerlink" title="Static Semantics"></a>Static Semantics</h4><ul><li><code>ref e : t ref</code> iff <code>e : t</code></li><li><code>e1 := e2 : unit</code> iff <code>e1 : t ref</code> and <code>e2 : t</code></li><li><code>!e : t</code> iff <code>e : t ref</code></li></ul><p>Done!</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;OCaml Notes, CSCI 2041, DNA of programming languages.&lt;/p&gt;
    
    </summary>
    
      <category term="Note" scheme="http://yoursite.com/categories/Note/"/>
    
    
      <category term="Programming" scheme="http://yoursite.com/tags/Programming/"/>
    
      <category term="Ocaml" scheme="http://yoursite.com/tags/Ocaml/"/>
    
      <category term="Functional Programming" scheme="http://yoursite.com/tags/Functional-Programming/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch Tutorial</title>
    <link href="http://yoursite.com/2019/05/11/PyTorch%20Tutorial/"/>
    <id>http://yoursite.com/2019/05/11/PyTorch Tutorial/</id>
    <published>2019-05-11T16:28:34.000Z</published>
    <updated>2019-10-14T23:31:03.654Z</updated>
    
    <content type="html"><![CDATA[<p>PyTorch</p><p>Tutorial</p><a id="more"></a><h1 id="Defining-network"><a href="#Defining-network" class="headerlink" title="Defining network"></a>Defining network</h1><p>PyTorch has a standard way to create your own models. The entire definition should stay inside an object that is a child of the class <strong>nn.Module</strong>. Inside this class, there are only two methods that must be implemented. These methods are <strong><strong><strong>init</strong></strong></strong> and <strong>forward</strong>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        <span class="comment"># Defining 3 linear layers but not the way they should be connected</span></span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">240</span>,<span class="number">120</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">120</span>,<span class="number">60</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">60</span>,<span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        <span class="comment"># Defining the way that the layers of the model should be connected</span></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>You can put inside the <strong>forward</strong> method all the layers that do not have any weights to be updated. On the other hand, you should put all the layers that have weights to be updated inside the <strong><strong><strong>init</strong></strong></strong>.</p><h1 id="Loading-data-Dataset-and-Data-Loaders"><a href="#Loading-data-Dataset-and-Data-Loaders" class="headerlink" title="Loading data: Dataset and Data Loaders"></a>Loading data: Dataset and Data Loaders</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExampleDataset</span><span class="params">(Dataset)</span>:</span></span><br><span class="line">    <span class="string">"""Example Dataset"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, csv_file)</span>:</span></span><br><span class="line">        <span class="string">""" </span></span><br><span class="line"><span class="string">        csv_file (string): Path to the csv file containing data.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.data_frame = pd.read_csv(csv_file)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.data_frame)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.data_frame[idx]</span><br><span class="line"></span><br><span class="line"><span class="comment"># instantiates the dataset  </span></span><br><span class="line">example_dataset = ExampleDataset(<span class="string">'my_data_file.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># batch size: number of samples returned per iteration</span></span><br><span class="line"><span class="comment"># shuffle: Flag to shuffle the data before reading so you don't read always in the same order</span></span><br><span class="line"><span class="comment"># num_workers: used to load the data in parallel</span></span><br><span class="line">example_data_loader = DataLoader(example_dataset, batch_size=<span class="number">4</span>, shuffle=<span class="keyword">True</span>, num_workers=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Loops over the data 4 samples at a time</span></span><br><span class="line"><span class="keyword">for</span> batch_index, batch <span class="keyword">in</span> enumerate(example_data_loader):</span><br><span class="line">    print(batch_index, batch)</span><br></pre></td></tr></table></figure><ul><li><strong><strong>init</strong></strong>:</li></ul><p>In the initialization, you should put your directories information and other things that would allow to access it.</p><ul><li><strong><strong>len</strong></strong>:</li></ul><p>You should implement a way to get the entire size of your dataset.</p><ul><li><strong><strong>getitem</strong></strong>:</li></ul><p>This is where you implement how to get a single item from your dataset.</p><p>In order to more efficiently access your dataset, we use the <strong>DataLoader</strong> class. This class simply reads a batch of data at a time in parallel while optionally shuffling your data.</p><h1 id="Training-Updating-the-Network-Weights"><a href="#Training-Updating-the-Network-Weights" class="headerlink" title="Training: Updating the Network Weights"></a>Training: Updating the Network Weights</h1><p>An optimizer goes over all the weights and update them for you. The optimizer must have a criterion to use for optimization. This is where you need to define your loss function.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="comment"># instantiate your network that should be defined by you</span></span><br><span class="line">net = Net()</span><br><span class="line"><span class="comment"># create your optimizer</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># define your criterion for optimization</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"><span class="comment"># dat_set comes from somewhere</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> data_set:</span><br><span class="line">  <span class="comment"># zero the gradient buffers</span></span><br><span class="line">  optimizer.zero_grad()  </span><br><span class="line">  <span class="comment"># Passes the data through your network</span></span><br><span class="line">  output = net.forward(data)</span><br><span class="line">  <span class="comment"># calculates the loss</span></span><br><span class="line">  loss = criterion(output, target)</span><br><span class="line">  <span class="comment"># Propagates the loss back</span></span><br><span class="line">  loss.backward()</span><br><span class="line">  <span class="comment"># Updates all the weights of the network</span></span><br><span class="line">  optimizer.step()</span><br></pre></td></tr></table></figure><h1 id="Summarizing-Steps"><a href="#Summarizing-Steps" class="headerlink" title="Summarizing Steps"></a>Summarizing Steps</h1><ol><li>Define your Network class by placing the layers with weights that can be updated inside the <strong><strong><strong>init</strong></strong></strong> method. Then define how the data flows through the layers inside the <strong>forward</strong> method.</li><li>Define how your data should be loaded using the <strong>Dataset</strong> class. Then use <strong>DataLoader</strong> class to loop over your data.</li><li>Choose an optimizer and a loss function. Loop over your training data and let the optimizer update the weights of your network.</li></ol><h1 id="Cleaned-up-code-of-model-with-nn-Sequential"><a href="#Cleaned-up-code-of-model-with-nn-Sequential" class="headerlink" title="Cleaned up code of model with nn.Sequential"></a>Cleaned up code of model with nn.Sequential</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NetSeq</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(NetSeq, self).__init__()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># conv layers: feature extractor</span></span><br><span class="line">        self.conv_layers = nn.Sequential(</span><br><span class="line">        nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, kernel_size=<span class="number">5</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, kernel_size=<span class="number">5</span>),</span><br><span class="line">            nn.Dropout2d(),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.ReLU()</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># fc layers: classifier</span></span><br><span class="line">        self.fc_layers = nn.Sequential(</span><br><span class="line">        nn.Linear(<span class="number">320</span>, <span class="number">50</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Dropout(),</span><br><span class="line">            nn.Linear(<span class="number">50</span>, <span class="number">10</span>),</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.conv_layers(x)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">320</span>)</span><br><span class="line">        x = self.fc_layers(x)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">model = NetSeq().cuda()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure><h1 id="Credits"><a href="#Credits" class="headerlink" title="Credits"></a>Credits</h1><p><a href="https://medium.com/deeplearningbrasilia/deep-learning-introduction-to-pytorch-5bd39421c84" target="_blank" rel="noopener">https://medium.com/deeplearningbrasilia/deep-learning-introduction-to-pytorch-5bd39421c84</a></p><p><a href="https://www.kaggle.com/azure0102/pytorch-tutorial-for-deep-learning-lovers" target="_blank" rel="noopener">https://www.kaggle.com/azure0102/pytorch-tutorial-for-deep-learning-lovers</a></p><p>(Working in progess)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PyTorch&lt;/p&gt;
&lt;p&gt;Tutorial&lt;/p&gt;
    
    </summary>
    
      <category term="Note" scheme="http://yoursite.com/categories/Note/"/>
    
    
      <category term="Programming" scheme="http://yoursite.com/tags/Programming/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="PyTorch" scheme="http://yoursite.com/tags/PyTorch/"/>
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>CS231n</title>
    <link href="http://yoursite.com/2019/05/10/CS231n/"/>
    <id>http://yoursite.com/2019/05/10/CS231n/</id>
    <published>2019-05-10T20:53:34.000Z</published>
    <updated>2019-10-14T23:30:09.280Z</updated>
    
    <content type="html"><![CDATA[<p>Stanford University</p><p>CS231n: Convolutional Neural Networks for Visual Recognition</p><p>Course Notes</p><a id="more"></a><h1 id="Lecture-1-Introduction-to-Convolutional-Neural-Networks-for-Visual-Recognition"><a href="#Lecture-1-Introduction-to-Convolutional-Neural-Networks-for-Visual-Recognition" class="headerlink" title="Lecture 1 | Introduction to Convolutional Neural Networks for Visual Recognition"></a>Lecture 1 | Introduction to Convolutional Neural Networks for Visual Recognition</h1><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-11-Lecture1-1.png" style="zoom:60%"><br></center><p><strong>Camera Obscure</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-11-Lecture1-2.png" style="zoom:60%"><br></center><p><strong>Object Detection Benchmark</strong></p><p>PASCAL Visual Object Challenge</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-11-Lecture1-3.png" style="zoom:60%"><br></center><p><strong>Image Classification Benchmark</strong></p><p>ImageNet</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-11-Lecture1-4.png" style="zoom:60%"><br></center><ul><li><p>Image classification</p></li><li><p>Object detection</p></li><li><p>Action classification</p></li><li><p>Image captioning</p></li></ul><p>Convolutional Neural Networks (CNN) have become an important tool for object recognition</p><p>Computer Vision Technology Can Better Our Lives</p><h1 id="Lecture-2-Image-Classification"><a href="#Lecture-2-Image-Classification" class="headerlink" title="Lecture 2 | Image Classification"></a>Lecture 2 | Image Classification</h1><p><strong>Image Classification</strong></p><p>A core task in Computer Vision</p><p>The Problem: Semantic Gap</p><p>Challenges: Viewpoint variation, Illumination, Deformation, Occlusion, Background Clutter, Intraclass variation</p><p><strong>Data-Driven Approach</strong></p><ol><li>Collect a dataset of images and labels</li><li>Use Machine Learning to train a classifier</li><li>Evaluate the classifier on new images</li></ol><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-11-Lecture2-1.png" style="zoom:60%"><br></center><p>First classifier: <strong>Nearest Neighbor</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-11-Lecture2-2.png" style="zoom:60%"><br></center><p>Example Dataset: <strong>CIFAR10</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-11-Lecture2-3.png" style="zoom:60%"><br></center><p><strong>Distance Metric</strong> to compare images</p><p>L1 distance:<br>$$<br>d_1(I_1,I_2)=\sum_p|I_1^p-I_2^p|<br>$$</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-11-Lecture2-4.png" style="zoom:60%"><br></center><p>Train O(1), predict O(N).</p><p>This is bad: we want classifiers that are <strong>fast</strong> at prediction; <strong>slow</strong> for training is ok</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-11-Lecture2-6.png" style="zoom:60%"><br></center><p><strong>K-Nearest Neighbors</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-11-Lecture2-5.png" style="zoom:60%"><br></center><p>k and distance matrix are hyperparameters: choice about the algorithm that we set rather than learn</p><p><strong>Setting Hyperparameters</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-11-Lecture2-7.png" style="zoom:60%"><br></center><p>Cross Validation is commonly used in small dataset, not commonly used in deep learning</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-11-Lecture2-8.png" style="zoom:60%"><br></center><p>k-Nearest Neighbor on images <strong>never used</strong>.</p><ul><li>Very slow at test time</li><li>Distance metrics on pixels are not informative</li></ul><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-11-Lecture2-9.png" style="zoom:60%"><br></center><ul><li>Curse of dimensionality</li></ul><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-11-Lecture2-10.png" style="zoom:60%"><br></center><p><strong>K-Nearest Neighbors: Summary</strong></p><p>In <strong>Image classification</strong> we start with a <strong>training set</strong> of images and labels, and must predict labels on the <strong>test set</strong></p><p>The <strong>K-Nearest Neighbors</strong> classifier predicts labels based on nearest training examples</p><p>Distance metric and K are <strong>hyperparameters</strong></p><p>Choose hyperparameters using the <strong>validation set</strong>; only run on the test set once at the very end!</p><p><strong>Linear Classification</strong> (Parametric Approach)</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-11-Lecture2-12.png" style="zoom:60%"><br></center><p>Sometimes we add a biased term $b$</p><p><strong>Interpreting a Linear Classifier</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-11-Lecture2-13.png" style="zoom:60%"><br></center><p><strong>Hard cases for a linear classifier</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-11-Lecture2-14.png" style="zoom:60%"><br></center><h1 id="Lecture-3-Loss-Functions-and-Optimization"><a href="#Lecture-3-Loss-Functions-and-Optimization" class="headerlink" title="Lecture 3 | Loss Functions and Optimization"></a>Lecture 3 | Loss Functions and Optimization</h1><ol><li>Define a <strong>loss function</strong> that quantifies our unhappiness with the score across the training data.</li><li>Come up with a way of efficiently finding the parameters that minimize the loss function (<strong>optimization</strong>)</li></ol><p>Loss over the dataset is a sum of loss over examples:<br>$$<br>L = \frac1N\sum_iL_i(f(x_i,W),y_i)<br>$$</p><p><strong>Multiclass SVM loss:</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-12-Lesson3-1.png" style="zoom:60%"><br></center><p>Numpy implementatin</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_i_vectorized</span><span class="params">(x, y, W)</span>:</span></span><br><span class="line">    scores = W.dot(x)</span><br><span class="line">    margins = np.maximum(<span class="number">0</span>, scores-scores[y]+<span class="number">1</span>)</span><br><span class="line">    margins[y] = <span class="number">0</span></span><br><span class="line">    loss_i = np.sum(margins)</span><br><span class="line">    <span class="keyword">return</span> loss_i</span><br></pre></td></tr></table></figure><p><strong>Data loss</strong>: Model predictions should match training data</p><p><strong>regularization loss</strong>: Model should be “simple”, so it works on test data</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-12-Lesson3-2.png" style="zoom:60%"><br></center><p><strong>Regularization</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-12-Lesson3-3.png" style="zoom:60%"><br></center><p><strong>Softmax Classifier</strong> (Multinomial Logistic Regression)</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-12-Lesson3-4.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-12-Lesson3-5.png" style="zoom:60%"><br></center><p><strong>Recap</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-12-Lesson3-6.png" style="zoom:60%"><br></center><p><strong>Optimization</strong></p><p>Follow the slope</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-12-Lesson3-7.png" style="zoom:60%"><br></center><ul><li>Numerical gradient: approximate, slow, easy to write</li><li>Analytic gradient: exact, fast, error-prone</li></ul><p>In practice: Always use analytic gradient, but check implementation with numerical gradient. This is called a <strong>gradient check.</strong></p><p><strong>Gradient Descent</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">    weights_grad = evaluate_gradient(loss_fun, data, weights)</span><br><span class="line">    weights += - step_size * weights_grad</span><br></pre></td></tr></table></figure><p><strong>Stochastic Gradient Descent (SGD)</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-12-Lesson3-8.png" style="zoom:60%"><br></center><p><strong>Histogram of Oriented Gradients (HoG)</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-12-Lesson3-9.png" style="zoom:60%"><br></center><p><strong>Bag of Words</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-12-Lesson3-10.png" style="zoom:60%"><br></center><h1 id="Assignment-1"><a href="#Assignment-1" class="headerlink" title="Assignment 1"></a>Assignment 1</h1><ul><li>understand the basic <strong>Image Classification pipeline</strong> and the data-driven approach (train/predict stages)</li><li>Understand the train/val/test <strong>splits</strong> and the use of validation data for <strong>hyperparameter tuning</strong></li><li>Develop proficiency in writing efficient <strong>vectorized</strong> code with numpy</li><li>Implement and apply k-Nearest Neighbor (<strong>kNN</strong>) classifier</li><li>Implement and apply a Multiclass Support Vector Machine (<strong>SVM</strong>) classifier</li><li>Implement and apply <strong>Softmax</strong> classifier</li><li>Implement and apply a <strong>Two layer neural network</strong> classifier</li><li>Understand the differences and tradeoffs between these classifiers</li><li>get a basic understanding of performance improvements from using <strong>higher-level representations</strong> than raw pixels (e.g. color histograms, Histogram of Gradient (HOG) features)</li></ul><h1 id="Lecture-4-Introduction-to-Neural-Networks"><a href="#Lecture-4-Introduction-to-Neural-Networks" class="headerlink" title="Lecture 4 | Introduction to Neural Networks"></a>Lecture 4 | Introduction to Neural Networks</h1><p><strong>computational graphs</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-13-Lecture4-1.png" style="zoom:60%"><br></center><p><strong>Backpropagation</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-13-Lecture4-2.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-13-Lecture4-3.png" style="zoom:60%"><br></center><p><strong>sigmoid function</strong><br>$$<br>\sigma(x) = \frac{1}{1+e^{-x}}<br>$$</p><p>$$<br>\frac{d\sigma(x)}{dx}=(1-\sigma(x))\sigma(x)<br>$$</p><p>We can group any node we want as long we can write down their local gradient</p><p><strong>add</strong> gate: gradient distributor</p><p><strong>max</strong> gate: gradient router</p><p><strong>mul</strong> gate: gradient switcher</p><p><strong>Gradients for vectorized code: Jacobian matrix</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-13-Lecture4-4.png" style="zoom:60%"><br></center><p>Always check: The gradient with respect to a variable should have the same shape as the variable</p><p><strong>Modularized implementation</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-13-Lecture4-5.png" style="zoom:60%"><br></center><p><strong>Neural networks</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-13-Lecture4-6.png" style="zoom:60%"><br></center><p><strong>Activation functions</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-13-Lecture4-7.png" style="zoom:60%"><br></center><p><strong>Neural networks Architectures</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-13-Lecture4-8.png" style="zoom:60%"><br></center><h1 id="Lecture-5-Convolutional-Neural-Networks"><a href="#Lecture-5-Convolutional-Neural-Networks" class="headerlink" title="Lecture 5 | Convolutional Neural Networks"></a>Lecture 5 | Convolutional Neural Networks</h1><p>Mark I Perceptron:</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture5-1.png" style="zoom:60%"><br></center><p><strong>Convolution Layer</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture5-2.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture5-3.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture5-4.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture5-5.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture5-6.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture5-7.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture5-8.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture5-9.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture5-10.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture5-11.png" style="zoom:60%"><br></center><h1 id="Lecture-6-Training-Neural-Networks-I"><a href="#Lecture-6-Training-Neural-Networks-I" class="headerlink" title="Lecture 6 | Training Neural Networks I"></a>Lecture 6 | Training Neural Networks I</h1><p><strong>Mini-batch SGD</strong></p><ol><li><strong>Sample</strong> a batch of data</li><li><strong>Forward</strong> prop it through the graph (network), get loss</li><li><strong>Backprop</strong> to calculate the gradients</li><li><strong>Update</strong> the parameters using the gradient</li></ol><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture6-1.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture6-2.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture6-3.png" style="zoom:60%"><br></center><p>People like to initialzie ReLU neurons with slightly positive biases (e.g. 0.01)</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture6-4.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture6-5.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture6-6.png" style="zoom:60%"><br></center><p><strong>In practice:</strong></p><ul><li>Use ReLU. Be careful with your learning rates</li><li>Try out Leaky ReLU / Maxout / ELU</li><li>Try out tanh but don’t expect much</li><li>Don’t use sigmoid</li></ul><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture6-7.png" style="zoom:60%"><br></center><p><strong>Reasonable initialization</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture6-8.png" style="zoom:60%"><br></center><p><strong>Batch Normalization</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture6-9.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture6-10.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture6-11.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture6-12.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture6-13.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture6-14.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture6-15.png" style="zoom:60%"><br></center><h1 id="Lecture-7-Training-Neural-Networks-II"><a href="#Lecture-7-Training-Neural-Networks-II" class="headerlink" title="Lecture 7 | Training Neural Networks II"></a>Lecture 7 | Training Neural Networks II</h1><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture7-1.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture7-2.png" style="zoom:60%"><br></center><ul><li>Fancier optimization</li><li>Regularization</li><li>Transfer Learning</li></ul><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture7-3.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture7-4.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture7-5.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture7-6.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture7-7.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture7-8.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture7-9.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture7-12.png" style="zoom:60%"><br></center><p>Learning rate decay is common with SGD momentum but less common with Adam</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture7-13.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture7-14.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture7-15.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture7-16.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture7-17.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture7-18.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture7-19.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture7-20.png" style="zoom:60%"><br></center><p><strong>Data Augmentation</strong></p><ul><li>translation</li><li>rotation</li><li>stretching</li><li>shearing</li><li>lens distorions</li></ul><p><strong>Transfer Learning</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture7-21.png" style="zoom:60%"><br></center><h1 id="Lecture-8-Deep-Learning-Software"><a href="#Lecture-8-Deep-Learning-Software" class="headerlink" title="Lecture 8 | Deep Learning Software"></a>Lecture 8 | Deep Learning Software</h1><p><strong>CPU vs GPU</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture8-1.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture8-2.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture8-3.png" style="zoom:60%"><br></center><p><strong>The point of deep learning frameworks</strong></p><ul><li>Easily build big computational graphs</li><li>Easily compute gradients in computational graphs</li><li><p>Run it all efficiently on GPU (wrap cuDNN, cuBLAS, etc)</p><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture8-4.png" style="zoom:60%"><br></center><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture8-5.png" style="zoom:60%"><br></center><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture8-6.png" style="zoom:60%"><br></center><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture8-7.png" style="zoom:60%"><br></center><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture8-8.png" style="zoom:60%"><br></center><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture8-9.png" style="zoom:60%"><br></center><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture8-10.png" style="zoom:60%"><br></center></li></ul><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture8-11.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture8-12.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture8-13.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture8-14.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture8-15.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture8-16.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture8-17.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture8-18.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture8-19.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture8-20.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture8-21.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-14-Lecture8-22.png" style="zoom:60%"><br></center><h1 id="Lecture-9-CNN-Architectures"><a href="#Lecture-9-CNN-Architectures" class="headerlink" title="Lecture 9 | CNN Architectures"></a>Lecture 9 | CNN Architectures</h1><h1 id="Lecture-10-Recurrent-Neural-Networks"><a href="#Lecture-10-Recurrent-Neural-Networks" class="headerlink" title="Lecture 10 | Recurrent Neural Networks"></a>Lecture 10 | Recurrent Neural Networks</h1><h1 id="Lecture-11-Detection-and-Segmentation"><a href="#Lecture-11-Detection-and-Segmentation" class="headerlink" title="Lecture 11 | Detection and Segmentation"></a>Lecture 11 | Detection and Segmentation</h1><h1 id="Lecture-12-Visualizing-and-Understanding"><a href="#Lecture-12-Visualizing-and-Understanding" class="headerlink" title="Lecture 12 | Visualizing and Understanding"></a>Lecture 12 | Visualizing and Understanding</h1><h1 id="Lecture-13-Generative-Models"><a href="#Lecture-13-Generative-Models" class="headerlink" title="Lecture 13 | Generative Models"></a>Lecture 13 | Generative Models</h1><h1 id="Lecture-14-Deep-Reinforcement-Learning"><a href="#Lecture-14-Deep-Reinforcement-Learning" class="headerlink" title="Lecture 14 | Deep Reinforcement Learning"></a>Lecture 14 | Deep Reinforcement Learning</h1><h1 id="Lecture-15-Efficient-Methods-and-Hardware-for-Deep-Learning"><a href="#Lecture-15-Efficient-Methods-and-Hardware-for-Deep-Learning" class="headerlink" title="Lecture 15 | Efficient Methods and Hardware for Deep Learning"></a>Lecture 15 | Efficient Methods and Hardware for Deep Learning</h1><h1 id="Lecture-16-Adversarial-Examples-and-Adversarial-Training"><a href="#Lecture-16-Adversarial-Examples-and-Adversarial-Training" class="headerlink" title="Lecture 16 | Adversarial Examples and Adversarial Training"></a>Lecture 16 | Adversarial Examples and Adversarial Training</h1><p>(Working in progress)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Stanford University&lt;/p&gt;
&lt;p&gt;CS231n: Convolutional Neural Networks for Visual Recognition&lt;/p&gt;
&lt;p&gt;Course Notes&lt;/p&gt;
    
    </summary>
    
      <category term="Note" scheme="http://yoursite.com/categories/Note/"/>
    
    
      <category term="Programming" scheme="http://yoursite.com/tags/Programming/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://yoursite.com/tags/TensorFlow/"/>
    
      <category term="Computer Vision" scheme="http://yoursite.com/tags/Computer-Vision/"/>
    
  </entry>
  
  <entry>
    <title>Practical Deep Learning for Coders</title>
    <link href="http://yoursite.com/2019/05/06/Practical%20Deep%20Learning%20for%20Coders/"/>
    <id>http://yoursite.com/2019/05/06/Practical Deep Learning for Coders/</id>
    <published>2019-05-06T05:17:34.000Z</published>
    <updated>2019-10-14T23:30:51.197Z</updated>
    
    <content type="html"><![CDATA[<p>Fast.ai</p><p>Course Notes</p><a id="more"></a><h1 id="Lesson-1"><a href="#Lesson-1" class="headerlink" title="Lesson 1"></a>Lesson 1</h1><p>Jupyter Notebook: Interactive environment for Python</p><p>Import necessary libraries:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastai <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> fastai.vision <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplob <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><p>Tips: Don’t try to stop and understand the first time</p><p>Making Deep Learning Accessible</p><ul><li><strong>Software</strong>: To make these available to use quickly, reliably, and with minimal code</li><li><strong>Education</strong>: So that as many people as possible can use these</li><li><strong>Research</strong>: Ways to make state of the art deep learning techniques more accessible</li><li><strong>Community</strong>: So that we can all help each other</li></ul><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-08-Lesson1-1.png" style="zoom:60%"><br></center><p>We can do a lot more quickly with PyTorch than with TensorFlow</p><p>Resources:</p><ul><li><a href="https://docs.fast.ai/" target="_blank" rel="noopener">fast.ai docs</a></li><li><a href="https://pytorch.org/" target="_blank" rel="noopener">PyTorch</a></li></ul><p>Fastai provides four application areas:</p><ul><li>Computer Vision</li><li>Natural Language Text</li><li>Tabular Data</li><li>Collaborative Filtering</li></ul><p>Fine-grained classification: distinguish between similar categories</p><p><strong>Some key functions</strong></p><p>Download and untar data: <code>path = untar_data(URLs.PETS); path</code></p><p>List the directory: <code>path.ls()</code></p><p>Path objects (of Python3): <code>path_anno = path/&#39;annotations&#39;</code>, <code>path_img = path/&#39;images&#39;</code></p><p>Grab an array of all image files: <code>fname = get_image_files(path_img)</code></p><p>Create a data bunch and normalize it:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pat = <span class="string">r'/([^/]+\d+.jpg$)'</span></span><br><span class="line">data = ImageDataBunch.from_name_re(path_img, fnames, pat, ds_tfms=get_transforms(), size=<span class="number">224</span>, bs=<span class="number">64</span>)<span class="comment"># bs represents the batch size</span></span><br><span class="line">data.normalize(imagenet_stats)</span><br></pre></td></tr></table></figure><p>224x224 for a image generally works</p><p>A data bunch contains typically 2 or 3 datasets</p><p>Show the data bunch: <code>data.show_batch(row=3, figsize=(7,6))</code></p><p>Labels: <code>data.classes</code>, <code>data.c</code> attributes</p><p>Training: resnet34 <code>learn = ConvLearner(data, models.resnet34, metrics=error_rate)</code></p><p>ResNet34 and ResNet50 are always good for choosing</p><p>Fit: <code>learn.fit_one_cycle(4)</code> the best way at the present for training</p><p> Save the trained model: <code>learn.save(&#39;stage-1&#39;)</code></p><p>What comes out: <code>interp = ClassificationInterpretation.from_learner(learn)</code></p><p>Plot top losses: <code>interp.plot_top_losses(9, figsize=(15,11))</code> (for error analysis)</p><p>See the documentation: <code>doc(interp.plot_top_losses)</code></p><p>Plot the confusion matrix: <code>interp.plot_confusion_matrix(figsize=(12,12), dpi=60)</code></p><p>The most confused examples: <code>interp.most_confused(min_val=2)</code></p><p>Unfreezing: <code>learn.unfreeze()</code> (train the whole model to get high accuracy)</p><p>Learning_rate find: <code>learn.lr_find()</code></p><p>Plot the learning_rate finder: <code>learn.recorder.plot()</code></p><p>Learning_rate slice: <code>learn.fit_one_cycle(2, max_lr=slice(1e-6,1e-4))</code>, distribute the learning_rate equally accross layers</p><p> ImageDataBunch from folder: <code>data = ImageDataBunch.from_folder(path, ds_tfms=tfms, size=26)</code></p><p>ImageDataBunch from csv: <code>df = pd.read_csv(path/&#39;labels.csv&#39;)</code> <code>data = ImageDataBunch.from_csv(path, ds_tfms=tfms, size=28)</code></p><p>Dataset: <a href="http://www.robots.ox.ac.uk/~vgg/data/pets/" target="_blank" rel="noopener">Oxford-IIIT Pet Dataset</a></p><h1 id="Lesson-2"><a href="#Lesson-2" class="headerlink" title="Lesson 2"></a>Lesson 2</h1><p><strong>Create an image dataset through Google Images</strong></p><ul><li><p>Starting point: Find some example pictures</p></li><li><p>Save the urls <code>folder = &#39;black&#39;</code> , <code>file = urls_black.txt</code>, <code>path = Path(&#39;data/bears&#39;)</code></p></li><li><p>Download images <code>download_iamges(path/file, dest, max_pics=200)</code></p></li><li>Verify the images <code>verify_images(path/c, delete=True, max_workers=8)</code></li></ul><p>Set random state: <code>np.random.seed(42)</code> (Make sure the solution is stable)</p><p>Create a databunch: <code>data = ImageDataBunch.from_folder(path, train=&#39;.&#39;, valid_pct=0.2, ds_tfms=get_transforms(), size=224, num_workers=4).normalize(imagenet_stats)</code></p><p>See the size of training set: <code>len(data.train_ds)</code></p><p>See the size of validation set: <code>len(data.valid(ds))</code></p><p>Picking the learning rate: picking the most steep part of the learning rate find curve</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-08-Lesson2-1.png" style="zoom:60%"><br></center><p><strong>Cleaning Up the data</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastai.widgets <span class="keyword">import</span> *</span><br><span class="line">losses, idxs = interp.top_losses()</span><br><span class="line">top_loss_paths = data.valid_ds.x[idxs]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fd = FileDeleter(file_paths=top_loss_paths)</span><br></pre></td></tr></table></figure><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-08-Lesson2-2.png" style="zoom:60%"><br></center><p><a href="https://ipywidgets.readthedocs.io/en/stable/" target="_blank" rel="noopener">ipywidgets</a></p><p><strong>Putting your model in production</strong></p><p>Use a CPU rather than GPU to do inference</p><p>Set CPU as defalut: <code>fastai.defaults.device = torch.device(&#39;cpu&#39;)</code></p><p>Create a single image data bunch: <code>data = ImageDataBunch.single_from_classes(path, classes, tfms=get_transforms(), size=224).normalize(imagenet)</code></p><p>Load the model: <code>learn = create_cnn(data, models.resnet34)</code> <code>learn.load(&#39;stage-2&#39;)</code></p><p>Make prediction: <code>pred_class, pred_idx, outputs = learn.predict(img)</code></p><p>Deployment: <a href="https://www.starlette.io/" target="_blank" rel="noopener">Starlette</a> (Web app)</p><p><strong>Problems</strong></p><p>Most likely are:</p><ul><li>Learning rate</li><li>Number of epochs</li></ul><p>Learning rate too high:</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-08-Lesson2-3.png" style="zoom:60%"><br></center><p>Learning rate too low:</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-08-Lesson2-4.png" style="zoom:60%"><br></center><p>If this kind of things happen, we have to redo again from the scratch</p><p>Another problem is that the training loss is higher than the validation loss, that always means that you haven’t fit enough (learning rate is too low or number of epochs is too low)</p><p>Any model that is trained correctly will have a lower training loss than the validation loss</p><p><strong>x@a</strong>: Matrix product</p><p>Tensor: Array with regular shape</p><p>Rank: How many dimensions are in the tensor</p><p>In PyTorch, any function with an underscore: don’t return to me but replace in place</p><p>type check: <code>a.type()</code></p><p>Gradient descent:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = nn.Parameters(a)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">()</span>:</span></span><br><span class="line">    y_hat = x@a</span><br><span class="line">    loss = mse(y, y_hat)</span><br><span class="line">    <span class="keyword">if</span> t % <span class="number">10</span> == <span class="number">0</span>: print(loss)</span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        a.sub_(lr * a.grad)</span><br><span class="line">        a.grad.zero_()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">1e-1</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>): update()</span><br></pre></td></tr></table></figure><h1 id="Lesson-3"><a href="#Lesson-3" class="headerlink" title="Lesson 3"></a>Lesson 3</h1><p><a href="https://www.kaggle.com/c/planet-understanding-the-amazon-from-space" target="_blank" rel="noopener">Planet Dataset</a></p><p>Multiple labels classification</p><p><strong>Download Data from Kaggle</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install kaggle --upgrade</span><br><span class="line">mkdir -p ~/.kaggle/</span><br><span class="line">mv kaggle.json ~/.kaggle</span><br></pre></td></tr></table></figure><p><a href="https://colab.research.google.com/drive/10vM1Sr6nn-1hv0kMrJsTLpmnl-DsWK81" target="_blank" rel="noopener">Instructions in detail</a></p><p><strong>Multiclassification</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-10-Week3-1.png" style="zoom:60%"><br></center><p><a href="https://docs.fast.ai/data_block.html" target="_blank" rel="noopener">data block API</a></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-10-Week3-2.png" style="zoom:60%"><br></center><p><code>Dataset</code> class for PyTorch (methods to manipulate data from the source):</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-10-Week3-3.png" style="zoom:60%"><br></center><p><code>DataLoader</code> class for PyTorch (load data from dataset):</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-10-Week3-4.png" style="zoom:60%"><br></center><p><code>DataBunch</code> class for fastai (split train and validation data):</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-10-Week3-5.png" style="zoom:60%"><br></center><p><strong>Transforms</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tfms = get_transforms()</span><br></pre></td></tr></table></figure><p><strong>Image Segmentation with CamVid</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">path = untar_data(URLs.CAMVID)</span><br><span class="line">fname = get_image_files(path_img)</span><br></pre></td></tr></table></figure><h1 id="Lesson-4"><a href="#Lesson-4" class="headerlink" title="Lesson 4"></a>Lesson 4</h1><h1 id="Lesson-5"><a href="#Lesson-5" class="headerlink" title="Lesson 5"></a>Lesson 5</h1><h1 id="Lesson-6"><a href="#Lesson-6" class="headerlink" title="Lesson 6"></a>Lesson 6</h1><h1 id="Lesson-7"><a href="#Lesson-7" class="headerlink" title="Lesson 7"></a>Lesson 7</h1><p>(Working in progess)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Fast.ai&lt;/p&gt;
&lt;p&gt;Course Notes&lt;/p&gt;
    
    </summary>
    
      <category term="Note" scheme="http://yoursite.com/categories/Note/"/>
    
    
      <category term="Programming" scheme="http://yoursite.com/tags/Programming/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="PyTorch" scheme="http://yoursite.com/tags/PyTorch/"/>
    
      <category term="Fast.ai" scheme="http://yoursite.com/tags/Fast-ai/"/>
    
  </entry>
  
  <entry>
    <title>Algorithms, Part 2</title>
    <link href="http://yoursite.com/2019/05/05/Algorithms-Part-2/"/>
    <id>http://yoursite.com/2019/05/05/Algorithms-Part-2/</id>
    <published>2019-05-06T04:42:34.000Z</published>
    <updated>2019-10-14T23:29:31.318Z</updated>
    
    <content type="html"><![CDATA[<p>Princeton Algorithm course on coursera. Part 2</p><a id="more"></a><ul><li>Intermediate-level survey course</li><li>Programming and problem solving, with applications</li></ul><p>Learning Purpose: </p><ul><li>Being specialized in Java. </li><li>Having fun with algorithms.</li></ul><p>Reference Books:</p><ul><li><a href="https://algs4.cs.princeton.edu/home/" target="_blank" rel="noopener">Algorithms, 4th Edition</a></li><li><a href="https://introcs.cs.princeton.edu/java/home/" target="_blank" rel="noopener">Computer Science:   An Interdisciplinary Approach</a></li></ul><h1 id="Week-1"><a href="#Week-1" class="headerlink" title="Week 1"></a>Week 1</h1><h2 id="Undirected-Graphs"><a href="#Undirected-Graphs" class="headerlink" title="Undirected Graphs"></a>Undirected Graphs</h2><h2 id="Directed-Graphs"><a href="#Directed-Graphs" class="headerlink" title="Directed Graphs"></a>Directed Graphs</h2><h2 id="Programming-Assignment-WordNet"><a href="#Programming-Assignment-WordNet" class="headerlink" title="Programming Assignment: WordNet"></a>Programming Assignment: WordNet</h2><p><a href>WordNet</a></p><h1 id="Week-2"><a href="#Week-2" class="headerlink" title="Week 2"></a>Week 2</h1><h2 id="Minimum-Spanning-Trees"><a href="#Minimum-Spanning-Trees" class="headerlink" title="Minimum Spanning Trees"></a>Minimum Spanning Trees</h2><h2 id="Shortest-Paths"><a href="#Shortest-Paths" class="headerlink" title="Shortest Paths"></a>Shortest Paths</h2><h2 id="Programming-Assignment-Seam-Carving"><a href="#Programming-Assignment-Seam-Carving" class="headerlink" title="Programming Assignment: Seam Carving"></a>Programming Assignment: Seam Carving</h2><p><a href>Seam Carving</a></p><h1 id="Week-3"><a href="#Week-3" class="headerlink" title="Week 3"></a>Week 3</h1><h2 id="Maximum-Flow-and-Minimum-Cut"><a href="#Maximum-Flow-and-Minimum-Cut" class="headerlink" title="Maximum Flow and Minimum Cut"></a>Maximum Flow and Minimum Cut</h2><h2 id="Programming-Assignment-Baseball-Elimination"><a href="#Programming-Assignment-Baseball-Elimination" class="headerlink" title="Programming Assignment: Baseball Elimination"></a>Programming Assignment: Baseball Elimination</h2><p><a href>Baseball Elimination</a></p><h2 id="Radix-Sorts"><a href="#Radix-Sorts" class="headerlink" title="Radix Sorts"></a>Radix Sorts</h2><h1 id="Week-4"><a href="#Week-4" class="headerlink" title="Week 4"></a>Week 4</h1><h2 id="Tries"><a href="#Tries" class="headerlink" title="Tries"></a>Tries</h2><h2 id="Substring-Search"><a href="#Substring-Search" class="headerlink" title="Substring Search"></a>Substring Search</h2><h2 id="Programming-Assignment-Boggle"><a href="#Programming-Assignment-Boggle" class="headerlink" title="Programming Assignment: Boggle"></a>Programming Assignment: Boggle</h2><p><a href>Boggle</a></p><h1 id="Week-5"><a href="#Week-5" class="headerlink" title="Week 5"></a>Week 5</h1><h2 id="Regular-Expressions"><a href="#Regular-Expressions" class="headerlink" title="Regular Expressions"></a>Regular Expressions</h2><h2 id="Data-Compression"><a href="#Data-Compression" class="headerlink" title="Data Compression"></a>Data Compression</h2><h2 id="Programming-Assignment-Burrows-Wheeler"><a href="#Programming-Assignment-Burrows-Wheeler" class="headerlink" title="Programming Assignment: Burrows-Wheeler"></a>Programming Assignment: Burrows-Wheeler</h2><p><a href>Burrows-Wheeler</a></p><h1 id="Week-6"><a href="#Week-6" class="headerlink" title="Week 6"></a>Week 6</h1><h2 id="Reductions"><a href="#Reductions" class="headerlink" title="Reductions"></a>Reductions</h2><h2 id="Linear-Programming"><a href="#Linear-Programming" class="headerlink" title="Linear Programming"></a>Linear Programming</h2><h2 id="Intractability"><a href="#Intractability" class="headerlink" title="Intractability"></a>Intractability</h2><p>(Working in progress)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Princeton Algorithm course on coursera. Part 2&lt;/p&gt;
    
    </summary>
    
      <category term="Note" scheme="http://yoursite.com/categories/Note/"/>
    
    
      <category term="Programming" scheme="http://yoursite.com/tags/Programming/"/>
    
      <category term="Java" scheme="http://yoursite.com/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>Sequence Models</title>
    <link href="http://yoursite.com/2019/05/04/Sequence%20Models/"/>
    <id>http://yoursite.com/2019/05/04/Sequence Models/</id>
    <published>2019-05-05T01:51:34.000Z</published>
    <updated>2019-10-14T23:31:30.547Z</updated>
    
    <content type="html"><![CDATA[<p>Deeplearning.ai Specialization</p><p>Course Notes</p><a id="more"></a><h1 id="Week-1"><a href="#Week-1" class="headerlink" title="Week 1"></a>Week 1</h1><h2 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a>Recurrent Neural Networks</h2><p>Speech recognition</p><p>Music generation</p><p>Sentiment classification</p><p>DNA sequence analysis</p><p>Machine translation</p><p>Video activity recognition</p><p>Name entity recognition</p><p><strong>Representing words</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-07-Week1-1.png" style="zoom:60%"><br></center><p>Problem of a standard network:</p><ul><li>Inputs, outputs can be different lengths in different examples.</li><li>Doesn’t share features learned across different positions of text.</li></ul><p><strong>Recurrent Neural Networks</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-07-Week1-2.png" style="zoom:60%"><br></center><p>Bidirectional RNN (BRNN)</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-07-Week1-3.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-07-Week1-4.png" style="zoom:60%"><br></center><p><strong>RNN architectures</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-07-Week1-5.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-07-Week1-6.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-07-Week1-7.png" style="zoom:60%"><br></center><p><strong>Language model</strong></p><p>Given a sentence, tell you the probability of that setence.</p><p>Training set: large corpus of English text</p><p>Tolenize: form a vocabulary and map each individual word into this vocabulary. The unknown is replaced with a unique token \&lt;UNK></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-07-Week1-8.png" style="zoom:60%"><br></center><p><strong>Sampling sequence from a trained RNN</strong></p><h2 id="Programming-Assignment-Building-a-recurrent-neural-network-step-by-step"><a href="#Programming-Assignment-Building-a-recurrent-neural-network-step-by-step" class="headerlink" title="Programming Assignment: Building a recurrent neural network - step by step"></a>Programming Assignment: Building a recurrent neural network - step by step</h2><p><a href>Building a recurrent neural network - step by step</a></p><h2 id="Programming-Assignment-Dinosaur-Island-Character-Level-Language-Modeling"><a href="#Programming-Assignment-Dinosaur-Island-Character-Level-Language-Modeling" class="headerlink" title="Programming Assignment: Dinosaur Island - Character-Level Language Modeling"></a>Programming Assignment: Dinosaur Island - Character-Level Language Modeling</h2><p><a href>Dinosaur Island - Character-Level Language Modeling</a></p><h2 id="Programming-Assignment-Jazz-improvisation-with-LSTM"><a href="#Programming-Assignment-Jazz-improvisation-with-LSTM" class="headerlink" title="Programming Assignment: Jazz improvisation with LSTM"></a>Programming Assignment: Jazz improvisation with LSTM</h2><p><a href>Jazz improvisation with LSTM</a></p><h1 id="Week-2"><a href="#Week-2" class="headerlink" title="Week 2"></a>Week 2</h1><h2 id="Natural-Language-Processing-amp-Word-Embeddings"><a href="#Natural-Language-Processing-amp-Word-Embeddings" class="headerlink" title="Natural Language Processing &amp; Word Embeddings"></a>Natural Language Processing &amp; Word Embeddings</h2><h2 id="Programming-Assignment-Oprations-on-word-vectors-Debiasing"><a href="#Programming-Assignment-Oprations-on-word-vectors-Debiasing" class="headerlink" title="Programming Assignment: Oprations on word vectors - Debiasing"></a>Programming Assignment: Oprations on word vectors - Debiasing</h2><p><a href>Operations on word vectors - Debiasing</a></p><h2 id="Programming-Assignment-Emojify"><a href="#Programming-Assignment-Emojify" class="headerlink" title="Programming Assignment: Emojify"></a>Programming Assignment: Emojify</h2><p><a href>Emojify</a></p><h1 id="Week-3"><a href="#Week-3" class="headerlink" title="Week 3"></a>Week 3</h1><h2 id="Sequence-models-amp-Attention-mechanism"><a href="#Sequence-models-amp-Attention-mechanism" class="headerlink" title="Sequence models &amp; Attention mechanism"></a>Sequence models &amp; Attention mechanism</h2><h2 id="Programming-Assignment-Neural-Machine-Translation-with-Attention"><a href="#Programming-Assignment-Neural-Machine-Translation-with-Attention" class="headerlink" title="Programming Assignment: Neural Machine Translation with Attention"></a>Programming Assignment: Neural Machine Translation with Attention</h2><p><a href>Neural Machine Translation with Attention</a></p><h2 id="Programming-Assignment-Trigger-word-detection"><a href="#Programming-Assignment-Trigger-word-detection" class="headerlink" title="Programming Assignment: Trigger word detection"></a>Programming Assignment: Trigger word detection</h2><p><a href>Trigger word detection</a></p><p>(Working in progess)</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Deeplearning.ai Specialization&lt;/p&gt;
&lt;p&gt;Course Notes&lt;/p&gt;
    
    </summary>
    
      <category term="Note" scheme="http://yoursite.com/categories/Note/"/>
    
    
      <category term="Programming" scheme="http://yoursite.com/tags/Programming/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://yoursite.com/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Convolutional Neural Networks</title>
    <link href="http://yoursite.com/2019/04/25/Convolutional%20Neural%20Networks/"/>
    <id>http://yoursite.com/2019/04/25/Convolutional Neural Networks/</id>
    <published>2019-04-26T04:11:34.000Z</published>
    <updated>2019-05-05T01:42:08.574Z</updated>
    
    <content type="html"><![CDATA[<p>Deeplearning.ai Specialization</p><p>Course Notes</p><a id="more"></a><h1 id="Week-1"><a href="#Week-1" class="headerlink" title="Week 1"></a>Week 1</h1><h2 id="Foundations-of-Convolutional-Neural-Networks"><a href="#Foundations-of-Convolutional-Neural-Networks" class="headerlink" title="Foundations of Convolutional Neural Networks"></a>Foundations of Convolutional Neural Networks</h2><ul><li>Image Classification</li><li>Object detection</li><li>Neural Style Transfer</li></ul><p>With so many parameters, it’s difficult to get enough data for a model to get rid of overfitting.</p><p><strong>Edge Detection</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-03-Week1-1.png" style="zoom:60%"><br></center><p><strong>Convolution</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-03-Week1-2.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-03-Week1-3.png" style="zoom:60%"><br></center><p><strong>Padding</strong></p><p>Two problems:</p><ul><li>The image may shrink after some convolution</li><li>The blocks in the middle of image have a higher chance to be utilized than the ones on the border</li></ul><p>For a deep neural network, we really don’t want the image to shrink because we have many convolution operations. Without padding, the image may shrink after every layer and after maybe 100 layers, we end up with a very small matrix like 1 by 1. The other problem is that we throw away information from the edges of the image.</p><p>Traditionally without padding, if the original matrix is $n$ by $n$ and the filter is $f$ by $f$. Then the matrix after convolution will have a shape of $n-f+1$ by $n-f+1$</p><p>p: padding size</p><p>After padding, the final output matrix will have shape $n+2p-f+1$ by $n+2p-f+1$</p><p><strong>Valid and Same convolutions</strong></p><p>“Valid”: no padding, p = 0, nxn * fxf -&gt; n-f+1 x n-f+1</p><p>“Same”: Pad so that output size is the same as the input size. $p=\frac{f-1}{2}$</p><p>By convention, f is usually odd. The reasons:</p><ul><li>padding convience</li><li>Odd dimensional filter has a central position</li></ul><p><strong>Strided convolution</strong></p><p>shape formula:</p><p>nxn * fxf with padding p and stride s</p><p>output shape (square):<br>$$<br>\frac{n+2p-f}{s} + 1<br>$$</p><p><strong>Cross-correlation vs. convolution</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-03-Week1-4.png" style="zoom:60%"><br></center><p>In computer vision, we don’t bother to use the non-flipped one because the flip operation is redundant in real computer vision applications.</p><p><strong>Convolution over volumes</strong></p><p>For example, convolutions on RGB images</p><p>Notation:</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-03-Week1-5.png" style="zoom:60%"><br></center><p>The number of the channels of a image must match the number of channels of the filter.</p><p>Each time, do the 27 multiplications and add up the 27 numbers and get one entry.</p><p><strong>Multiple filters</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-03-Week1-6.png" style="zoom:60%"><br></center><p>Summary: $n\times n\times n_c$ <em> $f\times f \times n_c$ - &gt; $n-f+1$ </em> $n-f+1$ $\times n_c’$</p><p>$n_c’$ is the number of filters we use.</p><p>It seems that the more filters we use, the more features we can extract from an image. (like detecting two features simultaneously, vertical lines and horizontal lines…)</p><p><strong>One layer of convolutional network</strong></p><p>The number of parameters has nothing to do with the input images.</p><p>If layer l is a convolution layer:</p><p>$f^{[l]}$ = filter size</p><p>$p^{[l]}$ = padding</p><p>$s^{[l]}$ = stride</p><p>$n_c^{[l]}$ = number of filters</p><p>Each filter is: $f^{[l]}\times f^{[l]}\times n_c^{[l-1]}$</p><p>Activations: $a^{[l]}-&gt;n_H^{[l]} \times n_W^{[l]} \times n_C^{[l]}$</p><p>Weights: $f^{[l]}\times f^{[l]}\times n_c^{[l-1]}\times n_C^{[l]}$</p><p>Bias: $n_c^{[l]}$ - (1,1,1,$n_c^{[l]}$)</p><p>Input: $n_H^{[l-1]}\times n_W^{[l-1]}\times n_c{[l-1]}$</p><p>Output: $n_H^{[l]}\times n_W^{[l]}\times n_c^{[l]}$<br>$$<br>n^{[l]} = \lfloor \frac{n^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1    \rfloor<br>$$<br>$A^{[l]}$ -&gt; $m\times n_H^{[l]}\times n_W^{[l]} \times n_C^{[l]}$</p><p><strong>Example ConvNet</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-03-Week1-7.png" style="zoom:60%"><br></center><p>Types of layer in a convolutional network:</p><ul><li>Convolution (CONV)</li><li>Pooling (POOL)</li><li>Fully connnected (FC)</li></ul><p><strong>Pooling layers</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-03-Week1-8.png" style="zoom:60%"><br></center><p>Max pooling has no parameters to learn.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-03-Week1-9.png" style="zoom:60%"><br></center><p>Max pooling is used much more in the neural network than average pooling.</p><p>Summary of pooling:</p><p>Hyperparameters:</p><ul><li>f: filter size</li><li><p>s: stride</p></li><li><p>Max or average pooling</p></li></ul><p>Common choice: f=2, s=2. Roughly shrink the height and width by a factor of 2. f=3, s=3 is also used sometimes.</p><p>padding is very very rare used when we do max pooling</p><p>Pooling layer reduces the height and width of the input. It helps reduce computation, as well as helps make feature detectors more invariant to its position in the input.</p><p>No parameters to learn!</p><p><strong>Neural network example</strong></p><p>Common patterns when we go deeper and deeper:</p><ul><li>The height and width will decrease</li><li>The number of channels will increase</li><li>One or more conv layers follow by a pooling layer. At the end follow by FC</li></ul><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-03-Week1-10.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-03-Week1-11.png" style="zoom:60%"><br></center><p>A lot of the parameters tend to be in FC. The activation tends to go down gradually when going deeper and deeper. If it drops too fast, that’s usually not great performance.</p><p><strong>Why convolution?</strong></p><p>Parameter sharing and sparsity of connections.</p><p>Reducing the number of parameters significantly</p><p>Parameter sharing: A feature detector (such as a vertical edge detector) that’s useful in one part of the image is probably useful in another part of the image such as the vertical edge detector.</p><p>Sparsity of connections: In each layer, each output value depends only on a small number of inputs.</p><p>Translation invariance</p><h2 id="Programming-Assignment-Convolutional-Model-step-by-step"><a href="#Programming-Assignment-Convolutional-Model-step-by-step" class="headerlink" title="Programming Assignment: Convolutional Model: step by step"></a>Programming Assignment: Convolutional Model: step by step</h2><p><a href="https://github.com/Aden-Q/Deep-Learning/blob/master/Convolutional Neural Networks/Week 1/Convolution%2Bmodel%2B-%2BStep%2Bby%2BStep%2B-%2Bv2.ipynb" target="_blank" rel="noopener">Convolutional Model: step by step</a></p><h2 id="Programming-Assignment-Convolutional-model-application"><a href="#Programming-Assignment-Convolutional-model-application" class="headerlink" title="Programming Assignment: Convolutional model: application"></a>Programming Assignment: Convolutional model: application</h2><p><a href="https://github.com/Aden-Q/Deep-Learning/blob/master/Convolutional Neural Networks/Week 1/Convolution%2Bmodel%2B-%2BApplication%2B-%2Bv1.ipynb" target="_blank" rel="noopener">Convolutional model: application</a></p><h1 id="Week-2"><a href="#Week-2" class="headerlink" title="Week 2"></a>Week 2</h1><h2 id="Deep-convolutional-models-case-studies"><a href="#Deep-convolutional-models-case-studies" class="headerlink" title="Deep convolutional models: case studies"></a>Deep convolutional models: case studies</h2><p>Read some research papers from the field of computer vision.</p><p>Classic networks:</p><ul><li>LeNet-5</li><li>AlexNet</li><li>VGG</li></ul><p>ResNet (152 layers)</p><p>Inception</p><p><strong>LeNet-5</strong></p><p>Goal: recognize hand written digits</p><p>Trained on grey scale images</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week2-1.png" style="zoom:60%"><br></center><p>Go from left to right, $n_H$ and $n_W$ tend to go down and $n_C$ tend to go up.</p><p><strong>AlexNet</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week2-2.png" style="zoom:60%"><br></center><p><strong>VGG-16</strong></p><p>CONV = 3x3 filters, s = 1, same</p><p>MAX-POOL = 2x2, s = 2</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week2-3.png" style="zoom:60%"><br></center><p><strong>Residual network (ResNet)</strong></p><p>skip connections (“short cut”)</p><p>Residual block:</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week2-4.png" style="zoom:60%"><br></center><p>Using residual block, we can train a much deeper neural networks.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week2-5.png" style="zoom:60%"><br></center><p>In ResNet, there are a lot of “SAME” convolution to preserve dimension so at to carry out skip connection.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week2-6.png" style="zoom:60%"><br></center><p><strong>Network in Network and 1x1 convolutions</strong></p><p>Imagine this like a “fully connected” layer</p><p>[Lin et al., 2013. Network in network]</p><p>This idea influence many other network architectures.</p><p>One useful example:</p><p>Shrinking the number of channels</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week2-7.png" style="zoom:60%"><br></center><p>This is very useful for building Inception neural network.</p><p><strong>Inception network</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week2-8.png" style="zoom:60%"><br></center><p>Do all the possible computation and concat them together.</p><p>The problem: computational cost</p><p>“Bottleneck layer”</p><p>Shrink the representation before increasing the size</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week2-9.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week2-10.png" style="zoom:60%"><br></center><p>So long as you implement this bottleneck layer with reason, you can shrink down the representation significant and don’t hurt the performance of the model.</p><p><strong>Inception module</strong></p><p>To deal with the problem of vanishing gradients: very deep network often have a gradient signal that goes to zero quickly, thus making gradient descent unbearably slow.</p><p>Two types of block:</p><ul><li>Identity block</li><li>Convolutional block</li></ul><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week2-21.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week2-22.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week2-11.png" style="zoom:60%"><br></center><p><strong>Inception network</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week2-12.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week2-13.png" style="zoom:60%"><br></center><p><strong>Transfer learning (a better strategy)</strong></p><p>ImageNet, MS COCO</p><p>When the training set is rather small, transfer learning is a good strategy.</p><p>Small training set (freeze more layers):</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week2-14.png" style="zoom:60%"><br></center><p>Large training set (freeze less layers):</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week2-15.png" style="zoom:60%"><br></center><p>A lot of data (freeze 0 layer):</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week2-16.png" style="zoom:60%"><br></center><p>Transfer is very worth consideration</p><p><strong>Data Augmentation</strong></p><p>Common augmentation method:</p><ul><li>Mirroring (frequently used)</li><li><p>Random Cropping (isn’t a perfect method while frequently used)</p></li><li><p>Rotation</p></li><li>Shearing</li><li><p>Local warping</p></li><li><p>Color shifting</p></li><li>PCA color augmentation</li></ul><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week2-17.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week2-18.png" style="zoom:60%"><br></center><p>Implementation details</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week2-19.png" style="zoom:60%"><br></center><p>Data augmentation also has many hyperparameters.</p><p>Two sources of knowledge in deep learning:</p><ul><li>Labeled data</li><li>Hand engineered features/network architecture/other components</li></ul><p><strong>Tips for doing well on benchmarks/winning competitions</strong></p><ul><li><p>Ensembling: Train several networks independently and average their outputs (3-15 networks typically, so slow, almost never used in a product)</p></li><li><p>Multi-crop at test time: Run classifier on multiple versions of test images and average results</p></li></ul><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week2-20.png" style="zoom:60%"><br></center><ul><li>Use architectures of networks published in the literature</li><li><p>Use open sourcfe implementations if possible</p></li><li><p>Use pretrained models and fine-tune on your dataset</p></li></ul><h2 id="Programming-Assignment-Keras-Tutorial"><a href="#Programming-Assignment-Keras-Tutorial" class="headerlink" title="Programming Assignment: Keras Tutorial"></a>Programming Assignment: Keras Tutorial</h2><p><a href="https://github.com/Aden-Q/Deep-Learning/blob/master/Convolutional Neural Networks/Week 2/Keras%2B-%2BTutorial%2B-%2BHappy%2BHouse%2Bv2.ipynb" target="_blank" rel="noopener">Keras Tutorial</a></p><h2 id="Programming-Assignment-Residual-Networks"><a href="#Programming-Assignment-Residual-Networks" class="headerlink" title="Programming Assignment: Residual Networks"></a>Programming Assignment: Residual Networks</h2><p><a href="https://github.com/Aden-Q/Deep-Learning/blob/master/Convolutional Neural Networks/Week 2/Residual%2BNetworks%2B-%2Bv2.ipynb" target="_blank" rel="noopener">Residual Networks</a></p><ul><li>Very deep “plain” networks don’t work in practice because they are hard to train due to vanishing gradients.</li><li>The skip-connections help to address the Vanishing Gradient problem. They also make it easy for a ResNet block to learn an identity function.</li><li>There are two main types of blocks: The identity block and the convolutional block.</li><li>Very deep Residual Networks are built by stacking these blocks together.</li></ul><h1 id="Week-3"><a href="#Week-3" class="headerlink" title="Week 3"></a>Week 3</h1><h2 id="Object-detection"><a href="#Object-detection" class="headerlink" title="Object detection"></a>Object detection</h2><p>Object localization</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week3-3.png" style="zoom:60%"><br></center><p>Defining the target label y</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week3-4.png" style="zoom:60%"><br></center><p><strong>Landmark detection</strong></p><p>Key position detection</p><p>Basis of emotion detection, computer graphics and pose detection</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week3-5.png" style="zoom:60%"><br></center><p>If you can hire labelers or label yourself a big enough data set to do the detection task, then a neural network can perform very well</p><p><strong>Sliding windows object detection algorithm</strong></p><ol><li><p>Choosing a window size</p></li><li><p>Slide the window accross the image with a fixed stride</p></li><li><p>Make detection on each position (feed into the ConvNet and make prediction)</p></li><li><p>Repeat with larger window</p></li></ol><p>Huge disadvantage: Computation cost</p><p><strong>Convolutional implementation of sliding window</strong></p><p>Turning FC layer into convolutional layers</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week3-6.png" style="zoom:60%"><br></center><p>Share computation:</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week3-7.png" style="zoom:60%"><br></center><p><strong>Bounding box predictions</strong></p><p>YOLO algorithm: You only look once</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week3-8.png" style="zoom:60%"><br></center><p>One single convolutional implementation</p><p>Efficient, works well for real-time object detection</p><p>Specify the bounding boxes: relative to the grid cell</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week3-9.png" style="zoom:60%"><br></center><p>YOLO: One of the hardest papers to read</p><p><strong>Intersection over Union (IoU)</strong></p><p>Evaluating object localization</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week3-10.png" style="zoom:60%"><br></center><p><strong>Non-max Suppression</strong></p><p>The problem is that some objects might be detected multiple times</p><p>Non-max Suppression make sure each object is detected only once</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week3-11.png" style="zoom:60%"><br></center><p><strong>Anchor boxes</strong></p><p>One grid detect multiple objects, overlapping objects</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week3-12.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week3-13.png" style="zoom:60%"><br></center><p><strong>YOLO algorithm</strong></p><p>None-max suppression in YOLO:</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week3-14.png" style="zoom:60%"><br></center><h2 id="Region-proposals"><a href="#Region-proposals" class="headerlink" title="Region proposals"></a>Region proposals</h2><p>R-CNN, Region with CNN, picking some regions to run CNN</p><p>The ways to choose regions: <strong>semantic segmentation</strong></p><p>Still quite slow</p><h2 id="Programming-Assignment-Car-detection-with-YOLOv2"><a href="#Programming-Assignment-Car-detection-with-YOLOv2" class="headerlink" title="Programming Assignment: Car detection with YOLOv2"></a>Programming Assignment: Car detection with YOLOv2</h2><p><a href="https://github.com/Aden-Q/Deep-Learning/blob/master/Convolutional Neural Networks/Week 3/Autonomous%2Bdriving%2Bapplication%2B-%2BCar%2Bdetection%2B-%2Bv3.ipynb" target="_blank" rel="noopener">Car detection with YOLOv2</a></p><p>YOLO model is very computationally expensive to train</p><p>Find intersection:</p><ul><li>You’ll also need to find the coordinates <code>(xi1, yi1, xi2, yi2)</code> of the intersection of two boxes. Remember that:<ul><li>xi1 = maximum of the x1 coordinates of the two boxes</li><li>yi1 = maximum of the y1 coordinates of the two boxes</li><li>xi2 = minimum of the x2 coordinates of the two boxes</li><li>yi2 = minimum of the y2 coordinates of the two boxes</li></ul></li><li>In order to compute the intersection area, you need to make sure the height and width of the intersection are positive, otherwise the intersection area should be zero. Use <code>max(height, 0)</code> and <code>max(width, 0)</code>.</li></ul><p><strong>What you should remember</strong>:</p><ul><li>YOLO is a state-of-the-art object detection model that is fast and accurate</li><li>It runs an input image through a CNN which outputs a 19x19x5x85 dimensional volume. </li><li>The encoding can be seen as a grid where each of the 19x19 cells contains information about 5 boxes.</li><li>You filter through all the boxes using non-max suppression. Specifically: <ul><li>Score thresholding on the probability of detecting a class to keep only accurate (high probability) boxes</li><li>Intersection over Union (IoU) thresholding to eliminate overlapping boxes</li></ul></li><li>Because training a YOLO model from randomly initialized weights is non-trivial and requires a large dataset as well as lot of computation, we used previously trained model parameters in this exercise. If you wish, you can also try fine-tuning the YOLO model with your own dataset, though this would be a fairly non-trivial exercise. </li></ul><h1 id="Week-4"><a href="#Week-4" class="headerlink" title="Week 4"></a>Week 4</h1><h2 id="Special-applications-Face-recognition-amp-Neural-style-transfer"><a href="#Special-applications-Face-recognition-amp-Neural-style-transfer" class="headerlink" title="Special applications: Face recognition &amp; Neural style transfer"></a>Special applications: Face recognition &amp; Neural style transfer</h2><p><strong>Face verification vs. face recognition</strong></p><p>Verification:</p><ul><li>Input image, name/ID</li><li>Output whether the input image is that of the claimed person</li><li>1:1</li></ul><p>Recognition</p><ul><li>Has a database of K persons</li><li>Get an input image</li><li>Output ID if the image is any of the K persons (or “not recognized”)</li></ul><p><strong>One-shot learning</strong></p><p>Learning from one example to recognize the person again</p><p>Strategy: Learning a “similarity” function</p><p>d(img1, img2) = degree of difference between images, and predict by difference</p><p><strong>Siamese network</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week3-15.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week3-16.png" style="zoom:60%"><br></center><p><strong>Triplet loss</strong></p><p>Always look at three images at a time: Anchor, Positive and Negative</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week3-17.png" style="zoom:60%"><br></center><p>Choosing the triplets A,P,N</p><p>During training, if A,P,N are chosen randomly, $d(A,P)+\alpha \le d(A,N)$ is easily satisfied.</p><p>Idea: Choose triplets that’re “hard” to train on. (refer to the paper FaceNet)</p><p>Popular way to name system:</p><ul><li>____ Net</li><li>Deep ____</li></ul><p><strong>Neural style transfer</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week3-18.png" style="zoom:60%"><br></center><p>Visualizing what a deep network is learning</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week3-19.png" style="zoom:60%"><br></center><p><strong>Cost function</strong><br>$$<br>J(G) = \alpha J_{content}(C,G)+\beta J_{style}(S,G)<br>$$</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week3-20.png" style="zoom:60%"><br></center><p><strong>Content cost function</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week3-21.png" style="zoom:60%"><br></center><p><strong>Style cost function</strong></p><p>Define style as correlation between activations across channels</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week3-22.png" style="zoom:60%"><br></center><p><strong>Convolutions in 2D and 1D</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week3-23.png" style="zoom:60%"><br></center><p><strong>3D convolution</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-05-04-Week3-24.png" style="zoom:60%"><br></center><h2 id="Programming-Assignment-Art-generation-with-Neural-Style-Transfer"><a href="#Programming-Assignment-Art-generation-with-Neural-Style-Transfer" class="headerlink" title="Programming Assignment: Art generation with Neural Style Transfer"></a>Programming Assignment: Art generation with Neural Style Transfer</h2><p><a href="https://github.com/Aden-Q/Deep-Learning/blob/master/Convolutional Neural Networks/Week 4/Art%2BGeneration%2Bwith%2BNeural%2BStyle%2BTransfer%2B-%2Bv2.ipynb" target="_blank" rel="noopener">Art generation with Neural Style Transfer</a></p><p>Neural Style Transfer (NST) uses a previously trained convolutional network, and builds on top of that.</p><h2 id="Programming-Assignment-Face-Recognition-for-the-Happy-House"><a href="#Programming-Assignment-Face-Recognition-for-the-Happy-House" class="headerlink" title="Programming Assignment: Face Recognition for the Happy House"></a>Programming Assignment: Face Recognition for the Happy House</h2><p><a href="https://github.com/Aden-Q/Deep-Learning/blob/master/Convolutional Neural Networks/Week 4/Face%2BRecognition%2Bfor%2Bthe%2BHappy%2BHouse%2B-%2Bv3.ipynb" target="_blank" rel="noopener">Face Recognition for the Happy House</a></p><ul><li>Face verification solves an easier 1:1 matching problem; face recognition addresses a harder 1:K matching problem.</li><li>The triplet loss is an effective loss function for training a neural network to learn an encoding of a face image.</li><li>The same encoding can be used for verification and recognition. Measuring distances between two images’ encodings allows you to determine whether they are pictures of the same person.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Deeplearning.ai Specialization&lt;/p&gt;
&lt;p&gt;Course Notes&lt;/p&gt;
    
    </summary>
    
      <category term="Note" scheme="http://yoursite.com/categories/Note/"/>
    
    
      <category term="Programming" scheme="http://yoursite.com/tags/Programming/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://yoursite.com/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Structuring Machine Learning Projects</title>
    <link href="http://yoursite.com/2019/04/24/Structuring%20Machine%20Learning%20Projects/"/>
    <id>http://yoursite.com/2019/04/24/Structuring Machine Learning Projects/</id>
    <published>2019-04-25T04:17:34.000Z</published>
    <updated>2019-04-26T14:46:56.955Z</updated>
    
    <content type="html"><![CDATA[<p>Deeplearning.ai Specialization</p><p>Course Notes</p><a id="more"></a><h1 id="Week-1"><a href="#Week-1" class="headerlink" title="Week 1"></a>Week 1</h1><h2 id="ML-Strategy"><a href="#ML-Strategy" class="headerlink" title="ML Strategy"></a>ML Strategy</h2><p>Ideas:</p><ul><li>Collect more data</li><li>Collect more diverse training set</li><li>Train algorithm longer with gradient descent</li><li>Try Adam instead of gradient descent</li><li>Try bigger network</li><li>Try smaller network</li><li>Try dropout</li><li>Add $L_2$ regularization</li><li>Network architecture<ul><li>Activation functions</li><li>Hidden units</li></ul></li></ul><p><strong>Orthogonalization</strong></p><p>Chain of assumptions in ML</p><ul><li>Fit training set well on cost function</li><li>Fit dev set well on cost function</li><li>Fit test set well on cost function</li><li>Performs well in real world</li></ul><p>Early stopping is not a good strategy</p><p><strong>Single number evaluation metric</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-25-Week1-1.png" style="zoom:60%"><br></center><p>$F_1$ Score can evaluate the model performance<br>$$<br>F_1 = \frac{2}{\frac1P+\frac1R}<br>$$</p><p><strong>Optimizing and satisficing</strong></p><p>E.g.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-25-Week1-2.png" style="zoom:60%"><br></center><p><strong>Splitting data</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-25-Week1-3.png" style="zoom:60%"><br></center><p>Set the test set to be big enough to give high confidence in the overall performance of your system</p><p>Model performance</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-25-Week1-4.png" style="zoom:60%"><br></center><p>So long as ML is worse than humans, you can:</p><ul><li>Get labeled data from humans</li><li>Gain insight from manual error analysis: Why did a person get this right?</li><li>Better analysis of bias/variance</li></ul><p>Human-level error as a proxy for Bayes error</p><p><strong>Surpassing human-level performance</strong></p><ul><li>Online advertising</li><li>Product recommendations</li><li>Logistics (predicting transit time)</li><li>Loan approvals</li><li>Speech recognition</li><li>Computer vision</li></ul><p><strong>Two fundamental assumptions of supervised learning</strong></p><ol><li>You can fit the training set pretty well.</li><li>The training set performance generalizes pretty well to the dev/test set.</li></ol><p>Avoidable bias: Train bigger model. Train longer/better optimization algorithms. NN architecture/hyperparameters search.</p><p>Variance: More data. Regularization. NN architecture/hyperparameters search.</p><h1 id="Week-2"><a href="#Week-2" class="headerlink" title="Week 2"></a>Week 2</h1><h2 id="ML-Strategy-1"><a href="#ML-Strategy-1" class="headerlink" title="ML Strategy"></a>ML Strategy</h2><p><strong>Incorrectly labeld examples</strong></p><p>DL algorithms are quite robust to random errors in the training set. (so long as the total dataset is large enough and the error percentage is not too high)</p><p>DL are less robust to systematic errors</p><p>Error analysis:</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-26-Week2-1.png" style="zoom:60%"><br></center><p>Effor evaluation</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-26-Week2-2.png" style="zoom:60%"><br></center><p><strong>Correcting incorrect dev/test set examples</strong></p><ul><li>Apply same process to your dev and test sets to make sure they continue come from the same distribution</li><li>Consider examining examples your algorithm got right as well as ones it got wrong</li><li>Train and dev/test data may now come slightly different distributions</li></ul><p>Tips for building a machine learning system</p><ul><li>Set up dev/test set and metric</li><li>Build initial system quickly</li><li>Use Bias/Variance analysis &amp; Error analysis to prioritize next steps</li></ul><p><strong>Build your first system quickly, then iterate!</strong></p><p><strong>Training and testing on different distributions</strong></p><p>To determine which aspect brings the effect of bias/variance, whether it’s the general reason or the reason caused by changing data distribution. It’s better to introduce a new set:</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-26-Week2-3.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-26-Week2-4.png" style="zoom:60%"><br></center><p>Introducing this set helps us determine whether the problem is data mismatch or variance.</p><p><strong>Bias/variance on mismatched training and dev/test sets</strong></p><ul><li>Human level</li><li>Training set error</li><li>Training_dev set error</li><li>Dev error</li><li>Test error</li></ul><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-26-Week2-5.png" style="zoom:60%"><br></center><p><strong>Addressing data mismatch</strong></p><ul><li>Carry out manual error analysis to try to understand difference between training and dev/test sets</li><li>Making training data more similar; or collect more data similar to dev/test sets</li><li>Artificial data synthesis</li></ul><p><strong>Transfer learning</strong></p><p>Pre-training and fune-tuning</p><p>When transfer learning makes sense</p><ul><li>Task A and B have the same input x.</li><li>You have a lot more data for Task A than Task B.</li><li>Low level features from A could be helpful for learning B.</li></ul><p><strong>Multi-task learning</strong></p><p>Multiple labels</p><p>When multi-task learning makes sense</p><ul><li>Training on a set of tasks could benefit from having shared lower-level features.</li><li>Usually: Amount of data you have for each task is quite similar.</li><li>Can train a big enough neural network to do well on all the tasks.</li></ul><p>Transfer learning is used much more often than multi-task learning</p><p><strong>End-to-end learning</strong></p><p>Multiple stages —&gt; A single neural network</p><p>End-to-end network requires a lot of data to gain high performance</p><p><strong>Pros and cons of end-to-end deep learning</strong></p><p>Pros:</p><ul><li>Let the data speak</li><li>Less hand-designing of components needed</li></ul><p>Cons:</p><ul><li>May need large amount of data</li><li>Excludes potentially useful hand-designed components</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Deeplearning.ai Specialization&lt;/p&gt;
&lt;p&gt;Course Notes&lt;/p&gt;
    
    </summary>
    
      <category term="Note" scheme="http://yoursite.com/categories/Note/"/>
    
    
      <category term="Programming" scheme="http://yoursite.com/tags/Programming/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://yoursite.com/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Scikit-learn Summary</title>
    <link href="http://yoursite.com/2019/04/24/Sklearn%20Summary/"/>
    <id>http://yoursite.com/2019/04/24/Sklearn Summary/</id>
    <published>2019-04-25T04:17:34.000Z</published>
    <updated>2019-05-03T18:55:56.773Z</updated>
    
    <content type="html"><![CDATA[<p>Scikit-learn</p><p>Summary</p><a id="more"></a><h1 id="ML-Methods"><a href="#ML-Methods" class="headerlink" title="ML Methods"></a>ML Methods</h1><ul><li><p>Decision tree </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line">model = DecisionTreeRegressor(random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>Random forest</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line">model = RandomForestRegressor(n_estimators=<span class="number">100</span>, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>Model selection</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">train_X, val_X, train_y, val_y = train_test_split(X, y, train_size=<span class="number">0.8</span>, test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></li><li><p>Evaluation metrics</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line">val_mae = mean_absolute_error(val_prediction, val_y)</span><br></pre></td></tr></table></figure></li><li><p>Save to csv</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">output = pd.DataFrame(&#123;<span class="string">'Id'</span>: test_data.Id, </span><br><span class="line">                       <span class="string">'SalePrice'</span>: test_preds&#125;)</span><br></pre></td></tr></table></figure></li><li><p>Imputation</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line"><span class="comment"># Imputation</span></span><br><span class="line">my_imputer = SimpleImputer()</span><br><span class="line">imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))</span><br><span class="line">imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Imputation removed column names; put them back</span></span><br><span class="line">imputed_X_train.columns = X_train.columns</span><br><span class="line">imputer_X_valid.columns = X_valid.columns</span><br><span class="line"></span><br><span class="line"><span class="comment"># Extension to Imputation</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> cols_with_missing:</span><br><span class="line">    X_train_plus[col + <span class="string">'_was_missing'</span>] = X_train_plus[col].isnull()</span><br><span class="line">    X_valid_plus[col + <span class="string">'_was_missing'</span>] = X_valid_plus[val].isnull()</span><br></pre></td></tr></table></figure></li><li><p>Numerical selection</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X = X_full.select_dtypes(exclude=[<span class="string">'object'</span>])</span><br></pre></td></tr></table></figure></li><li><p>Drop NA</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cols_with_missing = [col <span class="keyword">in</span> X_train.columns <span class="keyword">if</span> X_train[col].isnull().any()]</span><br><span class="line">X_train = X_train.drop(cols_with_missing, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><p>Drop Categorical Variables (typically perform worst)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">drop_X_train = X_train.select_dtypes(exclude=[<span class="string">'object'</span>])</span><br><span class="line">drop_X_valid = X_valid.select_dtypes(exclude=[<span class="string">'object'</span>])</span><br></pre></td></tr></table></figure></li><li><p>Label Encoding (randomly)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line">label_X_train = X_train.copy()</span><br><span class="line">label_X_valid = X_valid.copy()</span><br><span class="line">label_encoder = LabelEncoder()</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> object_cols:</span><br><span class="line">    label_X_train[col] = label_encoder.fit_transform(X_train[col])</span><br><span class="line">    label_X_valid[col] = label_encoder.transform(X_valid[col])</span><br></pre></td></tr></table></figure></li><li><p>One-Hot Encoding (typically perform best)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line">OH_encoder = OneHotEncoder(handle_unknown=<span class="string">'ignore'</span>, sparse=<span class="keyword">False</span>)</span><br><span class="line">OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))</span><br><span class="line">OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))</span><br><span class="line"><span class="comment"># One-hot encoding removed index; put it back</span></span><br><span class="line">OH_cols_train.index = X_train.index</span><br><span class="line">OH_cols_valid.index = X_valid.index</span><br><span class="line"><span class="comment"># Remove categorical columns</span></span><br><span class="line">num_X_train = X_train.drop(object_cols, axis=<span class="number">1</span>)</span><br><span class="line">num_X_valid = X_valid.drop(object_cols, axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># Add one-hot encoded columns to numerical features</span></span><br><span class="line">OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=<span class="number">1</span>)</span><br><span class="line">OH_X_valid = pd.concat([num_X_train, OH_cols_valid], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># pandas implementation</span></span><br><span class="line">X_train = pd.get_dummies(X_train)</span><br><span class="line">X_valid = pd.get_dummies(X_valid)</span><br><span class="line">X_test = pd.get_dummies(X_test)</span><br><span class="line">X_train, X_valid = X_train.align(X_valid, join=<span class="string">'left'</span>, axis=<span class="number">1</span>)</span><br><span class="line">X_train, X_test = X_train.align(X_test, join=<span class="string">'left'</span>, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><p>Remove rows with missing target</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.dropna(axis=<span class="number">0</span>, subset=[<span class="string">'SalePrice'</span>, inplace=<span class="keyword">True</span>])</span><br></pre></td></tr></table></figure></li><li><p>Object columns selection</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">object_cols = [col <span class="keyword">for</span> col <span class="keyword">in</span> X_train.columns <span class="keyword">if</span> X_train[col].dtype == <span class="string">"object"</span>]</span><br><span class="line"><span class="comment"># Columns that can be safely label encoded</span></span><br><span class="line">good_label_cols = [col <span class="keyword">for</span> col <span class="keyword">in</span> object_cols <span class="keyword">if</span> set(X_train[col]) == set(X_valid[col])]</span><br><span class="line"><span class="comment"># Problematic columns that will be dropped from the dataset</span></span><br><span class="line">bad_label_cols = list(set(object_cols)-set(good_label_cols)</span><br></pre></td></tr></table></figure></li><li><p>Ont-Hot columns selection</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">low_cardinality_cols = [col <span class="keyword">for</span> col <span class="keyword">in</span> object_cols <span class="keyword">if</span> X_train[col].nunique() &lt; <span class="number">10</span>]</span><br><span class="line">high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))</span><br></pre></td></tr></table></figure></li><li><p>Pipelines</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.compose <span class="keyword">import</span> ColumnTransformer</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="comment"># from sklearn.pipeline import make_pipeline # used when there is no data preprocessing</span></span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line"><span class="comment"># Preprocessing for numerical data</span></span><br><span class="line">numerical_transformer = SimpleImputer(strategy=<span class="string">'constant'</span>)</span><br><span class="line"><span class="comment"># Preprocessing for categorical data</span></span><br><span class="line">categorical_transformer = Pipeline(steps=[</span><br><span class="line">    (<span class="string">'imputer'</span>, SimpleImputer(strategy=<span class="string">'most_frequent'</span>)),</span><br><span class="line">    (<span class="string">'onehot'</span>, OneHotEncoder(handdle_unknown=<span class="string">'ignore'</span>))</span><br><span class="line">])</span><br><span class="line"><span class="comment"># Bundle preprocessing for numerical and categorical data</span></span><br><span class="line">preprocessor = ColumnTransformer(</span><br><span class="line">transformers=[</span><br><span class="line">        (<span class="string">'num'</span>, numerical_transformer, numerical_cols),</span><br><span class="line">        (<span class="string">'cat'</span>, categorical_transformer, categorical_cols)</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"><span class="comment"># Bundle preprocessing and modeling code in a pipeline</span></span><br><span class="line">my_pipeline = Pipeline(steps=[(<span class="string">'preprossor'</span>, preprocessor),</span><br><span class="line">                              (<span class="string">'model'</span>, model)</span><br><span class="line">                             ])</span><br><span class="line"><span class="comment"># Preprocessing of training data, fit model</span></span><br><span class="line">my_pipeline.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># Preprocessing of validation data, get predictions</span></span><br><span class="line">preds = my_pipeline.predict(X_valid)</span><br><span class="line"><span class="comment"># Evaluate the model</span></span><br><span class="line">score = mean_absolute_error(y_valid, preds)</span><br></pre></td></tr></table></figure></li><li><p>Proper columns selection</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Select categorical columns with relatively low cardinality</span></span><br><span class="line">categorical_cols = [cname <span class="keyword">for</span> cname <span class="keyword">in</span> X_train_full.columns <span class="keyword">if</span> </span><br><span class="line">                    X_train_full[cname].nunique() &lt; <span class="number">10</span> <span class="keyword">and</span></span><br><span class="line">                    X_train_full[cname].dtype == <span class="string">'object'</span>]</span><br><span class="line"><span class="comment"># Select numerical columns</span></span><br><span class="line">numerical_cols = [cname <span class="keyword">for</span> cname <span class="keyword">in</span> X_train_full.columns <span class="keyword">if</span></span><br><span class="line">                  X_train_full[cname].dtype <span class="keyword">in</span> [<span class="string">'int64'</span>, <span class="string">'float64'</span>]]</span><br></pre></td></tr></table></figure></li><li><p>Read in data</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_full = pd.read_csv(<span class="string">'../input/train.csv'</span>, index_col=<span class="string">'Id'</span>)</span><br></pre></td></tr></table></figure></li><li><p>Output data</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">output = pd.DataFrame(&#123;<span class="string">'Id'</span>: X_test.index,</span><br><span class="line">                       <span class="string">'SalePrice'</span>: preds_test&#125;)</span><br><span class="line">output.to_csv(<span class="string">'submission.csv'</span>, index=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure></li><li><p>Cross validation</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line">score = <span class="number">-1</span> * cross_val_score(my_pipeline, X, y</span><br><span class="line">                             cv=<span class="number">5</span>,<span class="comment"># 5 folds cross validation</span></span><br><span class="line">                             scoring=<span class="string">'neg_mean_absolute_error'</span>) <span class="comment"># scoring='accuracy' for classification problem</span></span><br></pre></td></tr></table></figure></li><li><p><strong>XGBoost</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBRegressor</span><br><span class="line"></span><br><span class="line">my_model = XGBRegressor()</span><br><span class="line"><span class="comment"># my_model = XGBRegressor(n_estimators=500)</span></span><br><span class="line"><span class="comment"># my_model.fit(X_train, y_train,</span></span><br><span class="line"><span class="comment">#   early_stopping_rounds=5,</span></span><br><span class="line"><span class="comment">#       eval_set=[(X_valid, y_valid)],</span></span><br><span class="line"><span class="comment">#              verbose=False)</span></span><br><span class="line"><span class="comment"># my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05)</span></span><br><span class="line"><span class="comment"># my_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)</span></span><br><span class="line">my_model.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><p>parameters:</p><ul><li><code>n_estimators</code>: specifies how many times to go through the modeling cycle. low: underfuttubg, high: overfitting. Typically value is 100-1000.</li><li><code>early_stopping_rounds</code>: this parameter causes the model to stop iterating when the validation scores stops improving. It’s smart to set a high value for <code>n_estimators</code> and then use <code>early_stopping_rounds</code> to find the optimal time to stop iterating. Since random chance sometimes causes a single round where validation scores don’t improve, you nedd to specify a number for how many rounds of straight deterioration to allow before stopping. Setting <code>early_stopping_rounds=5</code> is a reasonable choice. In this case, we stop after 5 straight rounds of deteriorating validation scores.</li><li><code>learning_rate</code>: Instead of getting predictions by simply adding up the predictions from each component model, we can multiply the predictions from each model by a small number (known as the <strong>learning rate</strong>) before adding them in. In general, a small learning rate and large number of estimators will yield more accurate XGBoost models, through it will also take the model longer to train since it does more iterations through the cycle. As default, XGBoost sets <code>learning_rate=0.1</code>.</li><li><code>n_jobs</code>: On large datasets where runtime is a consideration, you can use parallelism to build your model faster. It’s common to set the parameter <code>n_jobs</code> equal to the number of cores on your machine. On smaller datasets, this won’t help.</li></ul></li><li><p>Data leakage: target leakage and train-test contamination.</p><ul><li>Target leakage: Any variable updated (or created) after the target value is realized should be excluded. Data exploration can really help identify target leakage.</li><li>Train-test contamination: Happens when validation data affects the preprocessing behavior. If the validation is based on a simple train-test split, exclude the validation from any type of fitting, including the fitting of preprocessing steps. Careful separation of training and validation data can prevent train-test contamination and pipelines can help.</li></ul></li></ul><p><strong>Machine Learning Pipeline</strong></p><ol><li>Define Preprocessing Steps</li><li>Define the Model</li><li>Create and Evaluate the Pipeline</li></ol><h1 id="Pandas"><a href="#Pandas" class="headerlink" title="Pandas"></a>Pandas</h1><ul><li><p>Creating data: <strong>DataFrame</strong> and <strong>Series</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DataFrame</span></span><br><span class="line">pd.DataFrame(&#123;<span class="string">'Yes'</span>: [<span class="number">50</span>, <span class="number">21</span>], <span class="string">'No'</span>: [<span class="number">131</span>, <span class="number">2</span>]&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># DataFrame with customized index (row labels)</span></span><br><span class="line">pd.DataFrame(&#123;<span class="string">'Bob'</span>: [<span class="string">'I liked it.'</span>, <span class="string">'It was awful.'</span>], </span><br><span class="line">              <span class="string">'Sue'</span>: [<span class="string">'Pretty good.'</span>, <span class="string">'Bland.'</span>]&#125;,</span><br><span class="line">             index=[<span class="string">'Product A'</span>, <span class="string">'Product B'</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Series</span></span><br><span class="line">pd.Series([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="comment"># Series with a name</span></span><br><span class="line">pd.Series([<span class="number">30</span>, <span class="number">35</span>, <span class="number">40</span>], index=[<span class="string">'2015 Sales'</span>, <span class="string">'2016 Sales'</span>, <span class="string">'2017 Sales'</span>], name=<span class="string">'Product A'</span>)</span><br></pre></td></tr></table></figure></li><li><p>Read file</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wine_reviews = pd.read_csv(<span class="string">"../input/wine-reviews/winemag-data-130k-v2.csv"</span>, index_col=<span class="number">0</span>) <span class="comment"># set the index column (otherwise pandas will create a default column)</span></span><br></pre></td></tr></table></figure></li><li><p>Summary functions (type-aware)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reviews.describe()</span><br></pre></td></tr></table></figure></li><li><p>List unique values</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">reviews.taster_name.unique()</span><br><span class="line">reviews.taster_name.value_counts()<span class="comment"># unique value counts</span></span><br></pre></td></tr></table></figure></li><li><p>Maps</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Series.map</span></span><br><span class="line">review_point_mean = reviews.points.mean()</span><br><span class="line">reviews_points.map(<span class="keyword">lambda</span> p: p - review_point_mean)</span><br><span class="line"><span class="comment"># Apply to a whole DataFrame</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remean_points</span><span class="params">(row)</span>:</span></span><br><span class="line">    row.points = row.points - review_points_mean</span><br><span class="line">    <span class="keyword">return</span> row</span><br><span class="line"></span><br><span class="line">reviews.apply(remean_points, axis=<span class="string">'columns'</span>)</span><br><span class="line"><span class="comment"># Alternative way (faster)</span></span><br><span class="line">review_points_mean = reviews.points.mean()</span><br><span class="line">review.points - review_points_mean</span><br></pre></td></tr></table></figure></li><li><p>Group and sorting</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">reviews.groupby(<span class="string">'points'</span>).points.count()</span><br><span class="line">reviews.groupby(<span class="string">'points'</span>).price.min()</span><br><span class="line"><span class="comment"># Value counts</span></span><br><span class="line">reviews_per_region = reviews.region_1.fillna(<span class="string">'Unknown'</span>).value_counts().sort_values(ascending=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">countries_reviewd = countries_reviewd.reset_index()</span><br><span class="line">countries_reviewd.sort_values(by=<span class="string">'len'</span>, ascending=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># Sort by index</span></span><br><span class="line">countries_reviewd.sort_index()</span><br><span class="line"><span class="comment"># Sort with multiple keys</span></span><br><span class="line">countries_reviewd.sort_values(by=[<span class="string">'country'</span>, <span class="string">'len'</span>])</span><br></pre></td></tr></table></figure></li><li><p>Data types</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reviews.price.dtype<span class="comment"># Get type for a single column</span></span><br><span class="line">reviews.dtypes<span class="comment"># Get types for all columns</span></span><br><span class="line">reviews.points.astype(<span class="string">'float64'</span>)<span class="comment"># Change data type</span></span><br></pre></td></tr></table></figure></li><li><p>Missing data</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">reviews[reviews.country.isnull()]</span><br><span class="line">reviews.region_2.fillna(<span class="string">"Unknown"</span>)<span class="comment"># Fill in NA data entries</span></span><br></pre></td></tr></table></figure></li><li><p>Rename (specifying a <code>index</code> or <code>column</code> keyword parameter)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reviews.rename(columns=&#123;<span class="string">'points'</span>: <span class="string">'score'</span>&#125;)</span><br><span class="line">reviews.rename(index=&#123;<span class="number">0</span>: <span class="string">'firstEntry'</span>, <span class="number">1</span>: <span class="string">'secondEntry'</span>&#125;)<span class="comment"># Rarely</span></span><br><span class="line">reviews.rename_axis(<span class="string">"wines"</span>, axis=<span class="string">'rows'</span>).rename_axis(<span class="string">"fields"</span>, axis=<span class="string">'columns'</span>)<span class="comment"># Set names for row index and column index</span></span><br></pre></td></tr></table></figure></li><li><p>Combining (<code>concat</code>, <code>join</code>, <code>merge</code>)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.concat([canadian_youtube, british_youtube])</span><br></pre></td></tr></table></figure></li><li><p>Indexing</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Single row</span></span><br><span class="line">df.iloc[<span class="number">2</span>]<span class="comment"># Make sure to specify a number, zero index</span></span><br><span class="line">df.loc[<span class="string">'BADL'</span>]<span class="comment"># Make sure to pass a value from dataframes's index</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiple rows</span></span><br><span class="line">df.loc[[<span class="string">'BADL'</span>, <span class="string">'ARCH'</span>, <span class="string">'ACAD'</span>]]</span><br><span class="line">df.iloc[[<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>]]</span><br><span class="line">df[:<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Indexing columns</span></span><br><span class="line">df[<span class="string">'State'</span>].head(<span class="number">3</span>)</span><br><span class="line">df.State.head(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Clean the column names</span></span><br><span class="line">df.columns = [col.replace(<span class="string">' '</span>, <span class="string">'_'</span>).lower() <span class="keyword">for</span> col <span class="keyword">in</span> df.columns]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Indexing columns and rows</span></span><br><span class="line">df[[<span class="string">'state'</span>, <span class="string">'acres'</span>]][:<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Indexing scalar value</span></span><br><span class="line">df.state.iloc[<span class="number">2</span>]<span class="comment"># return a scalar rather than a Series or DataFrame</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Boolean indexing</span></span><br><span class="line">df[df.state == <span class="string">'UT'</span>]</span><br><span class="line"><span class="comment"># Some helpful logical operations: ~: not; |: or; &amp;: and</span></span><br><span class="line"><span class="comment"># E.g.</span></span><br><span class="line">df[(df.latitude &gt; <span class="number">60</span>) | (df.acres &gt; <span class="number">10</span>**<span class="number">6</span>)].head(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># isin and isnull</span></span><br><span class="line">df[df.state.isin([<span class="string">'WA'</span>, <span class="string">'OR'</span>, <span class="string">'CA'</span>])].head()</span><br></pre></td></tr></table></figure></li><li><p>Bar charts and categorical data (nominal categories and ordinal categories)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reviews[<span class="string">'province'</span>].value_counts().head(<span class="number">10</span>).plot.bar()</span><br></pre></td></tr></table></figure><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-30-1.png" style="zoom:70%"><br></center></li><li><p>Line charts</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reviews[<span class="string">'points'</span>].value_counts().sort_index().plot.line()</span><br></pre></td></tr></table></figure><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-30-2.png" style="zoom:70%"><br></center></li><li><p>Area charts (line charts with the bottom shaded in)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reviews[<span class="string">'point'</span>].value_count().sort_index().plot.area()</span><br></pre></td></tr></table></figure><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-30-3.png" style="zoom:70%"><br></center></li><li><p>Histograms (work best for interval variables without skew or for ordinal categorical variables)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reviews[reviews[<span class="string">'price'</span>] &lt; <span class="number">200</span>][<span class="string">'price'</span>].plot.hist()</span><br></pre></td></tr></table></figure><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-30-4.png" style="zoom:70%"><br></center></li><li><p>Scatter plot (remember to downsample to avoid duplicate), works best with relatively small datasets and large number of unique values</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">review[reviews[<span class="string">'price'</span>] &lt; <span class="number">100</span>].sample(<span class="number">100</span>).plot.scatter(x=<span class="string">'price'</span>, y=<span class="string">'points'</span>)</span><br></pre></td></tr></table></figure><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-30-5.png" style="zoom:70%"><br></center></li><li><p>Hexplot</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reviews[reviews[<span class="string">'price'</span>] &lt; <span class="number">100</span>].plot.hexbin(x=<span class="string">'price'</span>, y=<span class="string">'points'</span>, gridsize=<span class="number">15</span>)</span><br></pre></td></tr></table></figure><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-30-6.png" style="zoom:70%"><br></center></li><li><p>Stacked plots (work best for nominal categorical or small ordinal categorical variables)</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-30-7.png" style="zoom:70%"><br></center><p>One categorical variable in the columns, one categorical variable in the rows, and counts of their intersection in the entries.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wine_counts.plot.bar(stacked=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-30-8.png" style="zoom:70%"><br></center><p>Area plot</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-30-9.png" style="zoom:70%"><br></center></li><li><p>Bivariate line chart</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wine_counts.plot.line()</span><br></pre></td></tr></table></figure><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-30-10.png" style="zoom:70%"><br></center></li></ul><h1 id="Seaborn"><a href="#Seaborn" class="headerlink" title="Seaborn"></a>Seaborn</h1><ul><li><p>Line charts</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.figure(figsize=(<span class="number">14</span>,<span class="number">6</span>))</span><br><span class="line">plt.title(<span class="string">"Daily Global Streams of Popular Songs in 2017-2018"</span>)</span><br><span class="line">sns.lineplot(daat=fifa_data)<span class="comment"># plot a line for every column in the dataset</span></span><br><span class="line"><span class="comment"># sns.lineplot(data=spotyfy_data['Shape of You'], label="Shape of You")# plot one column, label represents adding legend</span></span><br></pre></td></tr></table></figure></li><li><p>Bar charts</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">6</span>))</span><br><span class="line">plt.title(<span class="string">"..."</span>)</span><br><span class="line">sns.barplot(x=flight_data.index, y=flight_data=[<span class="string">'NK'</span>])</span><br></pre></td></tr></table></figure></li><li><p>Heatmap</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">14</span>,<span class="number">7</span>))</span><br><span class="line">plt.title(<span class="string">"..."</span>)</span><br><span class="line">sns.heatmap(data=flight_data, annot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure></li><li><p>Scatter plot</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns.scatterplot(x=insurance_data[<span class="string">'bmi'</span>], y=[insurance_data[<span class="string">'charges'</span>]])</span><br><span class="line"><span class="comment"># Add a regression line</span></span><br><span class="line">sns.regplot(x=insurance_data[<span class="string">'bmi'</span>], y=insurance_data[<span class="string">'charges'</span>])</span><br><span class="line"><span class="comment"># Color code with a boolean/categorical column</span></span><br><span class="line">sns.scatterplot(x=insurance_data[<span class="string">'bmi'</span>], y=insurance_data[<span class="string">'charges'</span>], hue=insurance_data[<span class="string">'smoker'</span>])</span><br><span class="line"><span class="comment"># Two regression line</span></span><br><span class="line">sns.lmplot(x=<span class="string">"bmi"</span>, y=<span class="string">"charges"</span>, hue=<span class="string">"smoker"</span>, data=insurance_data)</span><br><span class="line"><span class="comment"># Categorical scatter plot</span></span><br><span class="line">sns.swarmplot(x=insurance_data[<span class="string">'smoker'</span>],</span><br><span class="line">              y=insurance_data[<span class="string">'charges'</span>])</span><br></pre></td></tr></table></figure><p><strong>One regression line</strong></p></li></ul><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-29-7.png" style="zoom:50%"><br></center><p>​        <strong>Two regression lines (hue specified, color coded)</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-29-8.png" style="zoom:50%"><br></center><p>​        <strong>Categorical scatter plot</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-29-9.png" style="zoom:50%"><br></center><ul><li><p>Histograms</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns.distplot(a=iris_data[<span class="string">'Petal Length (cm)'</span>], kde=<span class="keyword">False</span>, bins=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># a: choose the column we'd like to plot</span></span><br><span class="line"><span class="comment"># kde=False (always included in a histogram)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Histograms for multiple columns</span></span><br><span class="line">sns.distplot(a=iris_set_data[<span class="string">'Petal Length (cm)'</span>], label=<span class="string">'Iris-setosa'</span>, kde=<span class="keyword">False</span>)</span><br><span class="line">sns.distplot(a=iris_ver_data[<span class="string">'Petal Length (cm)'</span>], label=<span class="string">'Iris-versicolor'</span>, kde=<span class="keyword">False</span>)</span><br><span class="line">sns.distplot(a=iris_vir_data[<span class="string">'Petal Length (cm)'</span>], label=<span class="string">'Iris-virginica'</span>, kde=<span class="keyword">False</span>)</span><br><span class="line">plt.legend()<span class="comment"># Force legend to appear</span></span><br></pre></td></tr></table></figure><p>​        <strong>General higtogram</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-29-1.png" style="zoom:50%"><br></center><p><strong>Histogram of multiple columns</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-29-4.png" style="zoom:50%"><br></center></li><li><p>Density plots</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns.kdeplot(data=iris_data[<span class="string">'Petal Length (cm)'</span>], shade=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># 2D kde plots</span></span><br><span class="line">sns.jointplot(x=iris_data[<span class="string">'Petal Length (cm)'</span>], y=iris_data[<span class="string">'Sepal Width (cm)'</span>], kind=<span class="string">"kde"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># KDE plot for multiple columns</span></span><br><span class="line">sns.kdeplot(data=iris_set_data[<span class="string">'Petal Length (cm)'</span>], label=<span class="string">'Iris-setosa'</span>, shade=<span class="keyword">True</span>)<span class="comment"># shade is used to fill in the color below the KDE plot</span></span><br><span class="line"><span class="comment"># and so on</span></span><br></pre></td></tr></table></figure><p>​        <strong>General KDE plot</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-29-2.png" style="zoom:50%"><br></center><p>​        <strong>2D KDE plot</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-29-3.png" style="zoom:50%"><br></center><pre><code>**KDE plot for multiple columns**</code></pre><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-29-5.png" style="zoom:50%"><br></center></li><li><p>Choosing the best type of chart</p><ul><li><strong>Trends</strong>: A trend is defined as a pattern of change.<ul><li><code>sns.lineplot</code> - <strong>Line charts</strong> are the best to show trends over a period of time, and multiple lines can be used to show trends in more than one  group.</li></ul></li><li><strong>Relationship</strong>: There are many different chart for show this.<ul><li><code>sns.barplot</code>- <strong>Bar charts</strong> are useful for comparing quantities corresponding to different groups.</li><li><code>sns.heatmap</code>- <strong>Heatmaps</strong> can be used to find color-coded patterns in table  of numbers.</li><li><code>sns.scatterplot</code>- <strong>Scatter plots</strong> show the relation between two continuous variables; if color-coded, we can also show the relationship with categorical variables.</li><li><code>sns.regplot</code>- Including a <strong>regression line</strong> in the  scatter plot makes it easier to see linear relationship between two variables.</li><li><code>sns.lmplot</code>- This command is useful for drawing multiple regression lines, if the scatter plot contains multiple, color-coded groups.</li><li><code>sns.swarmplot</code>- <strong>Categorical scatter plots</strong> show the relationship between a continuous variable and a categorical variable.</li></ul></li><li><strong>Distribution</strong>: We visualize distribution to show the possible values that we can expect to see in a variable.<ul><li><code>sns.distplot</code>- <strong>Histograms</strong> show the distribution of a single numerical variable.</li><li><code>sns.kdeplot</code>- <strong>KDE plots</strong> (or <strong>2D KDE plots</strong>) to show anestimated, smooth distribution of a single numerical variable (or two numerical variables).</li><li><code>sns.jointplot</code>- This command is useful for simultaneously displaying a 2D KDE plot with the correspoinding KDE plots for each individual variable.</li></ul></li></ul></li><li><p>Set styles for the figure</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sns.set_style(<span class="string">"dark"</span>)<span class="comment"># set dark style for the figure, five themes: "darkgrid", "whitegrid", "dark", "white", "ticks". The default theme is "darkgrid"</span></span><br><span class="line">plt.figure(figure=(<span class="number">12</span>,<span class="number">6</span>))<span class="comment"># change the figure size</span></span><br></pre></td></tr></table></figure></li><li><p>Countplot (doesn’t require us to shape the data)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.countplot(reviews[<span class="string">'points'</span>])</span><br></pre></td></tr></table></figure><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-30-11.png" style="zoom:70%"><br></center></li><li><p>KDE plot (kernel density estimate), worse choice for ordinal categorical data, KDE plot will fit to something that doesn’t exist.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.kdeplot(reviews.query(<span class="string">'price &lt; 200'</span>).price)</span><br></pre></td></tr></table></figure><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-30-12.png" style="zoom:70%"><br></center></li><li><p>Bivariate hex plot</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sns.jointplot(x=<span class="string">'price'</span>, y=<span class="string">'points'</span>, data=reviews[reviews[<span class="string">'price'</span>] &lt; <span class="number">100</span>], kind=<span class="string">'hex'</span>, gridsize=<span class="number">20</span>)</span><br></pre></td></tr></table></figure><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-30-13.png" style="zoom:70%"><br></center></li><li><p>Boxplot</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">df = reviews[reviews.variety.isin(reviews.variety.value_counts().head(<span class="number">5</span>).index)]</span><br><span class="line"></span><br><span class="line">sns.boxplot(</span><br><span class="line">    x=<span class="string">'variety'</span>,</span><br><span class="line">    y=<span class="string">'points'</span>,</span><br><span class="line">    data=df</span><br><span class="line">)</span><br></pre></td></tr></table></figure><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-30-14.png" style="zoom:70%"><br></center><p>The center of the distributions shown above is the “box” in boxplot. The top of the box is the 75th percentile, while the bottom is the 25th percentile. In other words, half of the data is distributed within the box. The green line in the middle is the median. (50% of the data in the distribution is located within the box)</p><p>The other part of the plot, the “whiskers”, shows the extent of the points beyond the center of the distribution. Individual circles beyond that are outliers.</p><p>Boxplots are great for summarizing the shape of many datasets. They also don’t have a limit in terms of numeracy: you can place as many boxes in the plot as you feel comfortable squeezing onto the page.</p><p>However, they only work for interval variables and nomial variables and nominal variables with a large number of possible values and they don’t carry any information about individual values, only treating the distribution as a whole. </p></li><li><p>Violinplot</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sns.violinplot(</span><br><span class="line">    x=<span class="string">'variety'</span>,</span><br><span class="line">    y=<span class="string">'points'</span>,</span><br><span class="line">    data=reviews[reviews.variety.isin(reviews.variety.value_counts()[:<span class="number">5</span>].index)]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-30-15.png" style="zoom:70%"><br></center><p>Harder to misinterpret and much prettier than the utilitarian boxplot</p></li><li><p>Facet</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-30-16.png" style="zoom:60%"><br></center></li></ul><h1 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h1><ul><li><p>Transfer Learning</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python.keras.applications <span class="keyword">import</span> ResNet50</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.layers <span class="keyword">import</span> Dense, Flatten, GlobalAveragePooling2D</span><br><span class="line"></span><br><span class="line">num_classes = <span class="number">2</span></span><br><span class="line">resnet_weights_path = <span class="string">'../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'</span></span><br><span class="line"></span><br><span class="line">my_new_model = Sequential()</span><br><span class="line">my_new_model.add(ResNet50(include_top=<span class="keyword">False</span>, pooling=<span class="string">'avg'</span>, weights=resnet_weights_path))</span><br><span class="line">my_new_model.add(Dense(num_classes, activation=<span class="string">'softmax'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Say not to train first layer (ResNet) model. It is already trained</span></span><br><span class="line">my_new_model.layers[<span class="number">0</span>].trainable = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Compipe model</span></span><br><span class="line">my_new_model.compile(optimizer=<span class="string">'sgd'</span>, loss=<span class="string">'categorical_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.applications.resnet50 <span class="keyword">import</span> preprocess_input</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"></span><br><span class="line">image_size = <span class="number">224</span></span><br><span class="line">data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit model</span></span><br><span class="line">train_generator = data_generator.flow_from_directory(</span><br><span class="line">        <span class="string">'../input/urban-and-rural-photos/rural_and_urban_photos/train'</span>,</span><br><span class="line">        target_size=(image_size, image_size),</span><br><span class="line">        batch_size=<span class="number">24</span>,</span><br><span class="line">        class_mode=<span class="string">'categorical'</span>)</span><br><span class="line"></span><br><span class="line">validation_generator = data_generator.flow_from_directory(</span><br><span class="line">        <span class="string">'../input/urban-and-rural-photos/rural_and_urban_photos/val'</span>,</span><br><span class="line">        target_size=(image_size, image_size),</span><br><span class="line">        class_mode=<span class="string">'categorical'</span>)</span><br><span class="line"></span><br><span class="line">my_new_model.fit_generator(</span><br><span class="line">        train_generator,</span><br><span class="line">        steps_per_epoch=<span class="number">3</span>,</span><br><span class="line">        validation_data=validation_generator,</span><br><span class="line">        validation_steps=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li></ul><h1 id="Relate-CoLab-and-Google-Drive"><a href="#Relate-CoLab-and-Google-Drive" class="headerlink" title="Relate CoLab and Google Drive"></a>Relate CoLab and Google Drive</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">!apt-get install -y -qq software-properties-common python-software-properties module-init-tools</span><br><span class="line">!add-apt-repository -y ppa:alessandro-strada/ppa 2&gt;&amp;1 &gt; /dev/null</span><br><span class="line">!apt-get update -qq 2&gt;&amp;1 &gt; /dev/null</span><br><span class="line">!apt-get -y install -qq google-drive-ocamlfuse fuse</span><br><span class="line">from google.colab import auth</span><br><span class="line">auth.authenticate_user()</span><br><span class="line">from oauth2client.client import GoogleCredentials</span><br><span class="line">creds = GoogleCredentials.get_application_default()</span><br><span class="line">import getpass</span><br><span class="line">!google-drive-ocamlfuse -headless -id=&#123;creds.client_id&#125; -secret=&#123;creds.client_secret&#125; &lt; /dev/null 2&gt;&amp;1 | grep URL</span><br><span class="line">vcode = getpass.getpass()</span><br><span class="line">!echo &#123;vcode&#125; | google-drive-ocamlfuse -headless -id=&#123;creds.client_id&#125; -secret=&#123;creds.client_secret&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># specify Google Drive root directory，名为drive</span><br><span class="line">!mkdir -p drive</span><br><span class="line">!google-drive-ocamlfuse drive</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">os.chdir(&quot;drive/Colab Notebooks&quot;)</span><br></pre></td></tr></table></figure><p>Success!</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Scikit-learn&lt;/p&gt;
&lt;p&gt;Summary&lt;/p&gt;
    
    </summary>
    
      <category term="Note" scheme="http://yoursite.com/categories/Note/"/>
    
    
      <category term="Programming" scheme="http://yoursite.com/tags/Programming/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://yoursite.com/tags/TensorFlow/"/>
    
      <category term="Keras" scheme="http://yoursite.com/tags/Keras/"/>
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
      <category term="Scikit-learn" scheme="http://yoursite.com/tags/Scikit-learn/"/>
    
  </entry>
  
  <entry>
    <title>Convolutional Neural Networks in TensorFlow</title>
    <link href="http://yoursite.com/2019/04/21/Convolutional%20Neural%20Networks%20in%20TensorFlow/"/>
    <id>http://yoursite.com/2019/04/21/Convolutional Neural Networks in TensorFlow/</id>
    <published>2019-04-22T01:03:34.000Z</published>
    <updated>2019-05-08T05:13:02.359Z</updated>
    
    <content type="html"><![CDATA[<p>Deeplearning.ai, Convolutional Neural Networks in TensorFlow</p><p>Course Notes</p><a id="more"></a><h1 id="Week-1"><a href="#Week-1" class="headerlink" title="Week 1"></a>Week 1</h1><h2 id="Exploring-a-Larger-Dataset"><a href="#Exploring-a-Larger-Dataset" class="headerlink" title="Exploring a Larger Dataset"></a>Exploring a Larger Dataset</h2><p>data generation:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.image</span><br><span class="line"><span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"></span><br><span class="line">train_datagen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</span><br><span class="line"></span><br><span class="line">train_generator = train_datagen.flow_from_directory(</span><br><span class="line">train_dir,</span><br><span class="line">    target_size=(<span class="number">150</span>,<span class="number">150</span>),</span><br><span class="line">    batch_size=<span class="number">20</span>,</span><br><span class="line">    class_mode=<span class="string">'binary'</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">history = model.fit_generator(</span><br><span class="line">train_generator,</span><br><span class="line">    steps_per_epoch=<span class="number">100</span>,</span><br><span class="line">    epochs=<span class="number">15</span>,</span><br><span class="line">    validation_data=validation_generator,</span><br><span class="line">    validation_steps=<span class="number">50</span>,</span><br><span class="line">    verbose=<span class="number">2</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>RMSProp optimization algorithm is preferable to stochastic gradient descent (SGD).</p><h1 id="Week-2"><a href="#Week-2" class="headerlink" title="Week 2"></a>Week 2</h1><h2 id="Augmentation-A-technique-to-avoid-overfitting"><a href="#Augmentation-A-technique-to-avoid-overfitting" class="headerlink" title="Augmentation: A technique to avoid overfitting"></a>Augmentation: A technique to avoid overfitting</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Updated to do image augmentation</span></span><br><span class="line">train_datagen = ImageDataGenerator(</span><br><span class="line">rescale = <span class="number">1.</span>/<span class="number">255</span>,</span><br><span class="line">    rotation_range=<span class="number">40</span>,</span><br><span class="line">    width_shift_range=<span class="number">0.2</span>,</span><br><span class="line">    height_shift_range=<span class="number">0.2</span>,</span><br><span class="line">    shear_range=<span class="number">0.2</span>,</span><br><span class="line">    zoom_range=<span class="number">0.2</span>,</span><br><span class="line">    horizontal_flip=<span class="keyword">True</span>,</span><br><span class="line">    fill_mode=<span class="string">'nearest'</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>We should do image augmentation both on training set and testing set.</p><p>Using keras API, all augmentation is done in memory, not in disk.</p><h1 id="Week-3"><a href="#Week-3" class="headerlink" title="Week 3"></a>Week 3</h1><h2 id="Transfer-Learning"><a href="#Transfer-Learning" class="headerlink" title="Transfer Learning"></a>Transfer Learning</h2><p>Take existing model rather than train from scratch.</p><p>Inception</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> Model</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.applications.inception_v3 <span class="keyword">import</span> InceptionV3</span><br><span class="line"></span><br><span class="line">local_weights_file = <span class="string">'/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'</span></span><br><span class="line"></span><br><span class="line">pre_trained_model = InceptionV3(input_shape = (<span class="number">150</span>,<span class="number">150</span>,<span class="number">3</span>),</span><br><span class="line">                               include_top = <span class="keyword">False</span>,</span><br><span class="line">                               weights = <span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">pre_trained_model.load_weights(local_weights_file)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> pre_trained_model.layers:</span><br><span class="line">    layer.trainable = <span class="keyword">False</span><span class="comment"># lock layers</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line"></span><br><span class="line">last_layer = pre_trained_model.get_layer(<span class="string">'mixed7'</span>)</span><br><span class="line">last_output = last_layer.output</span><br><span class="line"></span><br><span class="line">x = layers.Flatten()(last_output)</span><br><span class="line">x = layers.Dense(<span class="number">1024</span>, activation=<span class="string">'relu'</span>)(x)</span><br><span class="line">x = layers.Dropout(<span class="number">0.2</span>)(x)</span><br><span class="line">x = layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)(x)</span><br><span class="line"></span><br><span class="line">model = Model(pre_trained_model.input, x)</span><br><span class="line">model.compile(optimizer = RMSprop(lr=<span class="number">0.0001</span>),</span><br><span class="line">             loss = <span class="string">'binary_crossentropy'</span>,</span><br><span class="line">             metrics = [<span class="string">'acc'</span>])</span><br></pre></td></tr></table></figure><h1 id="Week-4"><a href="#Week-4" class="headerlink" title="Week 4"></a>Week 4</h1><h2 id="Multiclass-Classifications"><a href="#Multiclass-Classifications" class="headerlink" title="Multiclass Classifications"></a>Multiclass Classifications</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">train_datagen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</span><br><span class="line"></span><br><span class="line">train_generator = tran_datagen.flow_from_directory(</span><br><span class="line">train_dir,</span><br><span class="line">    target_size=(<span class="number">300</span>,<span class="number">300</span>),</span><br><span class="line">    batch_size=<span class="number">128</span>,</span><br><span class="line">    class_mode=<span class="string">'categorical'</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> RMSprop</span><br><span class="line"></span><br><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,</span><br><span class="line">             optimizer=RMSprop(lr=<span class="number">0.001</span>),</span><br><span class="line">             metrics=[<span class="string">'acc'</span>])</span><br></pre></td></tr></table></figure><p>Useful function:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># summary</span></span><br><span class="line">model.summary()</span><br><span class="line"><span class="comment"># visualization</span></span><br><span class="line">plot_model(model, to_file=<span class="string">'model.png'</span>)</span><br><span class="line">SVG(model_to_dot(happyModel).create(prog=<span class="string">'dot'</span>, format=<span class="string">'svg'</span>))</span><br><span class="line"><span class="comment"># evaluate</span></span><br><span class="line">model.evaluate(X_test, y_test)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Deeplearning.ai, Convolutional Neural Networks in TensorFlow&lt;/p&gt;
&lt;p&gt;Course Notes&lt;/p&gt;
    
    </summary>
    
      <category term="Note" scheme="http://yoursite.com/categories/Note/"/>
    
    
      <category term="Programming" scheme="http://yoursite.com/tags/Programming/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://yoursite.com/tags/TensorFlow/"/>
    
      <category term="Keras" scheme="http://yoursite.com/tags/Keras/"/>
    
  </entry>
  
  <entry>
    <title>Improving Deep Neural Networks</title>
    <link href="http://yoursite.com/2019/04/20/Improving%20Deep%20Neural%20Networks/"/>
    <id>http://yoursite.com/2019/04/20/Improving Deep Neural Networks/</id>
    <published>2019-04-20T22:55:34.000Z</published>
    <updated>2019-04-25T02:47:39.632Z</updated>
    
    <content type="html"><![CDATA[<p>Deeplearning.ai Specialization</p><p>Course Notes</p><a id="more"></a><h1 id="Week-1"><a href="#Week-1" class="headerlink" title="Week 1"></a>Week 1</h1><h2 id="Practical-aspects-of-Deep-Learning"><a href="#Practical-aspects-of-Deep-Learning" class="headerlink" title="Practical aspects of Deep Learning"></a>Practical aspects of Deep Learning</h2><p><strong>Train/dev/test sets</strong></p><p>Applied ML is a highly iterative process</p><ul><li>layers</li><li>Hidden units</li><li>Learning rates</li><li>Activation functions</li></ul><p>Idea - Code - Experiment</p><p>Make sure the dev and test come from the same distribution. Because we want to evaluate effects from the dev set.</p><p>Not having a test set might be okay. (Only dev set.)</p><p><strong>Bias and Variance</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-24-Week1-1.png" style="zoom:60%"><br></center><p><strong>Basic recipe for machine learning</strong></p><p>Hign bias -&gt; Bigger network</p><p>High variance -&gt; More data / Regularization / NN architecture</p><p>Training a bigger network almost never hurts if regularized well</p><p><strong>Regularization</strong></p><p>L2 regularization is used much more often</p><p>Frobenius norm is used in Neural network -&gt; weight decay</p><p>Another technical regularization method is dropout</p><p><strong>Inverted dropout</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keep_prob = <span class="number">0.8</span> <span class="comment"># chosen by user</span></span><br><span class="line">d = np.random.rand(a.shape[<span class="number">0</span>],a.shape[<span class="number">1</span>]) &lt; keep_prob</span><br><span class="line">a = np.multiply(a,d)</span><br><span class="line">a /= keep_prob<span class="comment"># because the output will be reduced by ...</span></span><br></pre></td></tr></table></figure><p>At test time, we <strong>should not</strong> include drop out because we don’t want our prediction to be random. Otherwise, there will be noise.</p><p>We should apply dropout both during forward and backward propagation and shutdown the same neurons in each iteration and we should scale the output by divide keep_prob</p><p>Intuition: Cant’t rely on any one feature, so have to spread out weights.</p><p>Drop out is very frequently used by computer vision. But doesn’t generalize in other application areas.</p><p>We will lose gradient checking tool when including drop out.</p><p><strong>So in conclusion, we should plot the loss figure when including drop out. Because the cost function J is no longer well-defined (thinking about the dead neurons)</strong></p><p>Other regularization methods:</p><ul><li><p>Data augmentation</p></li><li><p>Early stopping</p></li></ul><p>L2 regularization is more useful but the search space for $\lambda$ is large</p><p><strong>Normalization</strong><br>$$<br>\mu=\frac1m\sum_{i=1}^mx^{(i)} \<br>\sigma=\frac1m\sum_{i=1}^mx^{(i)}**2<br>$$</p><p><strong>Normalize the training set and test set the same way</strong></p><p>Zero mean and variance one</p><p><strong>Vanishing / Exploding gradients</strong></p><p><strong>Random initialization</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Wl = np.random.randn(shape) * np.sqrt(<span class="number">2</span>/n)</span><br></pre></td></tr></table></figure><p>Different initialization lead to different results</p><p>Random initialization is used to break symmetry and make sure different hidden units can learn different things</p><p>We should not initialize to values that are too large</p><p>In the weight initialization procedure, we can also tuning the hyper parameter for the variance.</p><p>Poor initialization can lead to vanishing/exploding gradients, which also slow down the optimization algorithm.</p><p><strong>He initialization （designed for ReLU activation), of significant importance</strong></p><p>Multiply random initialization by<br>$$<br>\sqrt{\frac{2}{dimension\space of\space the\space previous \space layer}}<br>$$</p><p><strong>Gradient checking</strong><br>$$<br>\frac{f(\theta+\epsilon)-f(\theta-\epsilon)}{2\epsilon} \approx g(\theta)<br>$$<br>Two-side difference is prefered<br>$$<br>\epsilon = 10^{-7}<br>$$</p><p><strong>Tips for Grad check</strong></p><ul><li>Don’t use in training - only to debug</li><li><p>If algorithm fails grad check, look at components to try to identify bug</p></li><li><p>Remeber regularization</p></li><li>Doesn’t work with dropout (set keep_prob = 1.0 to do grad check)</li><li>Run at random initialization</li></ul><h2 id="Programming-Assignment-Initialization"><a href="#Programming-Assignment-Initialization" class="headerlink" title="Programming Assignment: Initialization"></a>Programming Assignment: Initialization</h2><p><a href="https://github.com/Aden-Q/Deep-Learning/blob/master/Improving Deep Neural Networks/Week 1/Initialization.ipynb" target="_blank" rel="noopener">Initialization</a></p><h2 id="Programming-Assignment-Regularization"><a href="#Programming-Assignment-Regularization" class="headerlink" title="Programming Assignment: Regularization"></a>Programming Assignment: Regularization</h2><p><a href="https://github.com/Aden-Q/Deep-Learning/blob/master/Improving Deep Neural Networks/Week 1/Regularization%2B-%2Bv2.ipynb" target="_blank" rel="noopener">Regularization</a></p><h2 id="Programming-Assignment-Gradient-Checking"><a href="#Programming-Assignment-Gradient-Checking" class="headerlink" title="Programming Assignment: Gradient Checking"></a>Programming Assignment: Gradient Checking</h2><p><a href="https://github.com/Aden-Q/Deep-Learning/blob/master/Improving Deep Neural Networks/Week 1/Gradient%2BChecking%2Bv1.ipynb" target="_blank" rel="noopener">Gradient Checking</a></p><h1 id="Week-2"><a href="#Week-2" class="headerlink" title="Week 2"></a>Week 2</h1><h2 id="Optimization-algorithms"><a href="#Optimization-algorithms" class="headerlink" title="Optimization algorithms"></a>Optimization algorithms</h2><p><strong>Mini-batch gradient descent</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-24-Week2-1.png" style="zoom:60%"><br></center><p>Choosing mini-batch size:</p><ul><li>If mini-batch size = m : Batch gradient descend</li><li>If mini-batch size size = 1: Stochastic gradient descent: Every example is a mini-batch</li></ul><p>Tipds:</p><ul><li>If small training set: Use batch gradient descent —— 2000</li><li>Otherwise typical mini-batch size: 64, 128, 256, 512</li><li>Make sure mini batch fit in CPU/GPU memory</li><li>Shuffling and Partitioning are the two steps required to build mini-batches</li></ul><p><strong>Exponentially weighted averages</strong><br>$$<br>v_t=\beta v_{t-1}+(1-\beta)\theta_t<br>$$<br><strong>Bias correction</strong><br>$$<br>\frac{v_t}{1-\beta^t}<br>$$</p><p><strong>Graduebt descent with momentum</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-24-Week2-2.png" style="zoom:60%"><br></center><p>Hyperparameters: $\alpha,\beta$<br>$$<br>\beta = 0.9<br>$$<br>In practice, people don’t usually do bias correction in deep learning because after just ten iterations, moving average will warm up.</p><p><strong>RMSprop</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-24-Week2-3.png" style="zoom:60%"><br></center><p><strong>Adam</strong></p><p>Take momentum and RMSprop and put them together</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-24-Week2-4.png" style="zoom:60%"><br></center><p>Hyperparameters choice:<br>$$<br>\beta_1=0.9 \\<br>\theta_2=0.999 \\<br>\epsilon=10^{-8}<br>$$<br>And try different range of $\alpha $</p><p>Adam: Adaptive moment estimation</p><p>For complex problems there will be bigger gains</p><p><strong>Learning rate decay</strong><br>$$<br>\alpha=\frac{1}{1+decay-rate*epoch-num}\alpha_0 \\<br>\alpha = 0.95^{epoch-num}\alpha_0 \\<br>\alpha =\frac{k}{\sqrt{epoch-num}}\alpha_0<br>$$</p><p>Saddle point is different from local optimum. In high dimensional space, we are more likely to encounter saddle point.</p><ul><li>Unlikely to get stuck in a bad local optima</li><li>Plateaus can make learning slow</li></ul><h2 id="Programming-Assignment-Optimization"><a href="#Programming-Assignment-Optimization" class="headerlink" title="Programming Assignment: Optimization"></a>Programming Assignment: Optimization</h2><p><a href="https://github.com/Aden-Q/Deep-Learning/blob/master/Improving Deep Neural Networks/Week 2/Optimization%2Bmethods.ipynb" target="_blank" rel="noopener">Optimization</a></p><h1 id="Week-3"><a href="#Week-3" class="headerlink" title="Week 3"></a>Week 3</h1><h2 id="Hyperparameter-tuning-Batch-Normalization-and-Programming-Frameworks"><a href="#Hyperparameter-tuning-Batch-Normalization-and-Programming-Frameworks" class="headerlink" title="Hyperparameter tuning, Batch Normalization and Programming Frameworks"></a>Hyperparameter tuning, Batch Normalization and Programming Frameworks</h2><p>In deep learning, choose the hyperparameter combination at random</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-24-Week3-1.png" style="zoom:60%"><br></center><p><strong>Batch Normalization</strong></p><p>Given some intermediate value in NN. $z^{(1)},…,z^{(m)}$<br>$$<br>\mu=\frac1m\sum z^{(i)} \\<br>\sigma^2=\frac1m\sum(z_i-\mu)^2 \\<br>z_{norm}^{(i)} = \frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\epsilon}} \\<br>\hat{z^{(i)}} = \gamma z_{norm}^{(i)} + \beta<br>$$<br>$\gamma $ and $\beta$  are learnable parameters</p><p>BN deals with the situation of Covariate shift</p><p>BN weakens the coupling between earlier layer’s paramers abd later layer’s parameters</p><p>BN has a slight regularization effect, similar to dropout</p><p>BN speeds up training process</p><p><strong>Softmax regression</strong></p><p>Softmax regressuib generalizes logistic regression to C classes</p><p>Loss function<br>$$<br>L(\hat{y},y) = -\sum_{j=1}^cy_jlog\hat{y_j}<br>$$</p><p><strong>Choosing deep learning frameworks</strong></p><ul><li><p>Ease of programming (development and deployment)</p></li><li><p>Running speed</p></li><li>Truly open (open source with good governance)</li></ul><p><strong>TensorFlow</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.Variable(<span class="number">0</span>, dtype.float32)</span><br><span class="line">tf.placeholder(tf.float32, [<span class="number">3</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>Typical steps:</p><ul><li>Create a graph containing Tensors (Variables, Placeholders …) and Operations (tf.matmul, tf.add)</li><li>Create a session</li><li>Initialize the session</li><li>Run the session to execute the graph</li></ul><h2 id="Programming-Assignment-TensorFlow"><a href="#Programming-Assignment-TensorFlow" class="headerlink" title="Programming Assignment: TensorFlow"></a>Programming Assignment: TensorFlow</h2><p><a href="https://github.com/Aden-Q/Deep-Learning/blob/master/Improving Deep Neural Networks/Week 3/Tensorflow%2BTutorial.ipynb" target="_blank" rel="noopener">TensorFlow</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Deeplearning.ai Specialization&lt;/p&gt;
&lt;p&gt;Course Notes&lt;/p&gt;
    
    </summary>
    
      <category term="Note" scheme="http://yoursite.com/categories/Note/"/>
    
    
      <category term="Programming" scheme="http://yoursite.com/tags/Programming/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://yoursite.com/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Neural Networks and Deep Learning</title>
    <link href="http://yoursite.com/2019/04/20/Neural%20Networks%20and%20Deep%20Learning/"/>
    <id>http://yoursite.com/2019/04/20/Neural Networks and Deep Learning/</id>
    <published>2019-04-20T06:55:34.000Z</published>
    <updated>2019-04-24T19:28:41.249Z</updated>
    
    <content type="html"><![CDATA[<p>Deeplearning.ai Specialization</p><p>Course Notes</p><a id="more"></a><h1 id="Week-1"><a href="#Week-1" class="headerlink" title="Week 1"></a>Week 1</h1><h2 id="Introduction-to-deep-learning"><a href="#Introduction-to-deep-learning" class="headerlink" title="Introduction to deep learning"></a>Introduction to deep learning</h2><ul><li>AI is the new Electricity</li><li><p>Electricitty had once transformed countless industries: transportation, manufacturing, healthcare, communications, and more</p></li><li><p>AI will now bring about an equally big transformation</p></li></ul><p>ReLU: Rectified Linear Unit</p><p>Supervised Learning</p><p><strong>Structured Data vs. Unstructured Data</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-20-Week1-1.png" style="zoom:60%"><br></center><p>Keys in deep learning:</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-20-Week1-2.png" style="zoom:60%"><br></center><h1 id="Week-2"><a href="#Week-2" class="headerlink" title="Week 2"></a>Week 2</h1><h2 id="Neural-Networks-Basics"><a href="#Neural-Networks-Basics" class="headerlink" title="Neural Networks Basics"></a>Neural Networks Basics</h2><p><strong>Binary Classification</strong></p><p>Input group:</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-20-Week1-8.png" style="zoom:60%"><br></center><p>output group:</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-20-Week1-9.png" style="zoom:60%"><br></center><p><strong>Logistic Regression (again…)</strong><br>$$<br>\hat{y} = \sigma(w^Tx+b) \\<br>\sigma(z) = \frac{1}{1+e^{-z}}<br>$$</p><p><strong>Logistic Regression cost function</strong></p><p>When deciding which loss function to use, we should really consider if it is convex or not.</p><p>$$<br>L(\hat{y},y) = -(ylog\hat{y}+(1-y)log(1-\hat{y}))<br>$$</p><p>$$<br>J(w,b)=\frac{1}{m}\sum_{i=1}^mL(\hat{y}^{(i)},y^{(i)})<br>$$</p><p><strong>Gradient Descent</strong><br>$$<br>w:=w-\alpha\frac{\partial J(w,b)}{\partial w} \\<br>w:=w-\alpha dw \\<br>b:=b-\alpha\frac{\partial J(w,b)}{\partial b} \\<br>b:=b-\alpha db<br>$$</p><p><strong>Computation Graph</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-20-Week1-10.png" style="zoom:60%"><br></center><p><strong>Logistic regression derivatives</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-20-Week1-11.png" style="zoom:60%"><br></center><p><strong>Vectorization</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.array([1,2,3,4])</span><br><span class="line">print(a)</span><br><span class="line"></span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">a = np.random.rand(100000)</span><br><span class="line">b = np.random.rand(100000)</span><br><span class="line">tic = time.time()</span><br><span class="line">c = np.dot(a,b)</span><br><span class="line">toc = time.time()</span><br><span class="line"></span><br><span class="line">print(&quot;Vectorized version:&quot; + str(1000*(toc-tic) + &quot;ms&quot;))</span><br></pre></td></tr></table></figure><p>Whenever possible, avoid explicit for-loops.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-20-Week1-12.png" style="zoom:60%"><br></center><h2 id="Programming-Assignment-Logistic-Regression-with-a-Neural-Network-mindset"><a href="#Programming-Assignment-Logistic-Regression-with-a-Neural-Network-mindset" class="headerlink" title="Programming Assignment: Logistic Regression with a Neural Network mindset"></a>Programming Assignment: Logistic Regression with a Neural Network mindset</h2><p><a href="https://github.com/Aden-Q/Deep-Learning/blob/master/Neural Networks and Deep Learning/Week2/Python%2BBasics%2BWith%2BNumpy%2Bv3.ipynb" target="_blank" rel="noopener">Python Basics with numpy</a></p><p><a href="https://github.com/Aden-Q/Deep-Learning/blob/master/Neural Networks and Deep Learning/Week2/Logistic%2BRegression%2Bwith%2Ba%2BNeural%2BNetwork%2Bmindset%2Bv5.ipynb" target="_blank" rel="noopener">Logistic Regression with a Neural Network mindset</a></p><h1 id="Week-3"><a href="#Week-3" class="headerlink" title="Week 3"></a>Week 3</h1><h2 id="Shallow-neural-networks"><a href="#Shallow-neural-networks" class="headerlink" title="Shallow neural networks"></a>Shallow neural networks</h2><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-21-Week3-1.png" style="zoom:60%"><br></center><p><strong>Neural Network Representation</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-21-Week3-2.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-21-Week3-3.png" style="zoom:60%"><br></center><p><strong>Vectorization</strong></p><p>Convention: stacking by column</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-21-Week3-4.png" style="zoom:60%"><br></center><p><strong>Activation functions</strong><br>$$<br>sigmoid(z)=\frac{1}{1+e^{-z}}    \\<br>tanh(z) = \frac{e^z-e^{-z}}{e^z+e^{-z}} \\<br>ReLU(z) = max(0,z)<br>$$<br>Tanh function is superior than sigmoid function. (generally)</p><p>sigmoid: binary classification (used in the output layer)</p><p>suggestions: use relu in all other units except for the output unit.</p><p>If we are doing a binary classification, we use sigmoid in the output unit in most cases.</p><p><strong>Fornulas for propagation</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-21-Week3-5.png" style="zoom:60%"><br></center><p><strong>Random initilization: Symmetry breaking</strong></p><p>Small initialization: make update faster</p><h2 id="Programming-Assignment-Planar-data-classification-with-a-hidden-layer"><a href="#Programming-Assignment-Planar-data-classification-with-a-hidden-layer" class="headerlink" title="Programming Assignment: Planar data classification with a hidden layer"></a>Programming Assignment: Planar data classification with a hidden layer</h2><p><a href="https://github.com/Aden-Q/Deep-Learning/blob/master/Neural Networks and Deep Learning/Week3/Planar%2Bdata%2Bclassification%2Bwith%2Bone%2Bhidden%2Blayer%2Bv5.ipynb" target="_blank" rel="noopener">Planar data classification with a hidden layer</a></p><h1 id="Week-4"><a href="#Week-4" class="headerlink" title="Week 4"></a>Week 4</h1><h2 id="Deep-Neural-Networks"><a href="#Deep-Neural-Networks" class="headerlink" title="Deep Neural Networks"></a>Deep Neural Networks</h2><p>Dimension:<br>$$<br>W^{[l]}:(n^{[l]},n^{[l-1]})<br>$$</p><p>$$<br>b^{[l]}:(n^{[l]},1)<br>$$</p><p>$$<br>z^{[l]},a^{[l]}:(n^{[l]},1)<br>$$</p><p>$$<br>Z^{[l]},A^{[l]}:(n^{[l]},m)<br>$$</p><p><strong>Circuit theory and deep learning</strong></p><p>Informally: There are functions you can compute with a “small”L-layer deep neural network that shallower networks require exponentially more hidden units to compute.</p><p><strong>Forward and backward functions</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-21-Week4-1.png" style="zoom:60%"><br></center><p><strong>Forward propagation for layer l</strong></p><p>Input $a^{[l-1]}$</p><p>Output $a^{[l]}$, cache ($z^{[l]}$)</p><p><strong>Backward propagation for layer l</strong></p><p>Input $da^{[l]}$</p><p>Output $da^{[l-1]}$, $dW^{[l]}$, $db^{[l]}$<br>$$<br>dz^{[l]}=da^{[l]}*g^{[l]’}(z^{[l]}) \\<br>dw^{[l]}=dz^{[l]} \cdot a^T  \\<br>db^{[l]} = dz^{[l]} \\<br>da^{[l-1]} = w^{[l]^T} \cdot dz^{[l]}<br>$$</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-21-Week4-2.png" style="zoom:60%"><br></center><h2 id="Programming-Assignment-Building-your-deep-neural-network-Step-by-Step"><a href="#Programming-Assignment-Building-your-deep-neural-network-Step-by-Step" class="headerlink" title="Programming Assignment: Building your deep neural network: Step by Step"></a>Programming Assignment: Building your deep neural network: Step by Step</h2><p><a href="https://github.com/Aden-Q/Deep-Learning/blob/master/Neural Networks and Deep Learning/Week4/Building%2Byour%2BDeep%2BNeural%2BNetwork%2B-%2BStep%2Bby%2BStep%2Bv8.ipynb" target="_blank" rel="noopener">Building your deep neural network: Step by Step</a></p><h2 id="Programming-Assignment-Deep-Neural-Network-Application"><a href="#Programming-Assignment-Deep-Neural-Network-Application" class="headerlink" title="Programming Assignment: Deep Neural Network Application"></a>Programming Assignment: Deep Neural Network Application</h2><p><a href="https://github.com/Aden-Q/Deep-Learning/blob/master/Neural Networks and Deep Learning/Week4/Deep%2BNeural%2BNetwork%2B-%2BApplication%2Bv8.ipynb" target="_blank" rel="noopener">Deep Neural Network Application</a></p><p>Done!</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Deeplearning.ai Specialization&lt;/p&gt;
&lt;p&gt;Course Notes&lt;/p&gt;
    
    </summary>
    
      <category term="Note" scheme="http://yoursite.com/categories/Note/"/>
    
    
      <category term="Programming" scheme="http://yoursite.com/tags/Programming/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://yoursite.com/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Introduction to TensorFlow</title>
    <link href="http://yoursite.com/2019/04/12/Introduction%20to%20TensorFlow/"/>
    <id>http://yoursite.com/2019/04/12/Introduction to TensorFlow/</id>
    <published>2019-04-12T11:03:34.000Z</published>
    <updated>2019-04-16T03:59:28.952Z</updated>
    
    <content type="html"><![CDATA[<p>Deeplearning.ai, Introduction to TensorFlow</p><p>Course Notes</p><a id="more"></a><h1 id="Week-1"><a href="#Week-1" class="headerlink" title="Week 1"></a>Week 1</h1><h2 id="A-New-Programming-Paradigm"><a href="#A-New-Programming-Paradigm" class="headerlink" title="A New Programming Paradigm"></a>A New Programming Paradigm</h2><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week1-1.png" style="zoom:60%"><br></center><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line">model = keras.Sequential([keras.layers.Dense(units=<span class="number">1</span>, input_shape=[<span class="number">1</span>])])</span><br><span class="line">model.compile(optimizer=<span class="string">'sgd'</span>, loss=<span class="string">'mean_squared_error'</span>)</span><br><span class="line">xs = np.array([<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],dtype=float)</span><br><span class="line">ys = np.array([<span class="number">50</span>,<span class="number">100</span>,<span class="number">150</span>,<span class="number">200</span>,<span class="number">250</span>,<span class="number">300</span>],dtype=float)</span><br><span class="line">model.fit(xs,ys,epochs=<span class="number">1000</span>)</span><br><span class="line">print(model.predict([<span class="number">7.0</span>]))</span><br></pre></td></tr></table></figure><br><br># Week2<br><br>## Introduction to Computer Vision<br><br><strong>Fashion MNIST</strong><br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fashion_mnist = keras.datasets.fashion_mnist</span><br><span class="line">(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()</span><br></pre></td></tr></table></figure><br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = keras.Sequential([</span><br><span class="line">    keras.layers.Flatten(input_shape=(28,28)),</span><br><span class="line">    keras.layers.Dense(128, actication=tf.nn.relu),</span><br><span class="line">    keras.layers.Dense(10, activation=tf.nn.softmax)</span><br><span class="line">])</span><br></pre></td></tr></table></figure><br><br>Using callbacks to control training<br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class myCallback(tf.keras.callbacks.Callback):</span><br><span class="line">def on_epoch_end(self, epoch, logs=&#123;&#125;):</span><br><span class="line">if(logs.get(&apos;loss&apos;)&lt;0.4):# logs.get(&apos;acc&apos;) control by accuracy</span><br><span class="line">            print(&apos;\nLoss is low so cancelling training!&apos;)</span><br><span class="line">            self.model.stop_training = True</span><br><span class="line"></span><br><span class="line">callbacks = myCallback()</span><br><span class="line">mnist = tf.keras.datasets.fashion_mnist</span><br><span class="line">(training_images, training_labels), (test_images, test_labels) = mnist.load_data()</span><br><span class="line">training_images = training_images/255.0</span><br><span class="line">test_images = test_images/255.0</span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    tf.keras.layers.Dense(512, activation = tf.nn.relu),</span><br><span class="line">    tf.keras.layers.Dense(10, activation = tf.nn.softmax)</span><br><span class="line">])</span><br><span class="line">model.compile(optimizer=&apos;adam&apos;, loss=&apos;sparse_categorical_crossentropy&apos;)</span><br><span class="line">model.fit(training_images, training_labels, epochs=5, callbacks=[callbacks])</span><br></pre></td></tr></table></figure><br><br># Week3<br><br>## Enhancing Vision with Convolutional Neural Networks<br><br>Effects of convolution<br><br><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-1.png" style="zoom:60%"><br></center><p>Pooling</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-2.png" style="zoom:60%"><br></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">'relu'</span>,</span><br><span class="line">                            input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>,<span class="number">3</span>), activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.MaxPooling2D(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">    tf.keras.layers.Flatten(),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">'softmax'</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure><p><code>model.summary()</code></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-3.png" style="zoom:60%"><br></center><h1 id="Week-4"><a href="#Week-4" class="headerlink" title="Week 4"></a>Week 4</h1><h2 id="Using-Real-world-Images"><a href="#Using-Real-world-Images" class="headerlink" title="Using Real-world Images"></a>Using Real-world Images</h2><p>Image generator in TensorFlow (directories structure):</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-4.png" style="zoom:60%"><br></center><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.preprocessing.image</span><br><span class="line"><span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"></span><br><span class="line">train_datagen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</span><br><span class="line">train_generator = train_datagon.flow_from_directory(</span><br><span class="line">train_dir,</span><br><span class="line">    target_size = (<span class="number">300</span>, <span class="number">300</span>),</span><br><span class="line">    batch_size = <span class="number">128</span>,</span><br><span class="line">    class_mode = <span class="string">'binary'</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>sigmoid is great for binary classification</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># learn from generator</span></span><br><span class="line">history = model.fit_generator(</span><br><span class="line">train_generator,</span><br><span class="line">    steps_per_epoch=<span class="number">8</span>,</span><br><span class="line">    epochs=<span class="number">15</span>,</span><br><span class="line">    validation_data=validation_generator,</span><br><span class="line">    validation_steps=<span class="number">8</span>,</span><br><span class="line">    verbose=<span class="number">2</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Deeplearning.ai, Introduction to TensorFlow&lt;/p&gt;
&lt;p&gt;Course Notes&lt;/p&gt;
    
    </summary>
    
      <category term="Note" scheme="http://yoursite.com/categories/Note/"/>
    
    
      <category term="Programming" scheme="http://yoursite.com/tags/Programming/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Deep Learning" scheme="http://yoursite.com/tags/Deep-Learning/"/>
    
      <category term="TensorFlow" scheme="http://yoursite.com/tags/TensorFlow/"/>
    
      <category term="Keras" scheme="http://yoursite.com/tags/Keras/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning</title>
    <link href="http://yoursite.com/2019/03/29/Machine%20Learning/"/>
    <id>http://yoursite.com/2019/03/29/Machine Learning/</id>
    <published>2019-03-29T20:03:34.000Z</published>
    <updated>2019-05-01T22:45:33.778Z</updated>
    
    <content type="html"><![CDATA[<p>Stanford University, Machine Learning</p><p>Course Notes</p><a id="more"></a><h1 id="Week-1"><a href="#Week-1" class="headerlink" title="Week 1"></a>Week 1</h1><p>Facebook, Apple’s photo application.</p><p>Google’s page rank algorithm.</p><p>Email spam filter.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Algorithms, math and how to get them work.</p><p>Machine Learning:</p><ul><li>Grew out of work in AI</li><li>New capability for computers</li></ul><p>Examples:</p><ul><li>Database mining, E.g., Web click data, medical records, biology, engineering.</li><li>Applications can’t program by hand. Helicopter, handwriting recognition, most of NLP, CV.</li><li>Self-customizing programs: E.g., Amazon, Netflix product recommendations.</li><li>Understanding human learning (brain, real AI).</li></ul><blockquote><p>  Arthur Samuel (1959). Machine Learning: Field of study that gives computers the ability to learn without being explicitly programmed.</p></blockquote><blockquote><p>  Tom Mitchell (1998). Well-posed Learnining Problem: A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.</p></blockquote><p>Machine learning algorithms:</p><ul><li>Supervised learning</li><li>Unsupervised learning</li></ul><p>Others: Reinforcement learning, recommender systems.</p><p>Supervised learning</p><hr><p>E.g., housing price prediction (regression)</p><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-1.png" style="zoom:60%"><br></center><p>E.g., Breast cancer (malignant, benign) (classification)</p><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-2.png" style="zoom:60%"><br></center><p>Unsupervised learning</p><hr><p>Clustering</p><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-3.png" style="zoom:60%"><br></center><p>Applications:</p><ul><li>Organize computing clusters</li><li>Social network analysis</li><li>Market segmentation</li><li>Astronomical data analysis</li></ul><p>E.g.</p><p>Cocktail party algorithm</p><h2 id="Linear-Regression-with-One-Variable"><a href="#Linear-Regression-with-One-Variable" class="headerlink" title="Linear Regression with One Variable"></a>Linear Regression with One Variable</h2><h3 id="Linear-Regression-with-One-Variable-1"><a href="#Linear-Regression-with-One-Variable-1" class="headerlink" title="Linear Regression with One Variable"></a>Linear Regression with One Variable</h3><p>Housing Prices.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-31-Week1-4.png" style="zoom:60% /"><br></center><p>Traing set: housing prices</p><p>Notation:</p><ul><li>m = Number of training examples</li><li>x’s = “input” variable / features</li><li>y’s = “output” variable / “target” variable</li></ul><p>(x, y) – one training example</p><p>$(x^{(i)}, y^{(i)})$ – $i^{th}$ trainining example</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-31-Week1-6.png" style="zoom:60%"><br></center><p>$$<br>h_\theta(x)=\theta_0+\theta_1x<br>$$<br>Linear regression with one variable.</p><p>Univariate linear regerssion.</p><p>Cost function</p><hr><p>Idea: Choose $\theta_0, \theta_1$ so that $h_\theta(x)$ is close to $y$ for our training examples $(x, y)$</p><p>square error cost function:<br>$$<br>J(\theta_0,\theta_1)=\frac1{2m}\sum_1^m(h_\theta(x^{(1)})-y^{(1)})^2<br>$$<br>Target: choose $\theta_0, \theta_1 $ to minimize $J(\theta_0,\theta_1)$</p><p>Contour plots to show 3D surface:</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-31-Week1-7.png" style="zoom:60%"><br></center><p>Gradient descent</p><hr><p>Have some function $J(\theta_0,\theta_1)$</p><p>Want $min_{\theta_0,\theta_1}J(\theta_0,\theta_1)$</p><p><strong>Outline:</strong></p><ul><li><p>Start with some $\theta_0, \theta_1 $</p></li><li><p>Keep changing $\theta_0, \theta_1$ to reduce $J(\theta_0,\theta_1) $ until we hopefully end up at a minimum</p></li></ul><p><strong>Gradient descent algorithm:</strong></p><p>repeat until convergence:<br>$$<br>\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)<br>$$<br>Note: Correct implementation is simultaneous update as following:<br>$$<br>temp0 :=\theta_0-\alpha\frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)<br>\\<br>temp1 :=\theta_1-\alpha\frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1)<br>\\<br>\theta_0:=temp0<br>\\<br>\theta_1:=temp1<br>$$</p><ul><li>:=    assignment</li><li>=         assertion</li><li>$\alpha$        step length</li></ul><p>If $\alpha$ is too small, gradient descent can be slow.</p><p>If $\alpha$ is too large, gradient descent can overshoot the minimum. It may fail to converge, or even diverge.</p><p>Gradient descent can converge to a local minimum, even with the learning rate $\alpha$ fixed.</p><p>As we approach a local minimum, gradient descent will automatically take smaller steps. So, no need to decrease $\alpha$ over time.</p><p>Gradient for linear regression:<br>$$<br>j = 0 : \frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1) = \frac1m\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})<br>\\<br>j = 1 : \frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1) = \frac1m\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})\cdot x^{(i)}<br>$$<br>Note again: update $\theta_0$ and $\theta_1$ simultaneously.</p><p>Gradient descent always works with convex function (without local optimum).</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-01-Week1-7.png" style="zoom:60%"><br></center><p>Batch Gradient Descent:</p><p>Batch: Each step of gradient descent uses all the training examples. (Refer to the fact that the cost function is over the entire training set.)</p><h2 id="Linear-Algebra-Review"><a href="#Linear-Algebra-Review" class="headerlink" title="Linear Algebra Review"></a>Linear Algebra Review</h2><p><strong>Matrix:</strong> Rectangular array of numbers:<br>$$<br>\begin {pmatrix}<br>1402 &amp; 191 \\<br>1371 &amp; 821<br>\end {pmatrix}<br>$$<br>Dimension of matrix: number of rows x number of columns</p><p>$A_{ij}$ = “$i,j$ entry” in the $i^{th}$ row, $j^{th}$ column.</p><p><strong>Vector:</strong> An nx1 matrix.<br>$$<br>\begin {pmatrix}<br>1402 \\<br>901<br>\end {pmatrix}<br>$$<br>$y_i$ = $i^{th}$ element</p><p>1-indexed or 0-indexed<br>$$<br>\begin {pmatrix}<br>y_1 \\<br>y_2 \\<br>y_3 \\<br>y_4<br>\end {pmatrix}<br>\space \space<br>\begin {pmatrix}<br>y_0 \\<br>y_1 \\<br>y_2 \\<br>y_3<br>\end {pmatrix}<br>$$</p><p>Convention: upper case to refer to matrics and lower case to refer to numbers or vectors.</p><p><strong>Matrix Addition</strong></p><p>Element-wise addition.</p><p>Legal matrix addtion requires matrixes with same dimension.</p><p><strong>Scalar Multiplication</strong></p><p>$$<br>3 *<br>\begin {bmatrix}<br>1 &amp; 0 \\<br>2 &amp; 5 \\<br>3 &amp; 1<br>\end {bmatrix}<br>=<br>\begin {bmatrix}<br>3 &amp; 0 \\<br>6 &amp; 15 \\<br>9 &amp; 3<br>\end {bmatrix}<br>$$</p><p><strong>Matrix Multiplication: </strong></p><p>Matrix multiply vector: To get $y_i$, multiply A’s $i^{th} $ row with elements of vector$x$, and add them up.</p><p>Prediction = DataMatrix * parameters</p><p>Matrix-matrix multiplication:</p><p>$$<br>\begin {bmatrix}<br>1 &amp; 3 &amp; 2 \\<br>4 &amp; 0 &amp; 1<br>\end {bmatrix}</p><p>\begin {bmatrix}<br>1 &amp; 3 \\<br>0 &amp; 1 \\<br>5 &amp; 2<br>\end {bmatrix}<br>=<br>\begin {bmatrix}<br>11 &amp; 10 \\<br>9 &amp; 14<br>\end {bmatrix}<br>$$</p><p><strong>Details:</strong>  </p><ul><li>A m$\times $n matrix</li><li>B n$\times $o matrix</li><li>m$\times $o matrix</li></ul><p>The $i^{th}$ column of the matrix $C$ is obtained by multiplying $A$ with the $i^{th}$ column of $B$. (for $i$ = 1,2,…,o). Then convert to matrix-vector multiplication.</p><p>Multiple compeing hypotheses: let each hypotheses corespond to a column in the second matrix.</p><p>Let $A$ and $B$ be matrices. Then in general,<br>$$<br>A \times B \ne B \times A<br>$$</p><p>$$<br>A \times (B \times C) = (A \times B) \times C<br>$$</p><p><strong>Identity Matrix:</strong></p><p>Denoted $I$ (or $I_{n\times n}$)</p><p>Examples of identity matrices:<br>$$<br>\begin {bmatrix}<br>1 &amp; 0 \\<br>0 &amp; 1<br>\end {bmatrix}<br>\<br>\begin {bmatrix}<br>1 &amp; 0 &amp; 0 \\<br>0 &amp; 1 &amp; 0 \\<br>0 &amp; 0 &amp; 1<br>\end {bmatrix}<br>\<br>\begin {bmatrix}<br>1 &amp; 0 &amp; 0 &amp; 0 \\<br>0 &amp; 1 &amp; 0 &amp; 0 \\<br>0 &amp; 0 &amp; 1 &amp; 0 \\<br>0 &amp; 0 &amp; 0 &amp; 1<br>\end {bmatrix}<br>$$<br>For any matrix $A$<br>$$<br>A \cdot I = I \cdot A = A<br>$$</p><p><strong>Matrix inverse:</strong></p><p>Not all numbers have an inverse.</p><p>If $A$ is an m$\times $m matrix (square matrix), and if it has an inverse,<br>$$<br>AA^{-1}=A^{-1}A=I<br>$$<br>Matrices that don’t have an inverse are “singular” or “degenerate”.</p><p><strong>Matrix Transpose:</strong></p><p>Example:</p><p>$$<br>A =<br>\begin {bmatrix}<br>1 &amp; 2 &amp; 0 \\<br>3 &amp; 5 &amp; 9<br>\end {bmatrix}</p><p>\</p><p>A^T =<br>\begin {bmatrix}<br>1 &amp; 3 \\<br>2 &amp; 5 \\<br>0 &amp; 9<br>\end {bmatrix}<br>$$</p><p>Let $A$ be an m$\times $n matrix, and let $B=A^T$. Then $B$ is an n$\times $m matrix, and<br>$$<br>B_{ij}=A_{ji}<br>$$</p><h1 id="Week-2"><a href="#Week-2" class="headerlink" title="Week 2"></a>Week 2</h1><h2 id="Linear-Regression-with-Multiple-Variables"><a href="#Linear-Regression-with-Multiple-Variables" class="headerlink" title="Linear Regression with Multiple Variables"></a>Linear Regression with Multiple Variables</h2><p>Multiple features (variables).</p><hr><p>Notation:</p><p>$n$ = number of features</p><p>$x^{(i)}$ = input (features) of $i^{th}$ training example.</p><p>$x_j^{(i)}$ = value of feature $j$ in $i^{th}$ training example.</p><p>Hypothesis:<br>$$<br>h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+….+\theta_nx_n<br>$$<br>For convenience of notation, define $x_0=1$. ($x_0^{(i)}=1$)<br>$$<br>x =<br>\begin {bmatrix}<br>x_0 \\<br>x_1 \\<br>… \\<br>x_n<br>\end {bmatrix}</p><p>\space<br>\space</p><p>\theta =<br>\begin {bmatrix}<br>\theta_0 \\<br>\theta_1 \\<br>… \\<br>\theta_n<br>\end {bmatrix}<br>\in<br>R^{n+1}<br>$$</p><p>$$<br>h_\theta(x)=\theta_0x_0+\theta_1x_1+\theta_2x_2+….+\theta_nx_n<br>=\theta^Tx<br>$$</p><p>Multivariate linear regression.</p><p>Gradient descent for multivariate linear regression</p><hr><p>$$<br>\theta_j:=\theta_j-\alpha \frac{\partial}{\partial\theta_j}J(\theta)<br>$$</p><p>$$<br>\theta_j:=\theta_j-\alpha \frac{1}{m}\sum_{i=1}^m<br>(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}<br>$$</p><p>Feature Scaling</p><hr><p>Idea: Make sure features are on a similar scale.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-03-Week2-1.png" style="zoom:60%"><br></center><p>Get every feature into approximately a $-1\leq x_i \leq 1$ range.</p><p><strong>Mean normalization</strong></p><p>Replace $x_i$ with $x_i-\mu_i$ to make features have approximately zero mean (Do not apply to $x_0=1$) and then divide it by its standard deviatioin (divide by max-min is also fine).</p><p>Convergence test:</p><p>Declare convergence if $J(\theta)$ decreases by less than $10^{-3}$ in one iteration.</p><ul><li>For sufficiently small $\alpha$, $J(\theta )$ should decrease on every iteration.</li><li>But if $\alpha$ is too small, gradient descent can be slow to converge.</li></ul><p><strong>Summary</strong></p><ul><li>If $\alpha$ is too small: slow convergence.</li><li>If $\alpha$ is too large: $J(\theta) $ may not decrease on every iteration; may not converge.</li></ul><p>To choose $\alpha$ , try: …, 0.001, 0.01, 0.1, 1, … (0.003, 0.03, 0.3), 3x each time.</p><p><strong>Polynomial regression</strong></p><p>Sometimes by defining new features we can get better data and model.</p><p>We can transform a polynomial regression into a mutlivariant linear regression model:</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-03-Week2-2.png" style="zoom:60%"><br></center><p><strong>Normal Equation</strong></p><p>Method to solve for $\theta$ analytically.</p><p>Design matrix: each row represents a sample.</p><p>With normal equation, feature scaling is not necessary.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-03-Week2-3.png" style="zoom:60%"><br></center><p><strong>Comparasion</strong></p><p>Gradient Descent:</p><ul><li>Need to choose $\alpha$.</li><li>Needs many iterations.</li><li>Works well even when $n$ is large.</li></ul><p>Normal Equation:</p><ul><li>No need to choose $\alpha$.</li><li>Don’t need to iterate.</li><li>Need to compute $(X^TX)^{-1}$</li><li>Slow if $n$ is very large. (depending on the number of features, typically n ~ 1000)</li></ul><p><strong>Noninvertibility when using normal equation</strong></p><p>For normal equation method:<br>$$<br>\theta = (X^TX)^{-1}X^Ty<br>$$<br>What if $X^TX$ is non-invertible? (Singular/degenerate)</p><ul><li>pinv</li><li>inv</li></ul><p>What causes $X^TX$ non-invertible?</p><ul><li>Redundant features (linearly dependent).</li><li>Too many features (e.g. $m\leq n$). (delete some features, or use regularization).</li></ul><h2 id="Octave-Matlab-Tutorial"><a href="#Octave-Matlab-Tutorial" class="headerlink" title="Octave/Matlab Tutorial"></a>Octave/Matlab Tutorial</h2><p>Choices: Octave, MATLAB, Python Numpy and R.</p><ul><li>who: list all the variables.</li><li>whos: list all the variables in details.</li><li>max(A(:)): take the maximum element of matrix A.</li><li>disp(“…”): print a string.</li></ul><p>Function:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">function y = squareThisNumber(x)</span><br><span class="line">y = x^2;</span><br></pre></td></tr></table></figure><ul><li>addpath(): add search path.</li></ul><p>Vectorization</p><hr><p>$$<br>h_\theta(x)=\sum_{j=0}^n\theta_jx_j=\theta^Tx<br>$$</p><p>$$<br>\theta =<br>\begin {bmatrix}<br>\theta_0 \\<br>\theta_1 \\<br>\theta_2<br>\end {bmatrix}<br>\<br>x =<br>\begin {bmatrix}<br>x_0 \\<br>x_1 \\<br>x_2<br>\end {bmatrix}<br>$$</p><p>Unvectorized implementation:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">prediction = 0.0;</span><br><span class="line">for j = 1:n+1</span><br><span class="line">prediction = prediction + theta(j) * x(j)</span><br><span class="line">end;</span><br></pre></td></tr></table></figure><p>Vectorized implementation:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prediction = theta&apos; * x;</span><br></pre></td></tr></table></figure><h2 id="Programming-Assignment-Linear-Regression"><a href="#Programming-Assignment-Linear-Regression" class="headerlink" title="Programming Assignment: Linear Regression"></a>Programming Assignment: Linear Regression</h2><p><a href="https://github.com/Aden-Q/Machine-Learning/tree/master/Week2" target="_blank" rel="noopener">Linear Regression</a></p><h1 id="Week-3"><a href="#Week-3" class="headerlink" title="Week 3"></a>Week 3</h1><h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><p><strong>Classification</strong></p><ul><li>Email: Spam / Not Spam?</li><li>Online Transactions: Fraudulent (Yes/ No)?</li><li>Tumor: Malignant / Benign?</li></ul><p>$$<br>y \in {0,1}<br>$$</p><p>0: “Negative Class”</p><p>1: “Positive Class”</p><p>Binary Classification by Linear Regression is easily affected by extreme value.</p><p><strong>Logistic Regression:</strong><br>$$<br>0 \leq h_\theta(x) \leq 1<br>$$<br>Hypothesis:<br>$$<br>h_\theta(x) = g(\theta^Tx)<br>\\<br>g(z) = \frac{1}{1+e^{-z}}<br>$$<br>g is called <strong>sigmoid function</strong> or <strong>logistic function</strong>.<br>$$<br>h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}<br>$$<br><strong>Interpretation of Hypothesis Output</strong></p><p>$h_\theta(x)$ = estimated probability that y = 1 on input x<br>$$<br>h_\theta(x)=P(y=1|x;\theta)<br>$$<br>“Probability that y = 1, given x, parameteruzed by $\theta $”<br>$$<br>P(y=0|x;\theta)+P(y=1|x;\theta)=1<br>$$<br><strong>Decision Boundary</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-05-Week3-1.png" style="zoom:60%"><br></center><p><strong>Cost Function</strong></p><p>If we use the same cost function as before, it would be non-convex. Thus it’s not a wise choice. So we tend to choose a convex function.</p><p>Logistic regression cost function:<br>$$<br>Cost(h_\theta(x),y)=<br>\begin{equation}<br>\left{<br>             \begin{array}{lr}<br>             -log(h_\theta(x)), &amp; if \space y =1  \\<br>             -log(1-h_\theta(x)), &amp;  if \space y = 0<br>             \end{array}<br>\right.<br>\end{equation}<br>$$</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-05-Week3-2.png" style="zoom:60%"><br></center><p>$$<br>Cost(h_\theta(x),y)=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))<br>$$</p><p>$$<br>J(\theta)=-\frac1m[\sum_{i=1}^my^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]<br>$$</p><p><strong>Gradient descent</strong><br>$$<br>\theta_j:=\theta_j - \alpha\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}<br>$$<br>It looks identical to linear regression! (only hypothesis changes)</p><p><strong>Advanced optimization</strong></p><ul><li>Conjugate gradient</li><li>BFGS</li><li>L-BFGS</li></ul><p>Advantages:</p><ul><li>No need to manually pick $\alpha $</li><li>Often faster than gradient descent</li></ul><p>Disadvantages:</p><ul><li>More complex</li></ul><p><strong>Multiclass classification</strong></p><p>Email foldering/tagging: Work, Friends, Family, Hobby</p><p>Medical diagrams: Nol ill, Cold, Flu</p><p>Weather: Sunny, Cloudy, Rain, Snow</p><p><strong>One-vs-all (one-vs-rest):</strong></p><p>Treat one multiple class problem as several binary classification problems.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-06-Week3-3.png" style="zoom:60%"><br></center><p>Train a logistic regression classifier $h_\theta^{(i)}(x)$ for each class $i$ to predict the probability that $y=i$.</p><p>On a new input $x$, to make a prediction, pick the class $i$ that maximizes<br>$$<br>max\space h_\theta^{(i)}(x)<br>$$<br>Which means the prediction takes the maximum probability the sample belongs to.</p><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-06-Week3-4.png" style="zoom:60%"><br></center><p>Underfit: High bias</p><p>Overfit: High variance</p><p><strong>Overfitting:</strong> If we have too many features, the learned hypothesis may fit the training set very well, but fail to generalize to new examples (predict prices on new examples).</p><p><strong>Addressing overfitting:</strong></p><ul><li>Reduce number of features<ul><li>Manually select which features to keep.</li><li>Model selection algorithm.</li></ul></li><li>Regularization<ul><li>Keep all the features, but reduce magnitude/values of parameters $\theta_j$.</li><li>Works well when we have a lot of features, each of which contributes a bit to predicting $y$.</li></ul></li></ul><p><strong>Regularization</strong></p><p>Small values for parameters $\theta_0,\theta_1,…,\theta_n$</p><ul><li>Simpler hypothesis</li><li>Less prone to overfitting</li></ul><p>$$<br>J(\theta)=\frac1{2m}[\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\lambda \sum_{i=1}^m\theta_j^2]<br>$$</p><p>$\lambda$: regularization parameter, control the trade off between fitting the training set well and keeping parameters small (keeping hypothesis simple)</p><p>If $\lambda $ is extremely large, then $h_\theta(x)=\theta_0$. (end up with the simplest hypothesis and high bias)</p><p><strong>Gradient descent with regularization</strong><br>$$<br>\theta_0:=\theta_0-\alpha \frac1m<br>\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}<br>$$</p><p>$$<br>\theta_j:=\theta_j-\alpha [\frac1m<br>\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac\lambda m\theta_j], \space for \space j = 1, 2, …, n<br>$$</p><p>Equivalent:<br>$$<br>\theta_j:=\theta_j(1-\alpha\frac\lambda m)-\alpha \frac1m<br>\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}<br>$$</p><p><strong>Normal equation with normalization</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-06-Week3-5.png" style="zoom:60%"><br></center><p><strong>Regularized logistic regression</strong></p><p>Cost function:<br>$$<br>J(\theta) = -[\frac1m \sum_{i=1}^my^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]+<br>\frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2<br>$$</p><p><strong>Regularized advanced optimization</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-06-Week3-6.png" style="zoom:60%"><br></center><h2 id="Programming-Assignment-Logistic-Regression"><a href="#Programming-Assignment-Logistic-Regression" class="headerlink" title="Programming Assignment: Logistic Regression"></a>Programming Assignment: Logistic Regression</h2><p><a href="https://github.com/Aden-Q/Machine-Learning/tree/master/Week3" target="_blank" rel="noopener">Logistic Regression</a></p><p>One way to fit the data better is to create more features from each data point.</p><p>Logistic classifier prones to be overfitting  on higher-dimension features. (consider the decision boundary)</p><h1 id="Week-4"><a href="#Week-4" class="headerlink" title="Week 4"></a>Week 4</h1><h2 id="Neural-Networks-Representation"><a href="#Neural-Networks-Representation" class="headerlink" title="Neural Networks: Representation"></a>Neural Networks: Representation</h2><p>Linear regression and logistic regression are not designed for Non-linear Classification problems and multiple features.</p><p><strong>Neural Networks</strong></p><p>Origins: Algorithms that try to mimic the brain.</p><p>Was very widely used in 80s and early 90s; popularity diminished in late 90s.</p><p>Recent resurgence: State-of-the-art technique for many applications</p><p><strong>Neuron model: Logistic unit</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-08-Week4-1.png" style="zoom:60%"><br></center><p>Sigmoid (logistic) activation function.<br>$$<br>g(z) = \frac{1}{1+e^{-z}}<br>$$<br>$\theta $: “weights”</p><p><strong>Neural Network</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-08-Week4-2.png" style="zoom:60%"><br></center><p>$a_i^{(j)}$ = “activation” of unit $i$ in layer $j$</p><p>$\theta^{(j)}$ = matrix of weights controlling function mapping from layer $j$ to layer $j+1$</p><p>If network has $s_j$ units in layer $j$, $s_{j+1}$ units in layer $j+1$, then $\theta^{(j)}$ will be of dimension $s_{j+1}*(s_j+1)$. The extra dimension represents the bias.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-08-Week4-3.png" style="zoom:60%"><br></center><p><strong>Forward propagation: Vectorized implementation</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-08-Week4-4.png" style="zoom:60%"><br></center><p><strong>Neural Network learning its own features</strong></p><p>“Architectures” refers to how the different neurons are connected to each other.</p><p><strong>Handwritten digit classification</strong></p><p><strong>Multi-class Classification</strong></p><p>Multiple output units: One-vs-all.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-08-Week4-5.png" style="zoom:60%"><br></center><h2 id="Programming-Assignment-Multi-class-Classification-and-Neural-Networks"><a href="#Programming-Assignment-Multi-class-Classification-and-Neural-Networks" class="headerlink" title="Programming Assignment: Multi-class Classification and Neural Networks"></a>Programming Assignment: Multi-class Classification and Neural Networks</h2><p><a href>Multile-class Classification and Neural Networks</a></p><h1 id="Week-5"><a href="#Week-5" class="headerlink" title="Week 5"></a>Week 5</h1><h2 id="Neural-Networks-Learning"><a href="#Neural-Networks-Learning" class="headerlink" title="Neural Networks: Learning"></a>Neural Networks: Learning</h2><p>$L$ = total no. of layers in network</p><p>$s_l$ = no. of units (not counting bias unit) in layer $l$</p><p><strong>Binary classification:</strong> $y$ = 0 or 1, 1 output unit</p><p><strong>Multi-class classification (K classes):</strong> $y \in R^K$</p><p><strong>Cost function</strong></p><p><strong>Gradient computation</strong></p><p>Given one training example $(x,y)$:</p><p>Forward propagation:<br>$$<br>a^{(1)} = x \\<br>z^{(2)}=\theta^{(1)}a^{(1)} \\<br>a^{(2)}=g(z^{(2)}) \space (add \space a_0^{(2)}) \\<br>z^{(3)}=\theta^{(2)}a^{(2)} \\<br>a^{(3)}=g(z^{(3)}) \space (add \space a_0^{(3)}) \\<br>z^{(4)}=\theta^{(3)}a^{(3)} \\<br>a^{(4)}=h_\theta(x)=g(z^{(4)})<br>$$<br><strong>Backpropagation algorithm</strong></p><p>Intuition: $\delta_j^{(l)}$ = “error” of node $j$ in layer $l$.<br>$$<br>\delta^{(4)} = a^{(4)}-y<br>$$</p><p>$$<br>\delta^{(3)}=(\theta^{(3)})^T\delta^{(4)}.*g’(z^{(3)})<br>$$</p><p>$$<br>\delta^{(2)}=(\theta^{(2)})^T\delta^{(3)}.*g’(z^{(2)})<br>$$</p><p>$$<br>\frac{\partial}{\partial\theta_{ij}^{(l)}}J(\theta) = a_j^{(l)}\delta_i^{(l+1)}<br>$$<br>Training set ${   (x^{(1)},y^{(1)}),…, (x^{(m)},y^{(m)})}$</p><p>Set $\Delta_{ij}^{(l)}=0 $ for all $l,i,j$.</p><p>For $i$ = 1 to $m$</p><p>​    Set $a^{(1)}=x^{(i)}$</p><p>​    Perform forward propagation to compute $a^{(l)}$ for $l$ = 2,3,…,L</p><p>​    Using $y^{(i)}$, compute $\delta^{(L)}=a^{(L)}-y^{(i)}$</p><p>​    Compute $\delta^{(L-1)},\delta^{(L-2)},…,\delta^{(2)}$    // backpropagation step</p><p>​    $\Delta_{ij}^{(l)}:=\Delta_{ij}^{(l)}+a_j^{(l)}\delta_i^{(l+1)}$<br>$$<br>D_{ij}^{(l)}:=\frac1m\Delta_{ij}^{(l)}+\lambda\theta_{ij}^{(l)} \space if \space j \neq0 \\<br>D_{ij}^{(l)}:=\frac1m\Delta_{ij}^{(l)} \space if \space j \ =0<br>$$</p><p><strong>Random initialization: Symmetry breaking</strong></p><p><strong>Training a neural network</strong></p><p>Pick a network architecture (connectivity pattern between neurons)</p><p>No. of input units: Dimension of features $x^{(i)}$</p><p>No. output units: Number of classes</p><p>Reasonable default: 1 hidden layer, or if &gt;1 hidden layer, have same no. of hidden units in every layer (usually the more the better)</p><ol><li>Randomly initialize weights</li><li>Implement forward propagation to get $h_\theta(x^{(i)})$ for any $x^{(i)}$</li><li>Implement code to compute cost function $J(\theta)$</li><li>Implement backprop to compute partial derivatives $\frac{\partial}{\partial\theta_{jk}^{(l)}}J(\theta)$</li><li>Use gradient checking to compare $\frac{\partial}{\partial\theta_{jk}^{(l)}}J(\theta)$ using backpropagation vs. using numerical estimate of gradient of $J(\theta)$, Then disable gradient checking code.</li><li>Use gradient descent or advanced optimization method with backpropagation to try to minimize $J(\theta)$ as a function of parameters $\theta$</li></ol><h2 id="Programming-Assignment-Neural-Network-Learning"><a href="#Programming-Assignment-Neural-Network-Learning" class="headerlink" title="Programming Assignment: Neural Network Learning"></a>Programming Assignment: Neural Network Learning</h2><p><a href="https://github.com/Aden-Q/Machine-Learning/tree/master/Week5" target="_blank" rel="noopener">Neural Network Learning</a></p><h1 id="Week-6"><a href="#Week-6" class="headerlink" title="Week 6"></a>Week 6</h1><h2 id="Advice-for-Applying-Machine-Learning"><a href="#Advice-for-Applying-Machine-Learning" class="headerlink" title="Advice for Applying Machine Learning"></a>Advice for Applying Machine Learning</h2><p><strong>Debuggin a learning algorithm:</strong></p><ul><li>Get more training examples</li><li>Try smaller sets of features</li><li>Try getting additional features</li><li>Try adding polynomial features $(x_1^2,x_2^2,x_1x_2,etc)$</li><li>Try decreasing $\lambda$</li><li>Try increasing $\lambda$</li></ul><p><strong>Evaluating your hypothesis</strong></p><p>Training set/ Test set split, typically 70% vs. 30% (remember to randomly shuffle)</p><ul><li>Learn parameter $\theta$ from training data (minimizing training error $J(\theta)$)</li><li>Compute test set error</li><li>Misclassification error (0/1 misclassification error)</li></ul><p><strong>Model selection</strong></p><p>Training set, cross validation set and test set split. Typically 60%, 20% and 20%</p><p>Select by validation set, and finally test on test set. Note that we should not select by test set. (Means that we fit an extra parameter to the validation set)</p><p><strong>Bias and variance</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-09-Week6-1.png" style="zoom:60%"><br></center><p><strong>Choosing the regularization parameter $\lambda$</strong></p><ol><li>Try $\lambda=0$</li><li>Try $\lambda=0.01$</li><li>Try $\lambda=0.02$</li><li>Try $\lambda=0.04$</li><li>…</li><li>Try $\lambda=10.24$</li></ol><p>Steps of choosing $\lambda $:</p><ol><li>Create a list of lambdas (i.e. λ∈{0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24});</li><li>Create a set of models with different degrees or any other variants.</li><li>Iterate through the \lambda<em>λ</em>s and for each \lambda<em>λ</em> go through all the models to learn some Θ.</li><li>Compute the cross validation error using the learned Θ (computed with λ) on the JCV(Θ) <strong>without</strong> regularization or λ = 0.</li><li>Select the best combo that produces the lowest error on the cross validation set.</li><li>Using the best combo Θ and λ, apply it on Jtest(Θ) to see if it has a good generalization of the problem.</li></ol><p><strong>Learning curves</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-09-Week6-2.png" style="zoom:60%"><br></center><p><strong>High bias</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-09-Week6-3.png" style="zoom:60%"><br></center><p>If a learning algorithm is suffering from high bias, getting more training data will not (by itself) help much.</p><p><strong>High variance</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-09-Week6-4.png" style="zoom:60%"><br></center><p>If a learning algorithm is suffering from high variance, getting more training data is likely to help.</p><p>“Small” neural network (fewer parameters; more prone to underfitting): Computationally cheaper</p><p>“Large” neural network (more parameters; more prone to overfitting): Computationally more expensive</p><h2 id="Programming-Assignment-Regularized-Linear-Regression-and-Bias-Variance"><a href="#Programming-Assignment-Regularized-Linear-Regression-and-Bias-Variance" class="headerlink" title="Programming Assignment: Regularized Linear Regression and Bias/Variance"></a>Programming Assignment: Regularized Linear Regression and Bias/Variance</h2><p><a href="https://github.com/Aden-Q/Machine-Learning/tree/master/Week6" target="_blank" rel="noopener">Regularized Linear Regression and Bias/Variance</a></p><h2 id="Machine-Learning-System-Design"><a href="#Machine-Learning-System-Design" class="headerlink" title="Machine Learning System Design"></a>Machine Learning System Design</h2><p><strong>Building a spam classifier</strong></p><p>Supervised learning. $x$ = features of emails. $y$ = spam (1) or not spam (0).</p><p>Feature $x$: Choose 100 words indicative of spam/not spam.</p><p>Note: In practice, take most frequently occurring $n$ words (10,000 to 50,000) in training set, rather than manually pick 100 words.</p><p>How to improve model performance?</p><ul><li>Collect lots of data</li><li>Develop sophisticated features based on email routing information (from email header).</li><li>Develop sophisticated features for message body, e.g. should “discount” and “discounts” be treated as the same word? How about “deal” and “Dealer”? Features about punctuation?</li><li>Develop sophisticated algorithm to detect misspellings (e.g. m0rtgage, med1cine, w4tches.)</li></ul><p><strong>Error Analysis</strong></p><ul><li>Start with a simple algorithm that you can implement quickly. Implement it and test it on your cross-validation data.</li><li>Plot learning curves to decide if more data, more features, etc. are likely to help.</li><li>Error analysis: Manually examine the examples (in cross validation set) that your algorithm made errors on. See if you spot any systematic trend in what type of examples it is making errors on.</li></ul><p><strong>Precision/Recall</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-09-Week6-6.png" style="zoom:60%"><br></center><p>We prefer high <strong>Precision</strong> and high <strong>Recall</strong>.</p><p><strong>Trade off precision and recall</strong></p><p><strong>$F_1$ Score (F score)</strong><br>$$<br>2\frac{PR}{P+R}<br>$$</p><h1 id="Week-7"><a href="#Week-7" class="headerlink" title="Week 7"></a>Week 7</h1><h2 id="Support-Vector-Machines"><a href="#Support-Vector-Machines" class="headerlink" title="Support Vector Machines"></a>Support Vector Machines</h2><p>Cost:<br>$$<br>minC<br>\sum_{i=1}^m[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+<br>\frac{1}{2}\sum_{i=0}^n\theta_j^2<br>$$<br>Trade-off<br>$$<br>CA+B<br>$$<br>Hypothesis<br>$$<br>h_\theta(x)=<br>\begin{equation}<br>\left{<br>             \begin{array}{lr}<br>             1, &amp; if \space \theta^Tx \geq0  \\<br>             0, &amp;  otherwise<br>             \end{array}<br>\right.<br>\end{equation}<br>$$</p><p><strong>SVM Decision Boundary: Linearly separable case</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-10-Week7-1.png" style="zoom:60%"><br></center><p><strong>Non-linear Decision Boundary</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-10-Week7-2.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-10-Week7-3.png" style="zoom:60%"><br></center><p>Each landmarks define a new feature thought all samples.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-10-Week7-4.png" style="zoom:60%"><br></center><p>Some intuition of the kernel function</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-10-Week7-5.png" style="zoom:60%"><br></center><p><strong>Choosing the landmarks</strong></p><p>Exactly the location as the training examples.</p><p>Given $(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})$.</p><p>choose $l^{(1)}=x^{(1)},l^{(2)}=x^{(2)},…,l^{(m)}=x^{(m)}$.</p><p>Given example $x$:<br>$$<br>f_1 = similarity(x,l^{(1)}) \\<br>f_2 = similarity(x,l^{(2)}) \\<br>…<br>$$</p><p>$$<br>f =<br>\begin {bmatrix}<br>f_0 \\<br>f_1 \\<br>f_2 \\<br>… \\<br>f_m<br>\end {bmatrix}<br>$$</p><p>Hypothesis: Given $x$, compute features $f \in R^{m+1}$</p><p>Predict “y=1” if $\theta^Tf \geq 0$</p><p>Training:<br>$$<br>minC<br>\sum_{i=1}^m[y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)})]+<br>\frac{1}{2}\sum_{i=0}^n\theta_j^2<br>$$<br>C (=$\frac{1}{\lambda}$)</p><ul><li>Large C: Lower bias, high variance. (small $\lambda $)</li><li>Small C: Higher bias, lower variance. (Large $\lambda $)</li></ul><p>$\sigma^2$</p><ul><li>Large $\sigma^2$: Features $f_i$ vary more smoothly. High bias, lower variance.</li><li>Small $\sigma^2$: Features $f_i$ vary less smoothly. Lower bias, higher variance.</li></ul><p>Use SVM software package (e.g. liblinear, libsvm, …) to solve for parameters $\theta$.</p><p>Need to specify:</p><ul><li>Choice of parameter C.</li><li>Choice of kernel (similarity function): No kernel (“linear kernel”, apply for n large, m small), Gaussian kernel (Non-linear classifier, apply for n small, m large).</li></ul><p>Note: Not all similarity functions $similarity(x,l)$ make valid kernels. (Need to satisfy technical condition called “Mercer’s Theorem” to make sure SVM packages’ optimizations run correctly, and do not diverge).</p><p>Many off-the-shelf kernels available:</p><ul><li>Polynomial kernel: $k(x,l) = (x^Tl)^2, (x^Tl)^3,(x^Tl+1)^3, (x^Tl+constant)^{degree}$</li><li>More esoteric: String kernel, chi-square kernel, histogram intersection kernel, …</li></ul><p><strong>Multi-class classification</strong></p><p>Many SVM packages already have built-in multi-class classification functionality.</p><p>Otherwise, use one-vs.-all method. (Train $K$ SVMs, one to distinguish $y=i$ from the rest, for $i$ = 1,2,…,$K$), get $\theta^{(1)},\theta^{(2)},…,\theta^{(K)}$ Pick class $i$ with largest $(\theta^{(i)})^Tx$</p><p><strong>Logistic regression vs. SVMs</strong></p><p>If $n$ is large (relative to $m$): Use logistic regression, or SVM without a kernel (“linear kernel”)</p><p>If $n$ is small, $m$ is intermediate: Use SVM with Gaussian kernel</p><p>If $n$ is small, $m$ is large: Create/add more features, then use logistic regression or SVM without a kernel</p><p>Neural network likely to work well for most of these settings, but may be slower to train.</p><h2 id="Programming-Assignment-Support-Vector-Machines"><a href="#Programming-Assignment-Support-Vector-Machines" class="headerlink" title="Programming Assignment: Support Vector Machines"></a>Programming Assignment: Support Vector Machines</h2><p><a href="https://github.com/Aden-Q/Machine-Learning/tree/master/Week7" target="_blank" rel="noopener">Support Vector Machines</a></p><h1 id="Week-8"><a href="#Week-8" class="headerlink" title="Week 8"></a>Week 8</h1><h2 id="Unsupervised-Learning"><a href="#Unsupervised-Learning" class="headerlink" title="Unsupervised Learning"></a>Unsupervised Learning</h2><p>Training set: ${  x^{(1)},x^{(2)},x^{(3)},…,x^{(m)} }$</p><p><strong>Applications of clustering</strong></p><ul><li>Market segmentation</li><li>Social network analysis</li><li>Organize computing clusters</li><li>Astronomical data analysis</li></ul><p><strong>K-means algorithm</strong></p><p>cluster centroids</p><p>Input:</p><ul><li>$K$ (number of clusters)</li><li>Training set ${x^{(1)},x^{(2)},…,x^{(m)}}$</li></ul><p>$x^{(i)}\in R^n$ (drop $=x_0=1$ convention )</p><p>Randomly initialize $K$ cluster centroids $\mu_1,\mu_2,…,\mu_K \in R^n$</p><p>Repeat{</p><p>​    for i = 1 to m</p><p>​        $c^{(i)}$ := index (from 1 to $K$) of cluster centroid closest to $x^{(i)}$</p><p>​    for k = 1 to $K$</p><p>​        $\mu_k$ := average (mean) of points assigned to cluster $k$</p><p>}</p><p><strong>K-means optimization objective</strong></p><p>$c^{(i)}$ = index of cluster (1,2,…,$K$) to which example $x^{(i)}$ is currently assigned</p><p>$\mu_k$ = cluster centroid $k$ ($\mu_k \in R^n$)</p><p>$\mu_{c^{(i)}}$ = cluster centroid of cluster to which example $x^{(i)}$ has been assigned</p><p>Optimization objective:<br>$$<br>J = \frac1m\sum_{i=1}^{m}||x^{(i)}-\mu_{c^{(i)}}||^2<br>$$<br>Distortion cost function</p><p><strong>Random initialization</strong></p><p>Should have $K &lt; m$</p><p>Randomly pick $K$ training examples.</p><p>Set $\mu_1 ,…, \mu_K$ equal to these $K$ examples.</p><p><strong>Choosing the value of K</strong></p><p>Elbow method.</p><h2 id="Dimensionality-Reduction"><a href="#Dimensionality-Reduction" class="headerlink" title="Dimensionality Reduction"></a>Dimensionality Reduction</h2><p><strong>Data Compression</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week8-1.png" style="zoom:60%"><br></center><p><strong>Data Visulization</strong></p><p>Prefer 2-d or 3d data.</p><p><strong>Principal Component Analysis (PCA) problem formulation</strong></p><p>Reduce from 2-dimension to 1-dimension: Find a direction (a vector $u^{(1)}\in R^n$) onto which to project the data so as to minimize the projection error.</p><p>Reduce from n-dimension to k-dimension: Find $k$ vectors $u^{(1)},u^{(2)},…,u^{(k)}$ onto which to project the data, so as to minimize the projection error.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week8-2.png" style="zoom:60%"><br></center><p><strong>PCA is not linear regression</strong></p><p>Left: linear regression</p><p>Right: PCA</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week8-3.png" style="zoom:60%"><br></center><p><strong>Principal Component Analysis algorithm</strong></p><p>Data preprocessing:</p><p>Training set: $x^{(1)},x^{(2)},…,x^{(m)}$</p><p>Preprocessing (feature scaling/mean normalization)<br>$$<br>\mu_j=\frac1m\sum_{i=1}^mx_j^{(i)}<br>$$<br>Replace each $x_j^{(i)}$ with $x_j-\mu_j$.</p><p>If different features on different scales, scale features to have comparable range of values.<br>$$<br>x_j^{(i)}=\frac{x_j^{(i)}-\mu_j}{s_j}<br>$$<br>Algorithm steps:</p><ol><li><p>Mean normalization and feature scaling.</p></li><li><p>Compute “covariance matrix”:</p></li></ol><p>$$<br>\Sigma = \frac{1}{m}\sum_{i=1}^n(x^{(i)})(x^{(i)})^T<br>$$</p><ol start="3"><li>Compute “eigenvectors” of matrix $\Sigma$:</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[U,S,V] = svd(Sigma);</span><br></pre></td></tr></table></figure><p>Sigma is a $n\times n$ matrix.</p><p>U is also a $n \times n$ matrix, with each column represents a eigenvector.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week8-4.png" style="zoom:60%"><br></center><ol start="4"><li><p>Ureduce = U(:, 1:k)</p><p>z = Ureduce’ * x;</p></li></ol><p><strong>Reconstruction from compressed representation</strong><br>$$<br>z = U^T_{reduce}x<br>$$</p><p>$$<br>x_{approx} = U_{reduce}*z<br>$$</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week8-5.png" style="zoom:60%"><br></center><p><strong>Choosing $k$ (number of principal components)</strong></p><p>Average squared projection error: $\frac1m\sum_{i=1}^m||x^{(i)}-x_{approx}^{(i)}||^2$</p><p>Total variation in the data: $\frac1m\sum_{i=1}^m||x^{(i)}||^2$</p><p>Typically, choose $k$ to be smallest value so that<br>$$<br>\frac{Average \space error}{Variation} \leq0.01<br>$$<br>“99%” of variance is retained</p><p><strong>Application of PCA</strong></p><ul><li>Compression<ul><li>Reduce memory/disk needed to store data</li><li>Speed up learning algorithm</li></ul></li><li>Visualization</li></ul><p>Before implementing PCA, first try running whaever you want to do with the original/raw data. Only if that doesn’t do what you want, then implement PCA.</p><h2 id="Programming-Assignment-K-Means-Clustering-and-PCA"><a href="#Programming-Assignment-K-Means-Clustering-and-PCA" class="headerlink" title="Programming Assignment: K-Means Clustering and PCA"></a>Programming Assignment: K-Means Clustering and PCA</h2><p><a href="https://github.com/Aden-Q/Machine-Learning/tree/master/Week8" target="_blank" rel="noopener">K-Means Clustering and PCA</a></p><h1 id="Week-9"><a href="#Week-9" class="headerlink" title="Week 9"></a>Week 9</h1><h2 id="Anomaly-Detection"><a href="#Anomaly-Detection" class="headerlink" title="Anomaly Detection"></a>Anomaly Detection</h2><p><strong>Anomaly detection example</strong></p><p>Dataset: ${ x^{(1)},x^{(2)},…,x^{(m)} }$</p><p>Is $x_{test}$ anomalous?</p><p>$p(x_{test}) &lt; \epsilon$ -&gt; flag anomaly</p><p>$p(x_{test}) \geq \epsilon$ -&gt; OK</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-1.png" style="zoom:60%"><br></center><p>Fraud detection (Online website):</p><p>$x^{(i)}$ = features of user $i$’s activities</p><p>Model $p(x)$ from data.</p><p>Identify unusual users by checking which have $p(x) &lt; \epsilon$</p><p>Manufacturing</p><p>Monitoring computers in a data center.</p><p>$x^{(i)}$ = features of machine $i$</p><p>$x_1$ = memory use, $x_2$ = number of disk accesses/sec,</p><p>$x_3$ = CPU load, $x_4$ = CPU load/network traffic.</p><p>…</p><p><strong>Gaussian (Normal) distribution</strong></p><p>N ~ N($\mu$, $\sigma ^2$)</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-2.png" style="zoom:60%"><br></center><p>$$<br>p(x;\mu,\sigma ^2) = \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2})<br>$$</p><p><strong>Parameter estimation</strong><br>$$<br>\mu = \frac1m\sum_{i=1}^mx^{(i)} \\<br>\sigma^2 = \frac1m\sum_{i=1}^m(x^{(i)}-\mu)^2<br>$$</p><p><strong>Density estimation</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-4.png" style="zoom:60%"><br></center><p>Data set split:</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-5.png" style="zoom:60%"><br></center><p>Possible evaluation metrics:</p><ul><li>True positive, false positive, false negative, true negative</li><li>Precision/Recall</li><li>$F_1$-score</li></ul><p>Can also use cross validation set to choose parameter $\epsilon$</p><p><strong>Anomaly detection vs. Supervised learning</strong></p><p>Anomaly detection:</p><ul><li>Very small number of positive examples (y = 1). (0-20 is common).</li><li>Large number of negative (y = 0) examples.</li><li>Many different “tyles” of anomalies. Hard for any algorithm to learn from positive examples what the anomalies may look like; future anomalies may look nothing like any of the anomalous examples we’ve seen so far.</li><li>Fraud detection</li><li>Manufacturing (e.g. aircraft engines)</li><li>Monitoring machines in a data center</li></ul><p>Supervised learning:</p><ul><li>Large number of positive and negative examples.</li><li>Enough positive examples for algorithm to get sense of what positive examples are like, future positive examples likely to be similar to ones in training set.</li><li>Email spam classification</li><li>Weather prediction (sunny/rainy/etc).</li><li>Cancer classification</li></ul><p><strong>Feature selection and transformation</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-6.png" style="zoom:60%"><br></center><p><strong>Error analysis for anomaly detection</strong></p><p>Want $p(x)$ large for normal examples $x$.</p><p>​      $p(x)$ small for anomalous examples $x$.</p><p>Most common problem:</p><p>​    $p(x)$ is comparable (say, both large) for normal and anomalous examples</p><p>Choosing features that might take on unusually large or small values in the event of an anomaly.</p><p><strong>Multivariate Gaussian (Normal) distribution</strong></p><p>$x\in R^n$. Don’t model $p(x_1),p(x_2),…,$ etc. separately. Model $p(x)$ all in one go.</p><p>Parameters: $\mu \in R^n$, $\Sigma \in R^{n\times n}$ (convariance matrix)<br>$$<br>p(x;\mu,\Sigma) = \frac{1}{(2\pi)^\frac n2 |\Sigma|^{\frac12}}exp(-\frac12(x-\mu)^T\Sigma^{-1}(x-\mu))<br>$$</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-7.png" style="zoom:60%"><br></center><p><strong>Original model vs. Multivariate Gaussian</strong></p><p>Original: Manually create features to capture anomalies where $x_1,x_2$ take unusual combinations of values.</p><p>Multivariate: Automatically captures correlations between features. Have many constrains.</p><h2 id="Recommender-Systems"><a href="#Recommender-Systems" class="headerlink" title="Recommender Systems"></a>Recommender Systems</h2><p><strong>Content-based recommendations</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-8.png" style="zoom:60%"><br></center><p>For each user $j$, learn a parameter $\theta^{(j)}\in R^3$. Predict user $j$ as rating movie $i$ with $(\theta^{(j)})^Tx^{(i)} $ stars.</p><p>Problem formulation:</p><p>$r(i,j)=1$ if user $j$ has rated movie $i$ (0 otherwise)</p><p>$y^{(i,j)}$ = rating by user $j$ on movie $i$ (if defined)</p><p>$\theta^{(j)}$ = parameter vector for user $j$</p><p>$x^{(i)}$ = feature vector for movie $i$</p><p>For user $j$, movie $i$, predicted rating: $(\theta^{(j)})^Tx^{(i)} $</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-9.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-10.png" style="zoom:60%"><br></center><p><strong>Collaborative filtering</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-11.png" style="zoom:60%"><br></center><p><strong>Algorithm</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-12.png" style="zoom:60%"><br></center><p><strong>Low Rank Vectorization</strong></p><p><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-13.png" alt></p><p><strong>Finding related movies</strong><br>$$<br>small \space ||x^{(i)}-x^{(j)}|| -&gt; movie \space j \space and \space i \space are \space “similar”<br>$$</p><p><strong>Mean Normalization</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-11-Week9-14.png" style="zoom:60%"><br></center><h2 id="Programming-Assignment-Anomaly-Detection-and-Recommender-Systems"><a href="#Programming-Assignment-Anomaly-Detection-and-Recommender-Systems" class="headerlink" title="Programming Assignment: Anomaly Detection and Recommender Systems"></a>Programming Assignment: Anomaly Detection and Recommender Systems</h2><p><a href="https://github.com/Aden-Q/Machine-Learning/tree/master/Week9" target="_blank" rel="noopener">Anomaly Detection and Recommender Systems</a></p><h1 id="Week-10"><a href="#Week-10" class="headerlink" title="Week 10"></a>Week 10</h1><h2 id="Large-Scale-Machine-Learning"><a href="#Large-Scale-Machine-Learning" class="headerlink" title="Large Scale Machine Learning"></a>Large Scale Machine Learning</h2><blockquote><p>  It’s not who has the best algorithm that wins. It’s who has the most data.</p></blockquote><p><strong>Stochatic gradient descent</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week10-1.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week10-2.png" style="zoom:60%"><br></center><p>Out loop typically 1-10 times.</p><p><strong>Mini-batch gradient descent</strong></p><p>Batch gradient descent: Use all $m$ examples in each iteration</p><p>Stochastic gradient descent: Use 1 example in each iteration</p><p>Mini-batch gradient descent: Use $b$ examples in each iteration</p><p>b = mini-batch size</p><p>b = 2~100</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week10-3.png" style="zoom:60%"><br></center><p>With vectorization, Mini-batch outperforms Stochastic as regard to computation efficiency.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week10-4.png" style="zoom:60%"><br></center><p>Learning rate decay:<br>$$<br>\alpha = \frac{const1}{iterationNumber+const2}<br>$$</p><p><strong>Online learning</strong></p><p><strong>Map-reduce and data parallelism</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week10-5.png" style="zoom:60%"><br></center><p>Many learning algorithms can be expressed as computing sums of functions over the training set.</p><h1 id="Week-11"><a href="#Week-11" class="headerlink" title="Week 11"></a>Week 11</h1><h2 id="Application-Example-Photo-OCR"><a href="#Application-Example-Photo-OCR" class="headerlink" title="Application Example: Photo OCR"></a>Application Example: Photo OCR</h2><p>Photo Optical Character Recognition</p><p><strong>Photo OCR pipeline</strong></p><ol><li>Text detection</li><li>Character segmentation</li><li>Character classification</li></ol><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week11-1.png" style="zoom:60%"><br></center><p><strong>Sliding window detection</strong></p><p>step-size / stride</p><p>These cencepts are related to computer vision which is introduced in the deeplearninng specialization. I should take notes for these in another course/specialization.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week11-2.png" style="zoom:60%"><br></center><p><strong>Artificial data systhesizing</strong></p><p>Make sure you have a low bias classifier before expanding the effor. (Plot learning curves). E.g. keep increasing the number  of features/number of hidden units in neural network until you have a low bias classifier.</p><p>“How much work would it be to get 10x as much data as we currently have?”</p><ul><li>Artificial data synthesis</li><li>Collect/label it yourself</li><li>“Crowd source” (E.g. Amazon Mechanical Turk)</li></ul><p><strong>Ceiling analysis</strong></p><p>Estimating the errors due to each component.</p><p>What part of the pipeline should you spend the most time trying to improve?</p><p>E.g.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week11-3.png" style="zoom:60%"><br></center><p>It is really important to do ceiling analysis.</p><p><strong>Summary</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-12-Week11-4.png" style="zoom:60%"><br></center>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Stanford University, Machine Learning&lt;/p&gt;
&lt;p&gt;Course Notes&lt;/p&gt;
    
    </summary>
    
      <category term="Note" scheme="http://yoursite.com/categories/Note/"/>
    
    
      <category term="Programming" scheme="http://yoursite.com/tags/Programming/"/>
    
      <category term="Machine Learning" scheme="http://yoursite.com/tags/Machine-Learning/"/>
    
      <category term="Matlab" scheme="http://yoursite.com/tags/Matlab/"/>
    
  </entry>
  
  <entry>
    <title>Robotics: Perception</title>
    <link href="http://yoursite.com/2019/03/26/Robotics-Perception/"/>
    <id>http://yoursite.com/2019/03/26/Robotics-Perception/</id>
    <published>2019-03-27T00:30:34.000Z</published>
    <updated>2019-05-01T23:01:25.519Z</updated>
    
    <content type="html"><![CDATA[<p>University of Pennsylvania, Robotics: Perception</p><p>Course Notes</p><a id="more"></a><p>Cover:</p><ul><li>Geometry of Image Formation</li><li>Projective Transformations</li><li>Pose Estimation</li><li>Multi-View Geometry</li></ul><p>Learning Purpose: </p><ul><li>To be a candidate.</li><li>Self promotion.</li><li>Be familiar with Matlab again.</li></ul><p>Matlab help Videos are all from <a href="https://www.youtube.com/" target="_blank" rel="noopener">Youtube</a></p><h1 id="Week-1-Geometry-of-Image-Formation"><a href="#Week-1-Geometry-of-Image-Formation" class="headerlink" title="Week 1, Geometry of Image Formation"></a>Week 1, Geometry of Image Formation</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li>Projection center</li><li>Augmented reality</li><li>Where am I?</li></ul><h2 id="Camera-Modeling"><a href="#Camera-Modeling" class="headerlink" title="Camera Modeling"></a>Camera Modeling</h2><p>Gannet: Estimate distance from water.</p><p>Camera: imaging chip and a lens</p><center><br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g1ixautiffj31050ho118.jpg" style="zoom:60%"><br></center><p>$\frac 1f =  \frac 1a + \frac 1b$</p><p>blured picture (when we change the image plane by sticking on the mannual mode of the camera)</p><center><br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g1ixksq1fij30zh0hcdlk.jpg" style="zoom:60%"><br></center><ul><li>A point object of the same size coming closer results on a larger image.</li><li>A point moving on the same ray does not change its image. (regarding to fixed image plane)</li></ul><h2 id="Single-View-Geometry"><a href="#Single-View-Geometry" class="headerlink" title="Single View Geometry"></a>Single View Geometry</h2><p>World Coordinates: x, y, z locations.</p><p>Camera Coordinates: Two dimensional plane.</p><center><br><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g1jawb2m00j30rn0httuj.jpg" style="zoom:60%"><br></center><p>Given the picture, how tall is the person in the picture?</p><p>“Single View Metrology”, ICCV 1999, CVPR 1998</p><p>Ideas 1: Measurements on planes. Moving “not vertical” to “vertical” and then measure.</p><center><br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g1jazxsamij30v70frk63.jpg" style="zoom:60%"><br></center><p>Ideas 2: Vanishing points.</p><center><br><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g1jb2872qmj312n0cf7gm.jpg" style="zoom:60%"><br></center><p>Phical lines in the real world which are parallel will meet at the vanishing point when approaching infinity.</p><h2 id="More-on-Perspective-Projection"><a href="#More-on-Perspective-Projection" class="headerlink" title="More on Perspective Projection"></a>More on Perspective Projection</h2><p>Perspectography.</p><p>Strategy for projecting $P^*$ to $P$ on the same plane rather than on the image plane.</p><center><br><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g1jbmbnrxyj30rs0efwjf.jpg" style="zoom:60%"><br></center><h2 id="Quiz-Introduction"><a href="#Quiz-Introduction" class="headerlink" title="Quiz: Introduction"></a>Quiz: Introduction</h2><center><br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g1jbxs8z4tj30hg0xs0v7.jpg" style="zoom:60%"><br></center><h2 id="Glimpse-on-Vanishing-Points"><a href="#Glimpse-on-Vanishing-Points" class="headerlink" title="Glimpse on Vanishing Points"></a>Glimpse on Vanishing Points</h2><p>Vanishing points</p><hr><p>Properties:</p><ul><li>Any two parallel lines have the same vanishing point.</li><li>The ray from C throught v point is parallel to the lines.</li><li>An image may have more than one vanishing point.</li></ul><center><br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g1jcjkxk8zj30k109fwgc.jpg" style="zoom:60%"><br></center><p>A single point can form a line between the optical center and that point, the ray can represent all the physical lines out there which are parallel.</p><p>Multiple Vanishing Points:</p><p>For every direction we have on the ground plane, there is a unique point in the image space. All the vanishing points will form a horizon.</p><ul><li>Any set of parallel lines on the plane define a vanishing point.</li><li>The union of all of these vanishing points is the horizon line (also called vanishing line)</li><li>Different planes define different vanishing lines.</li></ul><p>Computing vanishing lines:</p><ul><li>I is intersection of horizontal plane through C with image plane.</li><li>Compute I from two sets of parallel lines on ground plane.</li><li>All points at same height as C project to I.</li><li>Provided way of comaring height of objects in the scene.</li></ul><center><br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g1jcu72jdpj30f809tdi5.jpg" style="zoom:60%"><br></center><p>Measuring height:</p><center><br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g1jcz65661j30vg0gatj6.jpg" style="zoom:60%"><br></center><p>Trace a line from the bottom of the ruler throught feet of the person to infinity (intersect with the horizon).</p><h2 id="Quiz-Vanishing-Points"><a href="#Quiz-Vanishing-Points" class="headerlink" title="Quiz: Vanishing Points"></a>Quiz: Vanishing Points</h2><center><br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g1jggbshiuj30au0ytds5.jpg" style="zoom:60%"><br></center><h2 id="Perspective-Projection-I"><a href="#Perspective-Projection-I" class="headerlink" title="Perspective Projection I"></a>Perspective Projection I</h2><p>One-Point Perspective</p><center><br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g1jgiybsamj30qf0jqnjg.jpg" style="zoom:60%"><br></center><p>Partial review.</p><p>Not all pictures should have the same vanishing points.</p><p>The projective plane:</p><ul><li>Represent points at infinity, homographies, perspective projection, multi-view relationships.</li><li>A point in the image is a ray in projective space.</li></ul><center><br><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g1jgvo6y0kj30cg07w0ue.jpg" style="zoom:60%"><br></center><p>$$<br>\begin{bmatrix}<br>x \\<br>y \\<br>\end{bmatrix}<br>-homogeneous coords-&gt;<br>\begin{bmatrix}<br>x \\<br>y \\<br>1 \\<br>\end{bmatrix}<br>$$<br>Projective lines</p><p>Eg:</p><center><br><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g1jhfkz7i1j30nx0h3e12.jpg" style="zoom:60%"><br></center><p>a, b and c are parameters in the 3D space but also related to the equation of the line.</p><p>3D plane equation:<br>$$<br>ax + by + cz = 0<br>$$</p><h2 id="Perspective-Projection-II"><a href="#Perspective-Projection-II" class="headerlink" title="Perspective Projection II"></a>Perspective Projection II</h2><p>Define a line: we use two points on the line to define it:</p><center><br><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g1jj068t8sj30sa0khgrx.jpg" style="zoom:60%"><br></center><p>We have:<br>$$<br>x\cdot l = 0  ,\space x’\cdot l = 0<br>$$<br>So $l$  is the surface normal vector of the orange plane, which is coresponding to the line on the plane $P_1P_2$ . (Please think that points on the line can be represented as $(x,y,1)$, then it’s easy to understand)</p><p>Reference materials of projective view:</p><p>surface normal: a unit vector at a given point of a surface which is perpendicular to the tangent plane.</p><p>Intersection of lines:</p><center><br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g1jjr4ft8pj30pk0m8jxa.jpg" style="zoom:60%"><br></center><p><strong>Mistake in this video at arround 8:30.</strong></p><h2 id="Point-Line-Duality"><a href="#Point-Line-Duality" class="headerlink" title="Point-Line Duality"></a>Point-Line Duality</h2><p>When P has the form $(x,y,0)$:</p><p>We convert a 3D point into a 2D point by dividing the last element out. (Homogeous coords)</p><p>Some intuitions: $x/0 -&gt; infinity$. So very likely it’s a vanishing point.</p><p>Point at infinity:</p><p>Eg:</p><center><br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g1jkecg0hyj30oq0fnq6u.jpg" style="zoom:60%"><br></center><p>More general of intersection of parallel lines:</p><center><br><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g1jkgb5rb1j30tr0kp799.jpg" style="zoom:60%"><br></center><p>All parallel lines intersects at the point at infinity:<br>$$<br>line \space l = (a,b,c)^T \space intersects \space at \space (b,-a,0)^T<br>$$</p><p>Line at infinity (a line passing all points at infinity) :<br>$$<br>l_\infty=(0,0,1)^T<br>$$<br>Because:<br>$$<br>\begin {bmatrix}<br>0\<br>0\<br>1<br>\end {bmatrix}<br>\begin {bmatrix}<br>x_1 \\<br>x_2 \\<br>0<br>\end {bmatrix}<br>= 0<br>$$</p><p>Ideal points and lines (noting that there are two coords system, one for real space which is 3D, one for image space which is 2D):</p><ul><li>Ideal point (“point at infinity”)<ul><li>$p = (x,y,0)$ — parallel to image plane</li><li>It has ifinite image coordinates (2D coordinates for point)</li></ul></li><li>Ideal line<ul><li>$l = (a,b,0)$ — parallel to image plane</li><li>Corresponds to a line in the image (finite coordinates)</li></ul></li></ul><h2 id="Quiz-Perspective-Projection"><a href="#Quiz-Perspective-Projection" class="headerlink" title="Quiz: Perspective Projection"></a>Quiz: Perspective Projection</h2><center><br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g1jmpa8tatj30k210tn07.jpg" style="zoom:60%"><br></center><h2 id="Rotations-and-Translations"><a href="#Rotations-and-Translations" class="headerlink" title="Rotations and Translations"></a>Rotations and Translations</h2><p>Transformation between camera and world coordinates systems</p><hr><p>Convention: Red for X, Green for Y and Blue for Z.<br>$$<br>^cP = ^cR_w \space^wP+^cT_w<br>$$<br>Point P can be expressed with respect to “w” or “c” coordinate frames.</p><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-29-Week1-18.png" style="zoom:60%"><br></center><p>If we set $^wP$ to zero, then, $^cP=^cR_w+^cT_w$ is the vector from camera origin to world origin.<br>$$<br>^cR_w=(r_1 \ r_2 \ r_3)<br>$$<br>Set $^wP=(1,0,0)$ and imagine $^cT_w=0$. Then $^cP = r_1 $ whch means that <strong>the rotation columns are the world axis expressed in the camera coordinate system</strong>.</p><p>Eg:</p><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-29-Week1-19.png" style="zoom:60%"><br></center><p>The translation is easy from the picture:<br>$$<br>\begin {pmatrix}<br>0 \\<br>5 \\<br>10<br>\end {pmatrix}<br>$$<br>We have to make sure that the 3x3 matrix is a rotation matrix, which means $R^TR=I$ and det(R) = 1.</p><p>Transform between coordinate systems of three:</p><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-29-Week1-20.png" style="zoom:60%"><br></center><p>Inverse:</p><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-29-Week1-21.png" style="zoom:60%"><br></center><h2 id="Quiz-Rotations-and-Translations"><a href="#Quiz-Rotations-and-Translations" class="headerlink" title="Quiz: Rotations and Translations"></a>Quiz: Rotations and Translations</h2><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-29-Week1-22.png" style="zoom:60%"><br></center><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-29-Week1-23.png" style="zoom:60%"><br></center><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-29-Week1-24.png" style="zoom:60%"><br></center><h2 id="Pinhole-Camera-Model"><a href="#Pinhole-Camera-Model" class="headerlink" title="Pinhole Camera Model"></a>Pinhole Camera Model</h2><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-25.png" style="zoom:60%"><br></center><p>Pinhole Camera:</p><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-26.png" style="zoom:60%"><br></center><p>$1^{st}$ Person Camera world</p><hr><p>3D to 2D image:<br>$$<br>x’=f\frac X Z \ y’=f\frac Y Z<br>$$</p><h2 id="Focal-Length-and-Dolly-Zoom-Effect"><a href="#Focal-Length-and-Dolly-Zoom-Effect" class="headerlink" title="Focal Length and Dolly Zoom Effect"></a>Focal Length and Dolly Zoom Effect</h2><p>Focal length</p><p>Process of changing focal length: zooming</p><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-27.png" style="zoom:60%"><br></center><p>Focal length longer: field of view narrow</p><h2 id="Intrinsic-Camera-a-Parameter"><a href="#Intrinsic-Camera-a-Parameter" class="headerlink" title="Intrinsic Camera a Parameter"></a>Intrinsic Camera a Parameter</h2><p>3D to 2D image:<br>$$<br>x’=f\frac X Z \ y’=f\frac Y Z<br>$$</p><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-28.png" style="zoom:60%"><br></center><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-29.png" style="zoom:60%"><br></center><p>$$<br>x=P_0X<br>$$</p><p>$P_0$ is the camera projection matrix</p><p>Conversion from mm to pixels</p><p>Optical center on the image plane: Principal point</p><p>What we do is shifting and scaling by the measurement of pixels:</p><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-30.png" style="zoom:60%"><br></center><p>Map camera coordinate to pixel coordinate (matrix form):</p><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-31.png" style="zoom:60%"><br></center><p>$Px,P_y$ is the principle point (where optical axis hits image plane) (not exactly the center of the image)</p><p>s is the slant factor, when the image plane is not normal to the optical axis</p><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-32.png" style="zoom:60%"><br></center><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-33.png" style="zoom:60%"><br></center><p>Three factors:</p><ul><li>A scale factor that converts physical focal length to pixel unit.</li><li>Position of image center (principal point).</li><li>A skew factor between x and y axis of the image.</li></ul><h2 id="3D-World-to-First-Person-Transformation"><a href="#3D-World-to-First-Person-Transformation" class="headerlink" title="3D World to First Person Transformation"></a>3D World to First Person Transformation</h2><p>Multiple View Geometry</p><p>$3^{rd}$ person view measurement</p><p>Why: When we are moving around to see different objects from different point of view, which means  that the origin is always changing.</p><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-34.png" style="zoom:60%"><br></center><p>Convert the 3D representation into first person coordinate.</p><p>Every camera has its own first person coordinate system.</p><ol><li>Translate the world coordinate into the camera coordiante</li><li>Translate the camera coordinate into the pixel coordinate</li></ol><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-35.png" style="zoom:60%"><br></center><p>Combining them together, we get:<br>$$<br>x=K[R,t]X<br>$$<br>Comlete form:</p><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-36.png" style="zoom:60%"><br></center><p>$$<br>3D \space from \space 3^{rd} \space person –&gt;3D \space from \space 1^{rd} \space person –&gt;2D \space pixel \space domain<br>$$</p><p>Special cases</p><hr><p>Plnar objects: all x,y,z sits in a plane</p><p>We have freedom to choose how we measure 3D points from $3^{rd}$ person view.</p><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-37.png" style="zoom:60%"><br></center><p>Rotating camera: known optical center</p><p>panorama</p><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-38.png" style="zoom:60%"><br></center><h2 id="Quiz-Dolly-Zoom"><a href="#Quiz-Dolly-Zoom" class="headerlink" title="Quiz: Dolly Zoom"></a>Quiz: Dolly Zoom</h2><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-14-Week1-3.png" style="zoom:60%"><br></center><h2 id="Quiz-Feeling-of-Camera-Motion"><a href="#Quiz-Feeling-of-Camera-Motion" class="headerlink" title="Quiz: Feeling of Camera Motion"></a>Quiz: Feeling of Camera Motion</h2><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-41.png" style="zoom:60%"><br></center><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-42.png" style="zoom:60%"><br></center><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-43.png" style="zoom:60%"><br></center><h2 id="How-to-Compute-Intrinsics-from-Vanishing-Points"><a href="#How-to-Compute-Intrinsics-from-Vanishing-Points" class="headerlink" title="How to Compute Intrinsics from Vanishing Points"></a>How to Compute Intrinsics from Vanishing Points</h2><p>Compute focal length and image center from pure geometry without using projection equations.</p><p>Eg: Three orthogonal sets of parallel lines create three orthogonal vanishing points.</p><center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-44.png" style="zoom:60%"><br></center><p>What does the horizon between A and B tells us about the camera?</p><p>It gives us information about how the horizon is oriented with respect to the camera.</p><p>Because the camera is tilted, point C is not on the horizon.</p><p>Just knowledge of A and B doesn’t determine C. We need also the focal length and the image center in order to fix C.</p><p>Later for how to compute:</p> <center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-45.png" style="zoom:60%"><br></center><p>H is the orthocenter of ABC</p><p>Focal length computation, easy as following:</p> <center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-46.png" style="zoom:60%"><br></center><p>So f can be determined from $d_1$, $d_2$ and $d_3$, which are all known.</p><h2 id="Quiz-How-to-Compute-Intrinsics-from-Vanishing-Points"><a href="#Quiz-How-to-Compute-Intrinsics-from-Vanishing-Points" class="headerlink" title="Quiz: How to Compute Intrinsics from Vanishing Points"></a>Quiz: How to Compute Intrinsics from Vanishing Points</h2> <center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-47.png" style="zoom:60%"><br></center><h2 id="Camera-Calibration"><a href="#Camera-Calibration" class="headerlink" title="Camera Calibration"></a>Camera Calibration</h2> <center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-49.png" style="zoom:60%"><br></center><p>Cameras with large field of view have radial distortions.</p> <center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-48.png" style="zoom:60%"><br></center><p>A procedure called calibration (Estimates the intrinsic parameters)</p><ul><li>f focal length</li><li>($u_0$,$v_0$) image center</li><li>$k_1,k_2,…$ radial distortion parameters</li></ul><p>Matlab has calibration toolbox.</p><h2 id="Quiz-Camera-Calibration"><a href="#Quiz-Camera-Calibration" class="headerlink" title="Quiz: Camera Calibration"></a>Quiz: Camera Calibration</h2> <center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-50.png" style="zoom:60%"><br></center> <center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-51.png" style="zoom:60%"><br></center> <center><br><img src="https://azure-pictures.s3.amazonaws.com/blog/2019-03-30-Week1-53.png" style="zoom:60%"><br></center><h2 id="Programming-Assignment-Dolly-Zoom"><a href="#Programming-Assignment-Dolly-Zoom" class="headerlink" title="Programming Assignment: Dolly Zoom"></a>Programming Assignment: Dolly Zoom</h2><p><a href="https://github.com/Aden-Q/Robotics-Perception/tree/master/Dolly-Zoom" target="_blank" rel="noopener">Dolly Zoom</a></p><h1 id="Week-2-Projective-Transformations"><a href="#Week-2-Projective-Transformations" class="headerlink" title="Week 2, Projective Transformations"></a>Week 2, Projective Transformations</h1><h2 id="Vanishing-Points-How-to-Compute-Camera-Orientation"><a href="#Vanishing-Points-How-to-Compute-Camera-Orientation" class="headerlink" title="Vanishing Points; How to Compute Camera Orientation"></a>Vanishing Points; How to Compute Camera Orientation</h2><p>$3^{rd}$ person perspective: World coordinate system</p><p>First person coordinate system: The person himself</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-02-Week2-1.png" style="zoom:60%"><br></center><p>How we orientate in the real world: $R$ and $t$</p><p>For $z_\infty=[0\space 0 \space 1 \space 0]^T$, $v_z=Kr_3$, K is the calibration matrix.</p><p>To fixed all rotation angles, we need 2 vanishing point in perpendicular direction.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-02-Week2-2.png" style="zoom:60%"><br></center><p>If we can recognize two perpendicular directions in the physical space, then we can recover the camera orientation relative to the world.</p><p>How to figure out both translation and rotation?</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-02-Week2-3.png" style="zoom:60%"><br></center><br>If we do  translation on the camera, the vanishing point remain itself. But when we rotate, the vanishing point will move.<br><br>We can recover rotation column $r_3$ from vanishing point z and $r_1$ from vanishiong point x. And using these two, we can figure out the camera orientation.<br><br>Rotation column $r_3$ tells us about the pan and tilt angles. (left and right, up and down)<br><br>Using K inverse transformation, we can transform from pixel-domain points into optical world.<br><br><br><br><br><br><br>## Quiz: Homogeneous Coordinates<br><br><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-02-Week2-4.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-02-Week2-5.png" style="zoom:60%"><br></center><h2 id="Compute-Projective-Transformations"><a href="#Compute-Projective-Transformations" class="headerlink" title="Compute Projective Transformations"></a>Compute Projective Transformations</h2><p>A perspective projection of a plane (like a camera image) is always a projective transformation.</p><p><strong>Definition</strong></p><p>A <strong>projective transformation</strong> is any invertible matrix transformation $P^2-&gt;P^2$</p><p>A projective transformation $A$ maps $p$ to $p’—Ap$</p><p>A projective transformation is also known as <strong>collineation or homography</strong></p><p>A projective transformation preserves incidence:</p><ul><li>Three collinear points are mapped to three collinear points.</li><li>And three concurrent lines are mapped to three concurrent lines.</li></ul><p>Projective transformation of lines</p><p>If $A$ maps a point to $Ap$, then where does a line $l$ map to?</p><p>Line equation in original plane<br>$$<br>l^Tp=0<br>$$<br>Line equation in image plane $p’—Ap$<br>$$<br>l^TA^{-1}p’=0<br>$$<br>implies that $l’=A^Tl$.</p><p>Computation of projective transformation:</p><hr><p>Assume that a mapping $A$ maps the three points<br>$$<br>\begin {pmatrix}<br>a \<br>b \<br>c<br>\end {pmatrix}<br>=<br>\begin {pmatrix}<br>a\alpha \<br>b\beta \<br>c\gamma<br>\end {pmatrix}<br>\begin {pmatrix}<br>1 &amp; 0 &amp; 0 \<br>0 &amp; 1 &amp; 0 \<br>0 &amp; 0 &amp; 1<br>\end {pmatrix}<br>$$<br>is a valid projective transformation, but 3 degrees of freedom so not sufficient.This transformation map (1,0,0), (0,1,0) and (0,0,1) to a,b and c correspondingly. We neeed to compute $\alpha,\beta$ and $ \gamma$.</p><p>Four points not three of them collinear suffice to recover unambiguously projective transformation.</p><p>With the forth point mapped point $d$, we can calculate as following:<br>$$<br>\begin {pmatrix}<br>a \<br>b \<br>c<br>\end {pmatrix}<br>\begin {pmatrix}<br>\alpha \\<br>\beta \\<br>\gamma<br>\end {pmatrix}<br>=d<br>$$</p><h2 id="Quiz-Projective-Transformations"><a href="#Quiz-Projective-Transformations" class="headerlink" title="Quiz: Projective Transformations"></a>Quiz: Projective Transformations</h2><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-02-Week2-6.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-02-Week2-7.png" style="zoom:60%"><br></center><h2 id="Projective-Transformations-and-Vanishing-Points"><a href="#Projective-Transformations-and-Vanishing-Points" class="headerlink" title="Projective Transformations and Vanishing Points"></a>Projective Transformations and Vanishing Points</h2><p>Geometric interpretation of the projective transformations.</p><p>Projective transformation</p><ul><li>Aka Homography or Collineation</li><li>Represents the perspective projection from a ground plane to an image plane!</li><li>It is an invertible 3x3 matrix but has 8 independent parameters</li><li>For example if (X,Y) measured in meters on the ground and (u,v) in pixels</li></ul><p>$$<br>\begin {pmatrix}<br>u \\<br>v \\<br>1</p><h2 id="end-pmatrix"><a href="#end-pmatrix" class="headerlink" title="\end {pmatrix}"></a>\end {pmatrix}</h2><p>H<br>\begin {pmatrix}<br>X \\<br>Y \\<br>1<br>\end {pmatrix}<br>$$</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-02-Week2-8.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-02-Week2-9.png" style="zoom:60%"><br></center><center><br>    &lt;img src = “<a href="https://azure-pictures.s3.amazonaws.com/blog/2019-04-02-Week2-10.png&quot;" target="_blank" rel="noopener">https://azure-pictures.s3.amazonaws.com/blog/2019-04-02-Week2-10.png&quot;</a><br>         style = “zoom:60%”<br></center><h2 id="Quiz-Vanishing-Points-1"><a href="#Quiz-Vanishing-Points-1" class="headerlink" title="Quiz: Vanishing Points"></a>Quiz: Vanishing Points</h2><p>Note for question 2 and 3: When zooming, the calibration matrix K varies, so the K inverse transformation will change the location of vanishing points. While for translation, the vanishing points remain in their original position because translation disappear.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-03-Week2-11.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-03-Week2-13.png" style="zoom:60%"><br></center><p>Question 4 requires one more step “inverse”.</p><h2 id="Cross-Ratios-and-Single-View-Metrology"><a href="#Cross-Ratios-and-Single-View-Metrology" class="headerlink" title="Cross Ratios and Single View Metrology"></a>Cross Ratios and Single View Metrology</h2><p>The middle point of a segment preserved under parallel projection.</p><p>For a camera, the middle point of a segment doesn’t preserve. But the <strong>Cross-Ratio</strong> is preserved.</p><p>The cross-ratio remain the same under any projective transformation.</p><p>We can use this to measure distances in the real world.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-03-Week2-14.png" style="zoom:60%"><br></center><p>E.g.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-03-Week2-15.png" style="zoom:60%"><br></center><p>What happens when one of the points is at infinity?</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-03-Week2-16.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-03-Week2-17.png" style="zoom:60%"><br></center><p>And we can also compute the position of the vanishing point in the image.</p><p>Distance transfer:</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-03-Week2-18.png" style="zoom:60%"><br></center><p>Single View Metrology via Cross Ratios:</p><ul><li>If we know a vanishing point we can compute any ratio along this direction!</li><li>We can transfer distances among parallel lines in the world if we know two vanishing points.</li><li>In none of these steps we used focal length or any other intrinsics.</li><li>We can do some image forensics on paintings or old photos!</li></ul><h2 id="Quiz-Cross-Ratios-and-Single-View-Metrology"><a href="#Quiz-Cross-Ratios-and-Single-View-Metrology" class="headerlink" title="Quiz: Cross Ratios and Single View Metrology"></a>Quiz: Cross Ratios and Single View Metrology</h2><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-03-Week2-19.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-03-Week2-20.png" style="zoom:60%"><br></center><h2 id="Two-View-Soccer-Metrology"><a href="#Two-View-Soccer-Metrology" class="headerlink" title="Two View Soccer Metrology"></a>Two View Soccer Metrology</h2><p>The forth week of the course will focus on this problem.</p><h2 id="Programming-Assignment-Image-Projection-using-Homographies"><a href="#Programming-Assignment-Image-Projection-using-Homographies" class="headerlink" title="Programming Assignment: Image Projection using Homographies"></a>Programming Assignment: Image Projection using Homographies</h2><p><a href="https://github.com/Aden-Q/Robotics-Perception/tree/master/Image-Projection%20using%20Homographies" target="_blank" rel="noopener">Image Projection using Homographies</a></p><h1 id="Week-3-Pose-Estimation"><a href="#Week-3-Pose-Estimation" class="headerlink" title="Week 3, Pose Estimation"></a>Week 3, Pose Estimation</h1><h2 id="Visual-Features"><a href="#Visual-Features" class="headerlink" title="Visual Features"></a>Visual Features</h2><p>What we want from features?</p><ol><li>Detection repeatability: When they are detected in one image to be detected in another one from the same scene even if image differs in scale and orientation.</li><li>We should be able to match features using a descriptor of the neighborhood.</li><li>This descriptor should not change significantly under viewpoint changes like scale and rotation.</li><li>We call this property descriptor invariance.</li></ol><p>The notion of scale space</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-5.png" style="zoom:60%"><br></center><br>We can build a same scale by subsampling.<br><br><br><br>Scale selection<br><br><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-6.png" style="zoom:60%"><br></center><br>Laplacian of Gaussian (LoG)<br><br><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-14-Week3-10.png" style="zoom:60%"><br></center><p>Scale Space</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-14-Week4-10.png" style="zoom:60%"><br></center><p>SIFT: Scale Invariant Feature Transform</p><p>Laplacian</p><p>Each key point corresponds to a 4x4 grid of histograms.</p><h2 id="Quiz-Visual-Features"><a href="#Quiz-Visual-Features" class="headerlink" title="Quiz: Visual Features"></a>Quiz: Visual Features</h2><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-14-Week4-11.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-14-Week4-12.png" style="zoom:60%"><br></center><h2 id="Singular-Value-Decomposition"><a href="#Singular-Value-Decomposition" class="headerlink" title="Singular Value Decomposition"></a>Singular Value Decomposition</h2><p>1890， Carl Friedrich Gauss</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-7.png" style="zoom:60%"><br></center><p>U/V: Column orthogonal matrix</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-8.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-9.png" style="zoom:60%"><br></center><p>D: Diagnal matrix:</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-10.png" style="zoom:60%"><br></center><p>SVD as basis + transformed Address</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-11.png" style="zoom:60%"><br></center><p>Eigenface generation:</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-12.png" style="zoom:60%"><br></center><p><strong>Rank</strong>:</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-13.png" style="zoom:60%"><br></center><p><strong>Nullspace:</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-14.png" style="zoom:60%"><br></center><p><strong>Matrix Inversion with SVD</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-15.png" style="zoom:60%"><br></center><p><strong>Two types of Least Square Problem:</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-16.png" style="zoom:60%"><br></center><p><strong>Line fitting:</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-17.png" style="zoom:60%"><br></center><h2 id="Quiz-Singular-Value-Decomposition"><a href="#Quiz-Singular-Value-Decomposition" class="headerlink" title="Quiz: Singular Value Decomposition"></a>Quiz: Singular Value Decomposition</h2><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-14-Week4-13.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-14-Week4-14.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-14-Week4-15.png" style="zoom:60%"><br></center><h2 id="RANSAC-Random-Sample-Consensus-I"><a href="#RANSAC-Random-Sample-Consensus-I" class="headerlink" title="RANSAC: Random Sample Consensus I"></a>RANSAC: Random Sample Consensus I</h2><p>Outlier effects (sensitivity):</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-20.png" style="zoom:60%"><br></center><p>Strategy: To find a model that accords with the maximum number of samples.</p><p>Assumptions:</p><ol><li>Majority of good samples agree with the underlying model (good apples are same and simple).</li><li>Bad samples does dot consistently agree with a single model (all bad apples are different and complicated).</li></ol><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-21.png" style="zoom:60%"><br></center><p>Each time choose two points, pick the ‘best’ two points pair.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-22.png" style="zoom:60%"><br></center><h2 id="Quiz-RANSAC"><a href="#Quiz-RANSAC" class="headerlink" title="Quiz: RANSAC"></a>Quiz: RANSAC</h2><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-23.png" style="zoom:60%"><br></center><br>Answer for the 3rd question is “264”, a little strange.<br><br><br><br><br><br><br>## Where am I? Part 1<br><br><strong>Homography Linear Estimation</strong><br><br><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-24.png" style="zoom:60%"><br></center><h2 id="Where-am-I-Part-2"><a href="#Where-am-I-Part-2" class="headerlink" title="Where am I? Part 2"></a>Where am I? Part 2</h2><p><strong>Perspective-n-Point</strong></p><p>6 points can give 12 constrains</p><h2 id="Pose-from-3D-Point-Correspondences-The-Procrustes-Problem"><a href="#Pose-from-3D-Point-Correspondences-The-Procrustes-Problem" class="headerlink" title="Pose from 3D Point Correspondences: The Procrustes Problem"></a>Pose from 3D Point Correspondences: The Procrustes Problem</h2><p><strong>Procrustes Problem</strong></p><p>Given two shapes find the scaling, rotation, and translation that fits one into the other.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-25.png" style="zoom:60%"><br></center><h2 id="Quiz-3D-3D-Pose"><a href="#Quiz-3D-3D-Pose" class="headerlink" title="Quiz: 3D-3D Pose"></a>Quiz: 3D-3D Pose</h2><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-26.png" style="zoom:60%"><br></center><h2 id="Pose-from-Projective-Transformations"><a href="#Pose-from-Projective-Transformations" class="headerlink" title="Pose from Projective Transformations"></a>Pose from Projective Transformations</h2><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-27.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-13-Week3-28.png" style="zoom:60%"><br></center><h2 id="Pose-from-Point-Correspondences-P3P"><a href="#Pose-from-Point-Correspondences-P3P" class="headerlink" title="Pose from Point Correspondences P3P"></a>Pose from Point Correspondences P3P</h2><p>The perspective 3-Point problem of P3P or in photogrammetry the Resection problem: The Snellius-Pothenot problem.</p><h2 id="Quiz-Pose-Estimation"><a href="#Quiz-Pose-Estimation" class="headerlink" title="Quiz: Pose Estimation"></a>Quiz: Pose Estimation</h2><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-14-Week3-30.png" style="zoom:60%"><br></center><h2 id="Programming-Assignment-Image-Projection"><a href="#Programming-Assignment-Image-Projection" class="headerlink" title="Programming Assignment: Image Projection"></a>Programming Assignment: Image Projection</h2><p><a href="https://github.com/Aden-Q/Robotics-Perception/tree/master/Image%20Projection" target="_blank" rel="noopener">Image Projection</a></p><p>Pose Estimation</p><h1 id="Week-4-Multi-View-Geometry"><a href="#Week-4-Multi-View-Geometry" class="headerlink" title="Week 4, Multi-View Geometry"></a>Week 4, Multi-View Geometry</h1><h2 id="Epipolar-Geometry-I"><a href="#Epipolar-Geometry-I" class="headerlink" title="Epipolar Geometry I"></a>Epipolar Geometry I</h2><p>Given two pictures of the scene, calculate the relative transformation and rotation between the two.</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-14-Week4-1.png" style="zoom:60%"><br></center><p>Epipole &amp; Epipolar line</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-14-Week4-2.png" style="zoom:60%"><br></center><p><strong>Point correspondence</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-14-Week4-4.png" style="zoom:60%"><br></center><p>Epipole calculation -&gt; Least Square problem</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-14-Week4-5.png" style="zoom:60%"><br></center><br>$$<br>E = [t]_xR<br>$$<br><br>$E$ is called the essential matrix. It hides inside both transformation and rotation relations between the two cameras. So we can compute $E$ by a few point correspondence.<br><br><br><br><br>## Epipolar Geometry II<br><br>Fundamental matrix:<br><br><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-14-Week4-6.png" style="zoom:60%"><br></center><p>$$<br>x_1=KX_1 \space x_2=KX_2<br>$$</p><p><strong>Fundamental matrix</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-14-Week4-16.png" style="zoom:60%"><br></center><p>8 correspondences are needed to solve $F$ (one for scaling).</p><p><strong>8 Point Algorithm</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-14-Week4-18.png" style="zoom:60%"><br></center><h2 id="Epipolar-Geometry-III"><a href="#Epipolar-Geometry-III" class="headerlink" title="Epipolar Geometry III"></a>Epipolar Geometry III</h2><p><strong>Recovery of R,T from Fundamental Matrix</strong></p><p>Essential matrix and fundamental matrix transformation:<br>$$<br>E = K^TFK<br>$$</p><p>$$<br>E = [t]_xR<br>$$</p><p>First recover t:</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-14-Week4-19.png" style="zoom:60%"><br></center><p>SVD of E</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-14-Week4-20.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-15-Week4-21.png" style="zoom:60%"><br></center><p>if $det(R) =-1,t=-t,R=-R$</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-15-Week4-22.png" style="zoom:60%"><br></center><p><strong>Point Triangulation</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-15-Week4-23.png" style="zoom:60%"><br></center><p>$$<br>C=-R^Tt<br>$$</p><h2 id="Quiz-Epipolar-Geometry"><a href="#Quiz-Epipolar-Geometry" class="headerlink" title="Quiz: Epipolar Geometry"></a>Quiz: Epipolar Geometry</h2><p>10.5/12 。。。。</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-15-Week4-24.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-15-Week4-25.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-15-Week4-28.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-15-Week4-27.png" style="zoom:60%"><br></center><h2 id="RANSAC-Random-Sample-Consensus-II"><a href="#RANSAC-Random-Sample-Consensus-II" class="headerlink" title="RANSAC: Random Sample Consensus II"></a>RANSAC: Random Sample Consensus II</h2><p>Error function:</p><p>distance to the Epipolar line<br>$$<br>\theta=\frac{|ax+by+c|}{\sqrt{a^2+b^2}}<br>$$</p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-15-Week4-29.png" style="zoom:60%"><br></center><h2 id="Nonlinear-Least-Squares-I"><a href="#Nonlinear-Least-Squares-I" class="headerlink" title="Nonlinear Least Squares I"></a>Nonlinear Least Squares I</h2><p><strong>The properti4es of linear least squares:</strong></p><ul><li>Has the global/unique solution.</li><li>Has the closed form solution (non-iterative solve).</li><li>Is solved efficiently (SVD).</li><li>Requires no extra parameters such as initialization.</li></ul><h2 id="Nonlinear-Least-Squares-II"><a href="#Nonlinear-Least-Squares-II" class="headerlink" title="Nonlinear  Least Squares II"></a>Nonlinear  Least Squares II</h2><p><strong>Reprojection Error (Geometric Error)</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-15-Week4-30.png" style="zoom:60%"><br></center><h2 id="Nonlinear-Least-Squares-III"><a href="#Nonlinear-Least-Squares-III" class="headerlink" title="Nonlinear Least Squares III"></a>Nonlinear Least Squares III</h2><p><strong>Jacobian matrix</strong></p><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-15-Week4-31.png" style="zoom:60%"><br></center><h2 id="Quiz-Nonlinear-Least-Squares"><a href="#Quiz-Nonlinear-Least-Squares" class="headerlink" title="Quiz: Nonlinear Least Squares"></a>Quiz: Nonlinear Least Squares</h2><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-15-Week4-32.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-15-Week4-33.png" style="zoom:60%"><br></center><h2 id="Optical-Flow-2D-Point-Correspondences"><a href="#Optical-Flow-2D-Point-Correspondences" class="headerlink" title="Optical Flow: 2D Point Correspondences"></a>Optical Flow: 2D Point Correspondences</h2><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-15-Week4-34.png" style="zoom:60%"><br></center><h2 id="3D-Velocities-from-Optical-Flow"><a href="#3D-Velocities-from-Optical-Flow" class="headerlink" title="3D Velocities from Optical Flow"></a>3D Velocities from Optical Flow</h2><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-15-Week4-36.png" style="zoom:60%"><br></center><h2 id="Quiz-3D-Velocities-from-Optical-Flow"><a href="#Quiz-3D-Velocities-from-Optical-Flow" class="headerlink" title="Quiz: 3D Velocities from Optical Flow"></a>Quiz: 3D Velocities from Optical Flow</h2><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-15-Week4-35.png" style="zoom:60%"><br></center><h2 id="3D-Motion-and-Structure-from-Multiple-Views"><a href="#3D-Motion-and-Structure-from-Multiple-Views" class="headerlink" title="3D Motion and Structure from Multiple Views"></a>3D Motion and Structure from Multiple Views</h2><h2 id="Visual-Odometry"><a href="#Visual-Odometry" class="headerlink" title="Visual Odometry"></a>Visual Odometry</h2><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-15-Week4-40.png" style="zoom:60%"><br></center><h2 id="Bundle-Adjustment-I"><a href="#Bundle-Adjustment-I" class="headerlink" title="Bundle Adjustment I"></a>Bundle Adjustment I</h2><h2 id="Bundle-Adjustment-II"><a href="#Bundle-Adjustment-II" class="headerlink" title="Bundle Adjustment II"></a>Bundle Adjustment II</h2><h2 id="Bundle-Adjustment-III"><a href="#Bundle-Adjustment-III" class="headerlink" title="Bundle Adjustment III"></a>Bundle Adjustment III</h2><h2 id="Quiz-Bundle-Adjustment"><a href="#Quiz-Bundle-Adjustment" class="headerlink" title="Quiz: Bundle Adjustment"></a>Quiz: Bundle Adjustment</h2><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-15-Week4-41.png" style="zoom:60%"><br></center><center><br>    <img src="https://azure-pictures.s3.amazonaws.com/blog/2019-04-15-Week4-41.png" style="zoom:60%"><br></center><h2 id="Programming-Assignment-Structure-from-Motion"><a href="#Programming-Assignment-Structure-from-Motion" class="headerlink" title="Programming Assignment: Structure from Motion"></a>Programming Assignment: Structure from Motion</h2><p><a href="https://github.com/Aden-Q/Robotics-Perception/tree/master/Structure%20from%20Motion" target="_blank" rel="noopener">Structure from Motion</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;University of Pennsylvania, Robotics: Perception&lt;/p&gt;
&lt;p&gt;Course Notes&lt;/p&gt;
    
    </summary>
    
      <category term="Note" scheme="http://yoursite.com/categories/Note/"/>
    
    
      <category term="Programming" scheme="http://yoursite.com/tags/Programming/"/>
    
      <category term="Matlab" scheme="http://yoursite.com/tags/Matlab/"/>
    
      <category term="Robotics" scheme="http://yoursite.com/tags/Robotics/"/>
    
  </entry>
  
  <entry>
    <title>MathJax Formula</title>
    <link href="http://yoursite.com/2019/03/23/MathJax%20Formula/"/>
    <id>http://yoursite.com/2019/03/23/MathJax Formula/</id>
    <published>2019-03-23T22:48:34.000Z</published>
    <updated>2019-03-29T06:12:38.697Z</updated>
    
    <content type="html"><![CDATA[<p>MathJax Formula memo.</p><a id="more"></a><blockquote><p>  MathJax is an open-source JavaScript display engine for Latex, MathML, and AsciiMath notation that works in all modern browsers.</p></blockquote><p>Two modes:</p><ul><li>Inline: embedded into text.</li><li>Displayed: displayed individually.</li></ul><h1 id="Greek-alphabet"><a href="#Greek-alphabet" class="headerlink" title="Greek alphabet"></a>Greek alphabet</h1><table><thead><tr><th style="text-align:center">Name</th><th style="text-align:center">Upper</th><th style="text-align:center">Tex</th><th style="text-align:center">Lower</th><th style="text-align:center">Tex</th></tr></thead><tbody><tr><td style="text-align:center">alpha</td><td style="text-align:center">$A$</td><td style="text-align:center">A</td><td style="text-align:center">$\alpha$</td><td style="text-align:center">\alpha</td></tr><tr><td style="text-align:center">beta</td><td style="text-align:center">$B$</td><td style="text-align:center">B</td><td style="text-align:center">$\beta$</td><td style="text-align:center">\beta</td></tr><tr><td style="text-align:center">gamma</td><td style="text-align:center">$\Gamma$</td><td style="text-align:center">\Gamma</td><td style="text-align:center">$\gamma$</td><td style="text-align:center">\gamma</td></tr><tr><td style="text-align:center">delta</td><td style="text-align:center">$\Delta$</td><td style="text-align:center">\Delta</td><td style="text-align:center">$\delta$</td><td style="text-align:center">\delta</td></tr><tr><td style="text-align:center">epsilon</td><td style="text-align:center">$E$</td><td style="text-align:center">E</td><td style="text-align:center">$\epsilon$</td><td style="text-align:center">\epsilon</td></tr><tr><td style="text-align:center">zeta</td><td style="text-align:center">$Z$</td><td style="text-align:center">Z</td><td style="text-align:center">$\zeta$</td><td style="text-align:center">\zeta</td></tr><tr><td style="text-align:center">eta</td><td style="text-align:center">$H$</td><td style="text-align:center">H</td><td style="text-align:center">$\eta$</td><td style="text-align:center">\eta</td></tr><tr><td style="text-align:center">theta</td><td style="text-align:center">$\Theta$</td><td style="text-align:center">\Theta</td><td style="text-align:center">$\theta$</td><td style="text-align:center">\theta</td></tr><tr><td style="text-align:center">iota</td><td style="text-align:center">$I$</td><td style="text-align:center">I</td><td style="text-align:center">$\iota$</td><td style="text-align:center">\iota</td></tr><tr><td style="text-align:center">kappa</td><td style="text-align:center">$K$</td><td style="text-align:center">K</td><td style="text-align:center">$\kappa$</td><td style="text-align:center">\kappa</td></tr><tr><td style="text-align:center">lambda</td><td style="text-align:center">$\Lambda$</td><td style="text-align:center">\Lambda</td><td style="text-align:center">$\lambda$</td><td style="text-align:center">\lambda</td></tr><tr><td style="text-align:center">mu</td><td style="text-align:center">$M$</td><td style="text-align:center">M</td><td style="text-align:center">$\mu$</td><td style="text-align:center">\mu</td></tr><tr><td style="text-align:center">nu</td><td style="text-align:center">$N$</td><td style="text-align:center">N</td><td style="text-align:center">$\nu$</td><td style="text-align:center">\nu</td></tr><tr><td style="text-align:center">xi</td><td style="text-align:center">$\Xi$</td><td style="text-align:center">\Xi</td><td style="text-align:center">$\xi$</td><td style="text-align:center">\xi</td></tr><tr><td style="text-align:center">omicron</td><td style="text-align:center">$O$</td><td style="text-align:center">O</td><td style="text-align:center">$\omicron$</td><td style="text-align:center">\omicron</td></tr><tr><td style="text-align:center">pi</td><td style="text-align:center">$\Pi$</td><td style="text-align:center">\Pi</td><td style="text-align:center">$\pi$</td><td style="text-align:center">\pi</td></tr><tr><td style="text-align:center">rho</td><td style="text-align:center">$P$</td><td style="text-align:center">P</td><td style="text-align:center">$\rho$</td><td style="text-align:center">\rho</td></tr><tr><td style="text-align:center">sigma</td><td style="text-align:center">$\Sigma$</td><td style="text-align:center">\Sigma</td><td style="text-align:center">$\sigma$</td><td style="text-align:center">\sigma</td></tr><tr><td style="text-align:center">tau</td><td style="text-align:center">$T$</td><td style="text-align:center">T</td><td style="text-align:center">$\tau$</td><td style="text-align:center">\tau</td></tr><tr><td style="text-align:center">upsilon</td><td style="text-align:center">$\Upsilon$</td><td style="text-align:center">\Upsilon</td><td style="text-align:center">$\upsilon$</td><td style="text-align:center">\upsilon</td></tr><tr><td style="text-align:center">phi</td><td style="text-align:center">$\Phi$</td><td style="text-align:center">\Phi</td><td style="text-align:center">$\phi$</td><td style="text-align:center">\phi</td></tr><tr><td style="text-align:center">chi</td><td style="text-align:center">$X$</td><td style="text-align:center">X</td><td style="text-align:center">$\chi$</td><td style="text-align:center">\chi</td></tr><tr><td style="text-align:center">psi</td><td style="text-align:center">$\Psi$</td><td style="text-align:center">\Psi</td><td style="text-align:center">$\psi$</td><td style="text-align:center">\psi</td></tr><tr><td style="text-align:center">omega</td><td style="text-align:center">$\Omega$</td><td style="text-align:center">\Omega</td><td style="text-align:center">$\omega$</td><td style="text-align:center">\omega</td></tr></tbody></table><h1 id="Superscript-and-Subscript"><a href="#Superscript-and-Subscript" class="headerlink" title="Superscript and Subscript"></a>Superscript and Subscript</h1><p>Group the notation by curly bracket “{}”, other wise, the script only count for the one consecutive letter.</p><table><thead><tr><th style="text-align:center">Text</th><th style="text-align:center">Tex</th></tr></thead><tbody><tr><td style="text-align:center">$x^{2}$</td><td style="text-align:center">x^{2}</td></tr><tr><td style="text-align:center">$x_0$</td><td style="text-align:center">x_0</td></tr><tr><td style="text-align:center">$\hat x$</td><td style="text-align:center">\hat x</td></tr><tr><td style="text-align:center">$\widehat {xy} $</td><td style="text-align:center">\widehat {xy}</td></tr><tr><td style="text-align:center">$\hat {xy} $</td><td style="text-align:center">\hat {xy}</td></tr><tr><td style="text-align:center">$\overline {xyz}$</td><td style="text-align:center">\overline {xyz}</td></tr><tr><td style="text-align:center">$\vec x$</td><td style="text-align:center">\vec x</td></tr><tr><td style="text-align:center">$\dot x$</td><td style="text-align:center">\dot x</td></tr><tr><td style="text-align:center">$\ddot x$</td><td style="text-align:center">\ddot x</td></tr></tbody></table><h1 id="Brackets"><a href="#Brackets" class="headerlink" title="Brackets"></a>Brackets</h1><table><thead><tr><th style="text-align:center">Left</th><th style="text-align:center">Tex</th><th style="text-align:center">Right</th><th style="text-align:center">Tex$$</th></tr></thead><tbody><tr><td style="text-align:center">$\lbrace$</td><td style="text-align:center">\lbrace</td><td style="text-align:center">$\rbrace$</td><td style="text-align:center">\rbrace</td></tr><tr><td style="text-align:center">$\langle$</td><td style="text-align:center">\langle</td><td style="text-align:center">$\rangle$</td><td style="text-align:center">\rangle</td></tr><tr><td style="text-align:center">$\lceil$</td><td style="text-align:center">\lceil</td><td style="text-align:center">$\rceil$</td><td style="text-align:center">\rceil</td></tr><tr><td style="text-align:center">$\lfloor$</td><td style="text-align:center">\lfloor</td><td style="text-align:center">$\rfloor$</td><td style="text-align:center">\rfloor</td></tr></tbody></table><h1 id="Fraction-and-Radical-expression"><a href="#Fraction-and-Radical-expression" class="headerlink" title="Fraction and Radical expression"></a>Fraction and Radical expression</h1><p>Similarly, we group numerator and denominator with “{}”, otherwise we only count the first digit or letter.</p><table><thead><tr><th style="text-align:center">Simple form</th><th style="text-align:center">Tex</th><th style="text-align:center">General form</th><th style="text-align:center">Tex</th></tr></thead><tbody><tr><td style="text-align:center">$\frac ab$</td><td style="text-align:center">\frac ab</td><td style="text-align:center">$\frac{abc}{def}$</td><td style="text-align:center">\frac{abc}{def}</td></tr></tbody></table><p>Radical:</p><table><thead><tr><th style="text-align:center">Text</th><th style="text-align:center">Tex</th></tr></thead><tbody><tr><td style="text-align:center">$\sqrt[2]{2}$</td><td style="text-align:center">\sqrt[2]{2}</td></tr></tbody></table><h1 id="Summation-and-Integral"><a href="#Summation-and-Integral" class="headerlink" title="Summation and Integral"></a>Summation and Integral</h1><table><thead><tr><th style="text-align:center">Text</th><th style="text-align:center">Tex</th></tr></thead><tbody><tr><td style="text-align:center">$\sum_1^n$</td><td style="text-align:center">\sum_1^n</td></tr><tr><td style="text-align:center">$\int_1^\infty$</td><td style="text-align:center">\int_1^\infty</td></tr><tr><td style="text-align:center">$\prod_1^n$</td><td style="text-align:center">\prod_1^n</td></tr></tbody></table><h1 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h1><table><thead><tr><th style="text-align:center">Text</th><th style="text-align:center">Tex</th></tr></thead><tbody><tr><td style="text-align:center">$\lt$</td><td style="text-align:center">\lt</td></tr><tr><td style="text-align:center">$\gt$</td><td style="text-align:center">\gt</td></tr><tr><td style="text-align:center">$\le$</td><td style="text-align:center">\leq</td></tr><tr><td style="text-align:center">$\geq$</td><td style="text-align:center">\geq</td></tr><tr><td style="text-align:center">$\neq$</td><td style="text-align:center">\neq</td></tr></tbody></table><h1 id="Arithmetic"><a href="#Arithmetic" class="headerlink" title="Arithmetic"></a>Arithmetic</h1><table><thead><tr><th style="text-align:center">Text</th><th style="text-align:center">Tex</th></tr></thead><tbody><tr><td style="text-align:center">$\times$</td><td style="text-align:center">\times</td></tr><tr><td style="text-align:center">$\div$</td><td style="text-align:center">\div</td></tr><tr><td style="text-align:center">$\pm$</td><td style="text-align:center">\pm</td></tr><tr><td style="text-align:center">$\mp$</td><td style="text-align:center">\mp</td></tr><tr><td style="text-align:center">$\cdot$</td><td style="text-align:center">\cdot</td></tr><tr><td style="text-align:center">$\approx$</td><td style="text-align:center">\approx</td></tr></tbody></table><h1 id="Set"><a href="#Set" class="headerlink" title="Set"></a>Set</h1><table><thead><tr><th style="text-align:center">Text</th><th style="text-align:center">Tex</th></tr></thead><tbody><tr><td style="text-align:center">$\cup$</td><td style="text-align:center">\cup</td></tr><tr><td style="text-align:center">$\cap$</td><td style="text-align:center">\cap</td></tr><tr><td style="text-align:center">$\setminus$</td><td style="text-align:center">\setminus</td></tr><tr><td style="text-align:center">$\subset$</td><td style="text-align:center">\subset</td></tr><tr><td style="text-align:center">$\subseteq $</td><td style="text-align:center">\subseteq</td></tr><tr><td style="text-align:center">$\subsetneq$</td><td style="text-align:center">\subsetneq</td></tr><tr><td style="text-align:center">$\supset$</td><td style="text-align:center">\supset</td></tr><tr><td style="text-align:center">$\in$</td><td style="text-align:center">\in</td></tr><tr><td style="text-align:center">$\notin$</td><td style="text-align:center">\notin</td></tr></tbody></table><p>Empty set: $\emptyset ​$ , Text: \emptyset</p><h1 id="Combination"><a href="#Combination" class="headerlink" title="Combination"></a>Combination</h1><table><thead><tr><th style="text-align:center">Text</th><th style="text-align:center">Tex</th><th style="text-align:center">Optional</th></tr></thead><tbody><tr><td style="text-align:center">${n+1 \choose 2k}$</td><td style="text-align:center">{n+1 \choose 2k}</td><td style="text-align:center">\binom{n+1}{2k}</td></tr></tbody></table><h1 id="Row"><a href="#Row" class="headerlink" title="Row"></a>Row</h1><table><thead><tr><th style="text-align:center">Text</th><th style="text-align:center">Tex</th></tr></thead><tbody><tr><td style="text-align:center">$\to$</td><td style="text-align:center">\rightarrow</td></tr><tr><td style="text-align:center">$\leftarrow$</td><td style="text-align:center">\leftarrow</td></tr><tr><td style="text-align:center">$\Rightarrow$</td><td style="text-align:center">\Rightarrow</td></tr><tr><td style="text-align:center">$\Leftarrow$</td><td style="text-align:center">\Leftarrow</td></tr></tbody></table><h1 id="Logical-operation"><a href="#Logical-operation" class="headerlink" title="Logical operation"></a>Logical operation</h1><table><thead><tr><th style="text-align:center">Text</th><th style="text-align:center">Tex</th></tr></thead><tbody><tr><td style="text-align:center">$\land$</td><td style="text-align:center">\land</td></tr><tr><td style="text-align:center">$\lor$</td><td style="text-align:center">\lor</td></tr><tr><td style="text-align:center">$\lnot$</td><td style="text-align:center">\lnot</td></tr><tr><td style="text-align:center">$\forall$</td><td style="text-align:center">\forall</td></tr><tr><td style="text-align:center">$\exists $</td><td style="text-align:center">\exists</td></tr></tbody></table><h1 id="Modular-arithmetic"><a href="#Modular-arithmetic" class="headerlink" title="Modular arithmetic"></a>Modular arithmetic</h1><table><thead><tr><th style="text-align:center">Text</th><th style="text-align:center">Tex</th></tr></thead><tbody><tr><td style="text-align:center">$a\equiv b\pmod n$</td><td style="text-align:center">a\equiv b\pmod b</td></tr></tbody></table><h1 id="Table"><a href="#Table" class="headerlink" title="Table"></a>Table</h1><p>Eg1, Tex: <code>\begin {array}{c+|lcr} n &amp; \text{Left} &amp; \text{Center} &amp; \text{Right} \\ \hline 1 &amp; 0.24 &amp; 1 &amp; 125 \\ 2 &amp; -1 &amp; 189 &amp; -8 \\ 3 &amp; -20 &amp; 2000 &amp; 1+10i \end {array}</code></p><p>$$<br>\begin {array} \text{n} &amp; \text{Left} &amp; \text{Center} &amp; \text{Right} \\<br>1 &amp; 0.24 &amp; 1 &amp; 125 \\<br>2 &amp; -1 &amp; 189 &amp; -8 \\<br>3 &amp; -20 &amp; 2000 &amp; 1+10i<br>\end {array}<br>$$</p><p>Eg2: Tex: <code>\begin {array}{c+|lcr} n &amp; \text{Left} &amp; \text{Center} &amp; \text{Right} \\ \hline 1 &amp; 0.24 &amp; 1 &amp; 125 \\ 2 &amp; -1 &amp; 189 &amp; -8 \\ 3 &amp; -20 &amp; 2000 &amp; 1+10i \end {array}</code></p><p>$$<br>\begin {array}{c+|lcr}<br>n &amp; \text{Left} &amp; \text{Center} &amp; \text{Right} \\<br>\hline 1 &amp; 0.24 &amp; 1 &amp; 125 \\<br>2 &amp; -1 &amp; 189 &amp; -8 \\<br>3 &amp; -20 &amp; 2000 &amp; 1+10i<br>\end {array}<br>$$</p><h1 id="Matrix"><a href="#Matrix" class="headerlink" title="Matrix"></a>Matrix</h1><h2 id="matrix"><a href="#matrix" class="headerlink" title="matrix"></a>matrix</h2><p> \begin {matrix} 1 &amp; x &amp; x^2 \ 1 &amp; y &amp; y^2 \ 1 &amp; z &amp; z^2 \\ \end{matrix}</p><p>$$<br>\begin {matrix}<br>1 &amp; x &amp; x^2 \\<br>1 &amp; y &amp; y^2 \\<br>1 &amp; z &amp; z^2<br>\end{matrix}<br>$$</p><h2 id="pmatrix"><a href="#pmatrix" class="headerlink" title="pmatrix"></a>pmatrix</h2><p>\begin {pmatrix} 1 &amp; x &amp; x^2 \ 1 &amp; y &amp; y^2 \ 1 &amp; z &amp; z^2 \\ \end{pmatrix}</p><p>$$<br>\begin {pmatrix}<br>1 &amp; x &amp; x^2 \\<br>1 &amp; y &amp; y^2 \\<br>1 &amp; z &amp; z^2<br>\end{pmatrix}<br>$$</p><h2 id="bmatrix"><a href="#bmatrix" class="headerlink" title="bmatrix"></a>bmatrix</h2><p>\begin {bmatrix} 1 &amp; x &amp; x^2 \ 1 &amp; y &amp; y^2 \ 1 &amp; z &amp; z^2 \\ \end{bmatrix}</p><p>$$<br>\begin {bmatrix}<br>1 &amp; x &amp; x^2 \\<br>1 &amp; y &amp; y^2 \\<br>1 &amp; z &amp; z^2<br>\end{bmatrix}<br>$$</p><h2 id="Bmatrix"><a href="#Bmatrix" class="headerlink" title="Bmatrix"></a>Bmatrix</h2><p>\begin {Bmatrix} 1 &amp; x &amp; x^2 \ 1 &amp; y &amp; y^2 \ 1 &amp; z &amp; z^2 \\ \end{Bmatrix}</p><p>$$<br>\begin {Bmatrix}<br>1 &amp; x &amp; x^2 \\<br>1 &amp; y &amp; y^2 \\<br>1 &amp; z &amp; z^2<br>\end{Bmatrix}<br>$$</p><h2 id="vmatrix"><a href="#vmatrix" class="headerlink" title="vmatrix"></a>vmatrix</h2><p> \begin {vmatrix} 1 &amp; x &amp; x^2 \ 1 &amp; y &amp; y^2 \ 1 &amp; z &amp; z^2 \\ \end{vmatrix}</p><p>$$<br>\begin {vmatrix}<br>1 &amp; x &amp; x^2 \\<br>1 &amp; y &amp; y^2 \\<br>1 &amp; z &amp; z^2<br>\end{vmatrix}<br>$$</p><h2 id="Vmatrix"><a href="#Vmatrix" class="headerlink" title="Vmatrix"></a>Vmatrix</h2><p> \begin {Vmatrix} 1 &amp; x &amp; x^2 \ 1 &amp; y &amp; y^2 \ 1 &amp; z &amp; z^2 \\ \end{Vmatrix}</p><p>$$<br>\begin {Vmatrix}<br>1 &amp; x &amp; x^2 \\<br>1 &amp; y &amp; y^2 \\<br>1 &amp; z &amp; z^2<br>\end{Vmatrix}<br>$$</p><h1 id="Reference-Hoocoln"><a href="#Reference-Hoocoln" class="headerlink" title="Reference: Hoocoln"></a>Reference: <a href="http://3iter.com/2015/10/14/Mathjax%E4%B8%8ELaTex%E5%85%AC%E5%BC%8F%E7%AE%80%E4%BB%8B/" target="_blank" rel="noopener">Hoocoln</a></h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;MathJax Formula memo.&lt;/p&gt;
    
    </summary>
    
      <category term="Note" scheme="http://yoursite.com/categories/Note/"/>
    
    
      <category term="Math" scheme="http://yoursite.com/tags/Math/"/>
    
      <category term="MathJax" scheme="http://yoursite.com/tags/MathJax/"/>
    
  </entry>
  
</feed>
